{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ebd4bb-2c27-43a5-a0aa-ea4992efe130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TensorFlow e bibliotecas ML configuradas\n",
      "üöÄ WDO GERADOR v21 - PARTE 1/6 CARREGADA\n",
      "‚úÖ Configura√ß√µes customiz√°veis implementadas\n",
      "‚úÖ Sistema de persist√™ncia de modelos criado\n",
      "‚úÖ Parser de argumentos configurado\n",
      "\n",
      "üìã Pr√≥ximo: Execute a Parte 2/6 para continuar...\n"
     ]
    }
   ],
   "source": [
    "# WDO GERADOR v21 - PARTE 1/6\n",
    "# CONFIGURA√á√ïES CUSTOMIZ√ÅVEIS, IMPORTA√á√ïES E ESTRUTURA BASE\n",
    "# Melhorias: Config customiz√°vel, hyperpar√¢metros, estrutura para salvamento de modelos\n",
    "import catboost as cb\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import MetaTrader5 as mt5\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import ta\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib import colors\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "import locale\n",
    "import json\n",
    "import yaml\n",
    "import joblib\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "# DEEP LEARNING COM CONFIGURA√á√ïES PERSONALIZ√ÅVEIS\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Configurar TensorFlow para m√°xima velocidade\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(0)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(0)\n",
    "    \n",
    "    # Configurar GPU se dispon√≠vel\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"GPU config: {e}\")\n",
    "    \n",
    "    from tensorflow.keras.models import Sequential, load_model\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D, GRU, Bidirectional\n",
    "    from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    \n",
    "    DEEP_LEARNING_AVAILABLE = True\n",
    "    print(\"‚úÖ TensorFlow e bibliotecas ML configuradas\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Bibliotecas de Deep Learning n√£o encontradas: {e}\")\n",
    "    print(\"   Para Deep Learning: pip install tensorflow scikit-learn\")\n",
    "    DEEP_LEARNING_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ConfigManager:\n",
    "    \"\"\"Gerenciador de configura√ß√µes customiz√°veis\"\"\"\n",
    "    \n",
    "    DEFAULT_CONFIG = {\n",
    "        \"data\": {\n",
    "            \"min_data_points\": 5,\n",
    "            \"max_data_points\": 500000,\n",
    "            \"price_range\": 80,\n",
    "            \"min_volume\": 1000,\n",
    "            \"top_levels\": 10\n",
    "        },\n",
    "        \"ml_models\": {\n",
    "            \"enabled_models\": [\"GradientBoosting\", \"XGBoost\", \"LightGBM\", \"CatBoost\"],\n",
    "            \"lstm\": {\n",
    "                \"units\": [64, 32],\n",
    "                \"dropout\": 0.2,\n",
    "                \"recurrent_dropout\": 0.1,\n",
    "                \"epochs\": 50,\n",
    "                \"batch_size\": 32,\n",
    "                \"learning_rate\": 0.001,\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"early_stopping_patience\": 10,\n",
    "                \"sequence_length\": 60,\n",
    "                \"bidirectional\": False\n",
    "            },\n",
    "\n",
    "            \"lstm_bidirectional\": {\n",
    "                \"units\": [128, 64, 32],\n",
    "                \"dropout\": 0.25,\n",
    "                \"recurrent_dropout\": 0.15,\n",
    "                \"epochs\": 60,\n",
    "                \"batch_size\": 32,\n",
    "                \"learning_rate\": 0.0008,\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"early_stopping_patience\": 15,\n",
    "                \"sequence_length\": 80,\n",
    "                \"bidirectional\": True  # Sempre True para este modelo\n",
    "            },\n",
    "            \n",
    "            # Adicionar ap√≥s a configura√ß√£o do SVR:\n",
    "            \"catboost\": {\n",
    "                \"iterations\": 500,\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"depth\": 6,\n",
    "                \"l2_leaf_reg\": 3,\n",
    "                \"border_count\": 128,\n",
    "                \"random_state\": 42,\n",
    "                \"verbose\": False,\n",
    "                \"early_stopping_rounds\": 50,\n",
    "                \"use_best_model\": True,\n",
    "                \"task_type\": \"CPU\"  # ou \"GPU\" se tiver GPU dispon√≠vel\n",
    "            },\n",
    "            \n",
    "            \"gru\": {\n",
    "                \"units\": [64, 32],\n",
    "                \"dropout\": 0.2,\n",
    "                \"recurrent_dropout\": 0.1,\n",
    "                \"epochs\": 50,\n",
    "                \"batch_size\": 32,\n",
    "                \"learning_rate\": 0.001,\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"early_stopping_patience\": 10,\n",
    "                \"sequence_length\": 60,\n",
    "                \"bidirectional\": True\n",
    "            },\n",
    "            \"cnn_lstm\": {\n",
    "                \"conv_filters\": [64, 32],\n",
    "                \"conv_kernel_size\": 3,\n",
    "                \"lstm_units\": 50,\n",
    "                \"dropout\": 0.2,\n",
    "                \"epochs\": 40,\n",
    "                \"batch_size\": 32,\n",
    "                \"learning_rate\": 0.001,\n",
    "                \"sequence_length\": 60\n",
    "            },\n",
    "            \"random_forest\": {\n",
    "                \"n_estimators\": 200,\n",
    "                \"max_depth\": 15,\n",
    "                \"min_samples_split\": 5,\n",
    "                \"min_samples_leaf\": 2,\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": -1,\n",
    "                \"max_features\": \"sqrt\"\n",
    "            },\n",
    "            \"gradient_boosting\": {\n",
    "                \"n_estimators\": 200,\n",
    "                \"learning_rate\": 0.1,\n",
    "                \"max_depth\": 6,\n",
    "                \"min_samples_split\": 5,\n",
    "                \"min_samples_leaf\": 2,\n",
    "                \"random_state\": 42,\n",
    "                \"subsample\": 0.8\n",
    "            },\n",
    "            \"svr\": {\n",
    "                \"C\": 100,\n",
    "                \"gamma\": \"scale\",\n",
    "                \"kernel\": \"rbf\",\n",
    "                \"epsilon\": 0.01\n",
    "            }\n",
    "        },\n",
    "        \"validation\": {\n",
    "            \"use_cross_validation\": True,\n",
    "            \"cv_folds\": 5,\n",
    "            \"test_size\": 0.2,\n",
    "            \"validation_split\": 0.1,\n",
    "            \"time_series_split\": True\n",
    "        },\n",
    "        \"clustering\": {\n",
    "            \"method\": \"kmeans\",  # kmeans, dbscan\n",
    "            \"kmeans_clusters\": 8,\n",
    "            \"dbscan_eps\": 0.5,\n",
    "            \"dbscan_min_samples\": 5,\n",
    "            \"feature_scaling\": \"standard\"\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"technical_indicators\": {\n",
    "                \"sma_periods\": [5, 10, 20, 50, 200],\n",
    "                \"ema_periods\": [5, 10, 20, 50],\n",
    "                \"rsi_period\": 14,\n",
    "                \"macd_fast\": 12,\n",
    "                \"macd_slow\": 26,\n",
    "                \"macd_signal\": 9,\n",
    "                \"bollinger_period\": 20,\n",
    "                \"bollinger_std\": 2,\n",
    "                \"stoch_k\": 14,\n",
    "                \"stoch_d\": 3,\n",
    "                \"atr_period\": 14,\n",
    "                \"williams_r_period\": 14\n",
    "            },\n",
    "            \"custom_features\": {\n",
    "                \"price_momentum_periods\": [5, 10, 20],\n",
    "                \"volume_momentum_periods\": [5, 10],\n",
    "                \"volatility_periods\": [10, 20, 30],\n",
    "                \"correlation_period\": 20\n",
    "            }\n",
    "        },\n",
    "        \"model_persistence\": {\n",
    "            \"save_models\": True,\n",
    "            \"models_directory\": \"./saved_models\",\n",
    "            \"cache_directory\": \"./cache\",\n",
    "            \"model_cache_hours\": 24,\n",
    "            \"auto_retrain_threshold\": 0.15  # Re-treinar se MAE aumentar 15%\n",
    "        },\n",
    "        \"optimization\": {\n",
    "            \"use_gpu\": True,\n",
    "            \"mixed_precision\": False,\n",
    "            \"parallel_jobs\": -1,\n",
    "            \"memory_limit_gb\": 8\n",
    "        },\n",
    "        \"logging\": {\n",
    "            \"level\": \"INFO\",\n",
    "            \"file\": \"wdo_analyzer.log\",\n",
    "            \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config_file=None, args=None):\n",
    "        self.config = self.DEFAULT_CONFIG.copy()\n",
    "        self.config_file = config_file\n",
    "        self.args = args\n",
    "        \n",
    "        # Carregar configura√ß√£o de arquivo\n",
    "        if config_file and os.path.exists(config_file):\n",
    "            self.load_from_file(config_file)\n",
    "        \n",
    "        # Aplicar argumentos da linha de comando\n",
    "        if args:\n",
    "            self.apply_args(args)\n",
    "        \n",
    "        # Criar diret√≥rios necess√°rios\n",
    "        self.create_directories()\n",
    "        \n",
    "        # Configurar logging\n",
    "        self.setup_logging()\n",
    "    \n",
    "    def load_from_file(self, config_file):\n",
    "        \"\"\"Carrega configura√ß√£o de arquivo YAML ou JSON\"\"\"\n",
    "        try:\n",
    "            with open(config_file, 'r', encoding='utf-8') as f:\n",
    "                if config_file.endswith('.yaml') or config_file.endswith('.yml'):\n",
    "                    file_config = yaml.safe_load(f)\n",
    "                else:\n",
    "                    file_config = json.load(f)\n",
    "            \n",
    "            # Merge recursivo com configura√ß√£o padr√£o\n",
    "            self.config = self._deep_merge(self.config, file_config)\n",
    "            print(f\"‚úÖ Configura√ß√£o carregada de: {config_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Erro ao carregar config: {e}. Usando configura√ß√£o padr√£o.\")\n",
    "    \n",
    "    def apply_args(self, args):\n",
    "        \"\"\"Aplica argumentos da linha de comando\"\"\"\n",
    "        if hasattr(args, 'epochs') and args.epochs:\n",
    "            for model in ['lstm', 'gru', 'cnn_lstm']:\n",
    "                if model in self.config['ml_models']:\n",
    "                    self.config['ml_models'][model]['epochs'] = args.epochs\n",
    "        \n",
    "        if hasattr(args, 'batch_size') and args.batch_size:\n",
    "            for model in ['lstm', 'gru', 'cnn_lstm']:\n",
    "                if model in self.config['ml_models']:\n",
    "                    self.config['ml_models'][model]['batch_size'] = args.batch_size\n",
    "        \n",
    "        if hasattr(args, 'models') and args.models:\n",
    "            self.config['ml_models']['enabled_models'] = args.models.split(',')\n",
    "        \n",
    "        if hasattr(args, 'no_save_models') and args.no_save_models:\n",
    "            self.config['model_persistence']['save_models'] = False\n",
    "        \n",
    "        if hasattr(args, 'cv_folds') and args.cv_folds:\n",
    "            self.config['validation']['cv_folds'] = args.cv_folds\n",
    "    \n",
    "    def _deep_merge(self, base_dict, override_dict):\n",
    "        \"\"\"Merge recursivo de dicion√°rios\"\"\"\n",
    "        result = base_dict.copy()\n",
    "        for key, value in override_dict.items():\n",
    "            if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n",
    "                result[key] = self._deep_merge(result[key], value)\n",
    "            else:\n",
    "                result[key] = value\n",
    "        return result\n",
    "    \n",
    "    def create_directories(self):\n",
    "        \"\"\"Cria diret√≥rios necess√°rios\"\"\"\n",
    "        directories = [\n",
    "            self.config['model_persistence']['models_directory'],\n",
    "            self.config['model_persistence']['cache_directory']\n",
    "        ]\n",
    "        \n",
    "        for directory in directories:\n",
    "            Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configura sistema de logging\"\"\"\n",
    "        log_config = self.config['logging']\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=getattr(logging, log_config['level']),\n",
    "            format=log_config['format'],\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_config['file']),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Reduzir verbosidade de bibliotecas externas\n",
    "        logging.getLogger('tensorflow').setLevel(logging.WARNING)\n",
    "        logging.getLogger('sklearn').setLevel(logging.WARNING)\n",
    "    \n",
    "    def get(self, key_path, default=None):\n",
    "        \"\"\"Obt√©m valor da configura√ß√£o usando nota√ß√£o de ponto\"\"\"\n",
    "        keys = key_path.split('.')\n",
    "        value = self.config\n",
    "        \n",
    "        try:\n",
    "            for key in keys:\n",
    "                value = value[key]\n",
    "            return value\n",
    "        except KeyError:\n",
    "            return default\n",
    "    \n",
    "    def save_config(self, output_file):\n",
    "        \"\"\"Salva configura√ß√£o atual em arquivo\"\"\"\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                if output_file.endswith('.yaml') or output_file.endswith('.yml'):\n",
    "                    yaml.dump(self.config, f, default_flow_style=False, indent=2)\n",
    "                else:\n",
    "                    json.dump(self.config, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"‚úÖ Configura√ß√£o salva em: {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao salvar configura√ß√£o: {e}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Imprime resumo da configura√ß√£o\"\"\"\n",
    "        print(\"\\nüìã CONFIGURA√á√ÉO ATUAL:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Modelos habilitados\n",
    "        models = self.config['ml_models']['enabled_models']\n",
    "        print(f\"ü§ñ Modelos habilitados: {', '.join(models)}\")\n",
    "        \n",
    "        # Valida√ß√£o cruzada\n",
    "        if self.config['validation']['use_cross_validation']:\n",
    "            folds = self.config['validation']['cv_folds']\n",
    "            print(f\"üîÑ Valida√ß√£o cruzada: {folds} folds\")\n",
    "        else:\n",
    "            print(\"üîÑ Valida√ß√£o cruzada: Desabilitada\")\n",
    "        \n",
    "        # Persist√™ncia de modelos\n",
    "        if self.config['model_persistence']['save_models']:\n",
    "            models_dir = self.config['model_persistence']['models_directory']\n",
    "            print(f\"üíæ Salvamento de modelos: {models_dir}\")\n",
    "        else:\n",
    "            print(\"üíæ Salvamento de modelos: Desabilitado\")\n",
    "        \n",
    "        # Clustering\n",
    "        cluster_method = self.config['clustering']['method']\n",
    "        print(f\"üéØ Clustering S/R: {cluster_method}\")\n",
    "        \n",
    "        # Dados\n",
    "        min_data = self.config['data']['min_data_points']\n",
    "        max_data = self.config['data']['max_data_points']\n",
    "        print(f\"üìä Dados: {min_data:,} - {max_data:,} registros\")\n",
    "\n",
    "class ModelPersistence:\n",
    "    \"\"\"Gerenciador de persist√™ncia e cache de modelos\"\"\"\n",
    "    \n",
    "    def __init__(self, config_manager):\n",
    "        self.config = config_manager\n",
    "        self.models_dir = Path(config_manager.get('model_persistence.models_directory'))\n",
    "        self.cache_dir = Path(config_manager.get('model_persistence.cache_directory'))\n",
    "        self.cache_hours = config_manager.get('model_persistence.model_cache_hours', 24)\n",
    "        \n",
    "        # Criar diret√≥rios\n",
    "        self.models_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def generate_model_hash(self, data_hash, model_config):\n",
    "        \"\"\"Gera hash √∫nico para identificar modelo\"\"\"\n",
    "        config_str = json.dumps(model_config, sort_keys=True)\n",
    "        combined = f\"{data_hash}_{config_str}\"\n",
    "        return hashlib.md5(combined.encode()).hexdigest()[:16]\n",
    "    \n",
    "    def generate_data_hash(self, df):\n",
    "        \"\"\"Gera hash dos dados para identificar mudan√ßas\"\"\"\n",
    "        # Usar amostra dos dados para performance\n",
    "        sample_size = min(1000, len(df))\n",
    "        sample_data = df.tail(sample_size)\n",
    "        \n",
    "        # Criar string representativa\n",
    "        data_str = f\"{len(df)}_{sample_data['√öLT. PRE√áO'].iloc[0]}_{sample_data['√öLT. PRE√áO'].iloc[-1]}_{sample_data['VOL.'].sum()}\"\n",
    "        return hashlib.md5(data_str.encode()).hexdigest()[:16]\n",
    "    \n",
    "    def is_model_cache_valid(self, model_hash):\n",
    "        \"\"\"Verifica se cache do modelo ainda √© v√°lido\"\"\"\n",
    "        model_file = self.models_dir / f\"model_{model_hash}.pkl\"\n",
    "        \n",
    "        if not model_file.exists():\n",
    "            return False\n",
    "        \n",
    "        # Verificar idade do arquivo\n",
    "        model_age = datetime.now() - datetime.fromtimestamp(model_file.stat().st_mtime)\n",
    "        return model_age.total_seconds() < (self.cache_hours * 3600)\n",
    "    \n",
    "    def save_model(self, model, model_name, model_hash, metadata=None):\n",
    "        \"\"\"Salva modelo treinado\"\"\"\n",
    "        try:\n",
    "            model_file = self.models_dir / f\"model_{model_hash}.pkl\"\n",
    "            metadata_file = self.models_dir / f\"metadata_{model_hash}.json\"\n",
    "    \n",
    "            # M√©todo espec√≠fico para modelos baseados em √°rvores (usando joblib)\n",
    "            if model_name in ['RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM', 'CatBoost']:\n",
    "                joblib_path = self.models_dir / f\"{model_name}_{model_hash}.joblib\"\n",
    "                joblib.dump(model, joblib_path)\n",
    "                \n",
    "                # Salvar refer√™ncia ao arquivo joblib\n",
    "                with open(model_file, 'wb') as f:\n",
    "                    pickle.dump({'type': 'joblib', 'path': str(joblib_path)}, f)\n",
    "                \n",
    "                print(f\"‚úÖ Modelo {model_name} salvo com sucesso usando joblib\")\n",
    "            \n",
    "            # M√©todo para modelos Keras/TensorFlow\n",
    "            elif hasattr(model, 'save'):  # TensorFlow/Keras model\n",
    "                model_dir = self.models_dir / f\"keras_model_{model_hash}\"\n",
    "                model.save(str(model_dir))\n",
    "                \n",
    "                # Salvar refer√™ncia\n",
    "                with open(model_file, 'wb') as f:\n",
    "                    pickle.dump({'type': 'keras', 'path': str(model_dir)}, f)\n",
    "            \n",
    "            # Fallback para outros modelos scikit-learn\n",
    "            else:\n",
    "                with open(model_file, 'wb') as f:\n",
    "                    pickle.dump({'type': 'sklearn', 'model': model}, f)\n",
    "            \n",
    "            # CORRE√á√ÉO: Salvar metadados sem objetos n√£o serializ√°veis\n",
    "            if metadata is None:\n",
    "                metadata = {}\n",
    "            \n",
    "            # Criar metadados serializ√°veis\n",
    "            serializable_metadata = {\n",
    "                'model_name': model_name,\n",
    "                'model_hash': model_hash,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'model_type': type(model).__name__\n",
    "            }\n",
    "            \n",
    "            # Adicionar metadados personalizados de forma segura\n",
    "            for key, value in metadata.items():\n",
    "                try:\n",
    "                    # Tentar serializar o valor para verificar se √© JSON-serializ√°vel\n",
    "                    json.dumps(value)\n",
    "                    serializable_metadata[key] = value\n",
    "                except (TypeError, ValueError):\n",
    "                    # Se n√£o for serializ√°vel, converter para string ou omitir\n",
    "                    if isinstance(value, (list, dict)):\n",
    "                        try:\n",
    "                            # Tentar converter elementos b√°sicos\n",
    "                            if isinstance(value, list):\n",
    "                                serializable_metadata[key] = [str(item) if not isinstance(item, (int, float, str, bool)) else item for item in value]\n",
    "                            elif isinstance(value, dict):\n",
    "                                serializable_metadata[key] = {k: str(v) if not isinstance(v, (int, float, str, bool)) else v for k, v in value.items()}\n",
    "                        except:\n",
    "                            serializable_metadata[key] = str(value)\n",
    "                    else:\n",
    "                        serializable_metadata[key] = str(value)\n",
    "            \n",
    "            # Salvar metadados serializ√°veis\n",
    "            with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(serializable_metadata, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"‚úÖ Modelo salvo: {model_name} ({model_hash})\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao salvar modelo {model_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def load_model(self, model_hash, model_id=None):\n",
    "        \"\"\"Carrega modelo salvo\"\"\"\n",
    "        try:\n",
    "            model_file = self.models_dir / f\"model_{model_hash}.pkl\"\n",
    "            metadata_file = self.models_dir / f\"metadata_{model_hash}.json\"\n",
    "            \n",
    "            if not model_file.exists():\n",
    "                return None, None\n",
    "            \n",
    "            # Carregar refer√™ncia do modelo\n",
    "            with open(model_file, 'rb') as f:\n",
    "                model_ref = pickle.load(f)\n",
    "            \n",
    "            # Carregar modelo baseado no tipo\n",
    "            if model_ref['type'] == 'keras':\n",
    "                model = load_model(model_ref['path'])\n",
    "            elif model_ref['type'] == 'joblib':\n",
    "                model = joblib.load(model_ref['path'])\n",
    "            else:  # sklearn\n",
    "                model = model_ref['model']\n",
    "            \n",
    "            # Carregar metadados\n",
    "            metadata = {}\n",
    "            if metadata_file.exists():\n",
    "                with open(metadata_file, 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "            \n",
    "            print(f\"‚úÖ Modelo carregado: {metadata.get('model_name', 'Unknown')} ({model_hash})\")\n",
    "            return model, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao carregar modelo {model_hash}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def cleanup_old_models(self, max_age_days=30):\n",
    "        \"\"\"Remove modelos antigos\"\"\"\n",
    "        try:\n",
    "            cutoff_time = datetime.now() - timedelta(days=max_age_days)\n",
    "            removed_count = 0\n",
    "            \n",
    "            for model_file in self.models_dir.glob(\"model_*.pkl\"):\n",
    "                file_time = datetime.fromtimestamp(model_file.stat().st_mtime)\n",
    "                if file_time < cutoff_time:\n",
    "                    # Remover arquivos relacionados\n",
    "                    model_hash = model_file.stem.replace('model_', '')\n",
    "                    \n",
    "                    for pattern in [f\"model_{model_hash}*\", f\"metadata_{model_hash}*\", f\"keras_model_{model_hash}\"]:\n",
    "                        for file_to_remove in self.models_dir.glob(pattern):\n",
    "                            if file_to_remove.is_file():\n",
    "                                file_to_remove.unlink()\n",
    "                            elif file_to_remove.is_dir():\n",
    "                                import shutil\n",
    "                                shutil.rmtree(file_to_remove)\n",
    "                    \n",
    "                    removed_count += 1\n",
    "            \n",
    "            if removed_count > 0:\n",
    "                print(f\"üßπ {removed_count} modelo(s) antigo(s) removido(s)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Erro na limpeza de modelos: {e}\")\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Parser de argumentos da linha de comando\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='WDO Analyzer v21 - An√°lise Avan√ßada com ML')\n",
    "    \n",
    "    # Configura√ß√£o\n",
    "    parser.add_argument('--config', type=str, help='Arquivo de configura√ß√£o (YAML/JSON)')\n",
    "    parser.add_argument('--save-config', type=str, help='Salvar configura√ß√£o atual em arquivo')\n",
    "    \n",
    "    # Modelos\n",
    "    parser.add_argument('--models', type=str, \n",
    "                       help='Modelos a usar (separados por v√≠rgula): LSTM,RandomForest,GradientBoosting,GRU,CNN_LSTM,SVR')\n",
    "    parser.add_argument('--epochs', type=int, help='N√∫mero de √©pocas para modelos de deep learning')\n",
    "    parser.add_argument('--batch-size', type=int, help='Tamanho do batch')\n",
    "    parser.add_argument('--cv-folds', type=int, help='N√∫mero de folds para valida√ß√£o cruzada')\n",
    "    \n",
    "    # Persist√™ncia\n",
    "    parser.add_argument('--no-save-models', action='store_true', help='N√£o salvar modelos treinados')\n",
    "    parser.add_argument('--force-retrain', action='store_true', help='For√ßar re-treinamento mesmo com cache v√°lido')\n",
    "    parser.add_argument('--cleanup-models', action='store_true', help='Limpar modelos antigos')\n",
    "    \n",
    "    # Dados\n",
    "    parser.add_argument('--data-dir', type=str, help='Diret√≥rio dos dados WDO')\n",
    "    parser.add_argument('--max-records', type=int, help='M√°ximo de registros a processar')\n",
    "    \n",
    "    # Sa√≠da\n",
    "    parser.add_argument('--output-dir', type=str, help='Diret√≥rio de sa√≠da para relat√≥rios')\n",
    "    parser.add_argument('--verbose', action='store_true', help='Sa√≠da detalhada')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "# Configurar localiza√ß√£o brasileira\n",
    "try:\n",
    "    locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')\n",
    "except locale.Error:\n",
    "    try:\n",
    "        locale.setlocale(locale.LC_ALL, 'Portuguese_Brazil.1252')\n",
    "    except locale.Error:\n",
    "        pass\n",
    "\n",
    "def format_currency_br(value):\n",
    "    \"\"\"Formata n√∫meros no padr√£o brasileiro: 5.688,00\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"N/A\"\n",
    "    \n",
    "    formatted = f\"{value:.2f}\"\n",
    "    parts = formatted.split('.')\n",
    "    integer_part = parts[0]\n",
    "    decimal_part = parts[1]\n",
    "    \n",
    "    if len(integer_part) > 3:\n",
    "        reversed_int = integer_part[::-1]\n",
    "        formatted_int = '.'.join([reversed_int[i:i+3] for i in range(0, len(reversed_int), 3)])\n",
    "        integer_part = formatted_int[::-1]\n",
    "    \n",
    "    return f\"{integer_part},{decimal_part}\"\n",
    "\n",
    "def format_volume_br(value):\n",
    "    \"\"\"Formata volume no padr√£o brasileiro\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"N/A\"\n",
    "    \n",
    "    formatted = f\"{int(value)}\"\n",
    "    \n",
    "    if len(formatted) > 3:\n",
    "        reversed_int = formatted[::-1]\n",
    "        formatted_int = '.'.join([reversed_int[i:i+3] for i in range(0, len(reversed_int), 3)])\n",
    "        formatted = formatted_int[::-1]\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# BUSCA DADOS REAIS\n",
    "DATA_DIR_OPTIONS = [\n",
    "    \"C://Users//thiag//OneDrive//ARQUIVOS//Bolsa//COLETA//MT//FUTUROS\", \n",
    "    \"./data\",\n",
    "    \"./\"\n",
    "]\n",
    "\n",
    "def find_wdo_files(custom_dir=None):\n",
    "    \"\"\"Encontra arquivos WDO reais\"\"\"\n",
    "    print(\"üîç Procurando arquivos WDO reais...\")\n",
    "    \n",
    "    search_dirs = [custom_dir] if custom_dir else DATA_DIR_OPTIONS\n",
    "    \n",
    "    for data_dir in search_dirs:\n",
    "        if data_dir and os.path.exists(data_dir):\n",
    "            wdo_files = glob.glob(os.path.join(data_dir, 'WDO*.csv'))\n",
    "            if wdo_files:\n",
    "                print(f\"üìÅ Encontrado: {data_dir}\")\n",
    "                print(f\"üìä Arquivos WDO: {len(wdo_files)}\")\n",
    "                return data_dir, wdo_files\n",
    "    \n",
    "    print(\"‚ùå NENHUM ARQUIVO WDO ENCONTRADO!\")\n",
    "    print(\"üí° Verifique se os arquivos est√£o em:\")\n",
    "    for dir_option in DATA_DIR_OPTIONS:\n",
    "        print(f\"   - {dir_option}\")\n",
    "    \n",
    "    return None, []\n",
    "\n",
    "print(\"üöÄ WDO GERADOR v21 - PARTE 1/6 CARREGADA\")\n",
    "print(\"‚úÖ Configura√ß√µes customiz√°veis implementadas\")\n",
    "print(\"‚úÖ Sistema de persist√™ncia de modelos criado\") \n",
    "print(\"‚úÖ Parser de argumentos configurado\")\n",
    "print(\"\\nüìã Pr√≥ximo: Execute a Parte 2/6 para continuar...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94882e60-806d-4069-8f65-34261a02ad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ WDO GERADOR v21 - PARTE 2/6 CARREGADA\n",
      "‚úÖ TimeSeriesValidator implementado\n",
      "‚úÖ AdvancedMLPredictor com novos modelos criado\n",
      "‚úÖ Valida√ß√£o cruzada temporal configurada\n",
      "‚úÖ Prepara√ß√£o avan√ßada de features baseada em config\n",
      "\n",
      "üìã Pr√≥ximo: Execute a Parte 3/6 para continuar...\n"
     ]
    }
   ],
   "source": [
    "# WDO GERADOR v21 - PARTE 2/6\n",
    "# VALIDA√á√ÉO CRUZADA TEMPORAL E MODELOS ML AVAN√áADOS\n",
    "# Melhorias: Cross-validation, novos modelos, hiperpar√¢metros customiz√°veis\n",
    "\n",
    "class TimeSeriesValidator:\n",
    "    \"\"\"Valida√ß√£o cruzada espec√≠fica para s√©ries temporais\"\"\"\n",
    "    \n",
    "    def __init__(self, config_manager):\n",
    "        self.config = config_manager\n",
    "        self.cv_folds = config_manager.get('validation.cv_folds', 5)\n",
    "        self.test_size = config_manager.get('validation.test_size', 0.2)\n",
    "        self.use_cv = config_manager.get('validation.use_cross_validation', True)\n",
    "        self.time_series_split = config_manager.get('validation.time_series_split', True)\n",
    "    \n",
    "    def create_time_series_splits(self, data_length, n_splits=None):\n",
    "        \"\"\"Cria splits temporais respeitando ordem cronol√≥gica\"\"\"\n",
    "        if n_splits is None:\n",
    "            n_splits = self.cv_folds\n",
    "        \n",
    "        if self.time_series_split:\n",
    "            # TimeSeriesSplit para dados temporais\n",
    "            tscv = TimeSeriesSplit(n_splits=n_splits, test_size=None)\n",
    "            return list(tscv.split(range(data_length)))\n",
    "        else:\n",
    "            # Split simples mantendo ordem temporal\n",
    "            test_size = int(data_length * self.test_size)\n",
    "            train_size = data_length - test_size\n",
    "            \n",
    "            splits = []\n",
    "            step_size = train_size // n_splits\n",
    "            \n",
    "            for i in range(n_splits):\n",
    "                train_start = i * step_size\n",
    "                train_end = train_start + train_size - test_size\n",
    "                test_start = train_end\n",
    "                test_end = min(test_start + test_size, data_length)\n",
    "                \n",
    "                if test_end > test_start:\n",
    "                    train_idx = list(range(train_start, train_end))\n",
    "                    test_idx = list(range(test_start, test_end))\n",
    "                    splits.append((train_idx, test_idx))\n",
    "            \n",
    "            return splits\n",
    "    \n",
    "    def validate_model_performance(self, model_func, X, y, model_name=\"Model\"):\n",
    "        \"\"\"Valida performance do modelo com cross-validation temporal\"\"\"\n",
    "        if not self.use_cv:\n",
    "            # Valida√ß√£o simples\n",
    "            split_idx = int(len(X) * (1 - self.test_size))\n",
    "            X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "            y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "            \n",
    "            model = model_func(X_train, y_train)\n",
    "            predictions = self._predict_model(model, X_test)\n",
    "            \n",
    "            metrics = self._calculate_metrics(y_test, predictions)\n",
    "            \n",
    "            return {\n",
    "                'cv_scores': [metrics['mae']],\n",
    "                'mean_mae': metrics['mae'],\n",
    "                'std_mae': 0.0,\n",
    "                'mean_rmse': metrics['rmse'],\n",
    "                'std_rmse': 0.0,\n",
    "                'mean_r2': metrics['r2'],\n",
    "                'std_r2': 0.0,\n",
    "                'best_model': model,\n",
    "                'fold_results': [metrics]\n",
    "            }\n",
    "        \n",
    "        # Valida√ß√£o cruzada temporal\n",
    "        splits = self.create_time_series_splits(len(X))\n",
    "        \n",
    "        cv_scores = []\n",
    "        rmse_scores = []\n",
    "        r2_scores = []\n",
    "        fold_results = []\n",
    "        best_model = None\n",
    "        best_mae = float('inf')\n",
    "        \n",
    "        print(f\"üîÑ Valida√ß√£o cruzada temporal: {len(splits)} folds para {model_name}\")\n",
    "        \n",
    "        for fold, (train_idx, test_idx) in enumerate(splits, 1):\n",
    "            try:\n",
    "                X_train = X[train_idx] if isinstance(X, np.ndarray) else X.iloc[train_idx]\n",
    "                X_test = X[test_idx] if isinstance(X, np.ndarray) else X.iloc[test_idx]\n",
    "                y_train = y[train_idx] if isinstance(y, np.ndarray) else y.iloc[train_idx]\n",
    "                y_test = y[test_idx] if isinstance(y, np.ndarray) else y.iloc[test_idx]\n",
    "                \n",
    "                if len(X_train) < 10 or len(X_test) < 5:\n",
    "                    continue\n",
    "                \n",
    "                # Treinar modelo para este fold com tratamento especial para CatBoost\n",
    "                if model_name == 'CatBoost':\n",
    "                    # Criar um conjunto de valida√ß√£o a partir dos dados de treino\n",
    "                    from sklearn.model_selection import train_test_split\n",
    "                    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42+fold)\n",
    "                    \n",
    "                    # Criar o modelo\n",
    "                    model = model_func(X_tr, y_tr)\n",
    "                    \n",
    "                    # Se for CatBoost, fornecer o eval_set\n",
    "                    if hasattr(model, 'get_params') and 'use_best_model' in model.get_params():\n",
    "                        # Verifica se o modelo tem o par√¢metro use_best_model\n",
    "                        try:\n",
    "                            model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "                        except Exception as fit_error:\n",
    "                            print(f\"   ‚ö†Ô∏è Erro ao treinar CatBoost com eval_set: {fit_error}\")\n",
    "                            # Fallback: tentar treinar sem usar eval_set\n",
    "                            params = model.get_params()\n",
    "                            params['use_best_model'] = False\n",
    "                            model.set_params(**params)\n",
    "                            model.fit(X_train, y_train)\n",
    "                    else:\n",
    "                        model.fit(X_train, y_train)\n",
    "                else:\n",
    "                    # Para todos os outros modelos, treinar normalmente\n",
    "                    model = model_func(X_train, y_train)\n",
    "                \n",
    "                # Fazer predi√ß√µes\n",
    "                predictions = self._predict_model(model, X_test)\n",
    "                \n",
    "                if predictions is not None and len(predictions) > 0:\n",
    "                    # Calcular m√©tricas\n",
    "                    metrics = self._calculate_metrics(y_test, predictions)\n",
    "                    \n",
    "                    cv_scores.append(metrics['mae'])\n",
    "                    rmse_scores.append(metrics['rmse'])\n",
    "                    r2_scores.append(metrics['r2'])\n",
    "                    fold_results.append(metrics)\n",
    "                    \n",
    "                    # Salvar melhor modelo\n",
    "                    if metrics['mae'] < best_mae:\n",
    "                        best_mae = metrics['mae']\n",
    "                        best_model = model\n",
    "                    \n",
    "                    print(f\"   Fold {fold}: MAE={metrics['mae']:.6f}, RMSE={metrics['rmse']:.6f}, R¬≤={metrics['r2']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Erro no fold {fold}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not cv_scores:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'cv_scores': cv_scores,\n",
    "            'mean_mae': np.mean(cv_scores),\n",
    "            'std_mae': np.std(cv_scores),\n",
    "            'mean_rmse': np.mean(rmse_scores),\n",
    "            'std_rmse': np.std(rmse_scores),\n",
    "            'mean_r2': np.mean(r2_scores),\n",
    "            'std_r2': np.std(r2_scores),\n",
    "            'best_model': best_model,\n",
    "            'fold_results': fold_results\n",
    "        }\n",
    "    \n",
    "    def _predict_model(self, model, X_test):\n",
    "        \"\"\"Faz predi√ß√µes baseado no tipo de modelo\"\"\"\n",
    "        try:\n",
    "            if hasattr(model, 'predict'):\n",
    "                predictions = model.predict(X_test)\n",
    "                \n",
    "                # Se √© modelo Keras, pode retornar array 2D\n",
    "                if len(predictions.shape) > 1 and predictions.shape[1] == 1:\n",
    "                    predictions = predictions.flatten()\n",
    "                \n",
    "                return predictions\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Erro na predi√ß√£o: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calcula m√©tricas de avalia√ß√£o\"\"\"\n",
    "        try:\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            \n",
    "            return {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100 if np.all(y_true != 0) else 0\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'mae': float('inf'), 'rmse': float('inf'), 'r2': -1, 'mape': float('inf')}\n",
    "\n",
    "class AdvancedMLPredictor:\n",
    "    \"\"\"Preditor ML avan√ßado com m√∫ltiplos modelos e valida√ß√£o cruzada\"\"\"\n",
    "\n",
    "\n",
    "    def create_lstm_bidirectional_model(self, input_shape, model_config):\n",
    "        \"\"\"Cria modelo LSTM Bidirecional dedicado\"\"\"\n",
    "        if not DEEP_LEARNING_AVAILABLE:\n",
    "            return None\n",
    "        \n",
    "        units = model_config.get('units', [128, 64, 32])\n",
    "        dropout = model_config.get('dropout', 0.25)\n",
    "        recurrent_dropout = model_config.get('recurrent_dropout', 0.15)\n",
    "        learning_rate = model_config.get('learning_rate', 0.0008)\n",
    "        optimizer_name = model_config.get('optimizer', 'adam')\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Primeira camada LSTM Bidirecional\n",
    "        model.add(Bidirectional(LSTM(\n",
    "            units[0], \n",
    "            return_sequences=len(units) > 1,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            input_shape=input_shape\n",
    "        )))\n",
    "        \n",
    "        # Camadas LSTM Bidirecionais adicionais\n",
    "        for i, unit_count in enumerate(units[1:], 1):\n",
    "            return_sequences = i < len(units) - 1\n",
    "            \n",
    "            model.add(Bidirectional(LSTM(\n",
    "                unit_count,\n",
    "                return_sequences=return_sequences,\n",
    "                dropout=dropout,\n",
    "                recurrent_dropout=recurrent_dropout\n",
    "            )))\n",
    "        \n",
    "        # Camadas densas\n",
    "        model.add(Dense(units[-1] // 2, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(units[-1] // 4, activation='relu'))\n",
    "        model.add(Dropout(dropout / 2))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        # Configurar otimizador\n",
    "        optimizers = {\n",
    "            'adam': Adam(learning_rate=learning_rate),\n",
    "            'rmsprop': RMSprop(learning_rate=learning_rate),\n",
    "            'sgd': SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "        }\n",
    "        \n",
    "        optimizer = optimizers.get(optimizer_name, Adam(learning_rate=learning_rate))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    \n",
    "    def ensure_required_features(self, df, required_count=20):\n",
    "        \"\"\"Garante que o DataFrame tenha o n√∫mero necess√°rio de features\"\"\"\n",
    "        try:\n",
    "            current_features = [col for col in df.columns if col not in ['DATA', 'VOL.'] and df[col].dtype in ['int64', 'float64']]\n",
    "            \n",
    "            if len(current_features) >= required_count:\n",
    "                return df\n",
    "            \n",
    "            print(f\"üîß Criando features adicionais... ({len(current_features)}/{required_count})\")\n",
    "            \n",
    "            df_work = df.copy()\n",
    "            \n",
    "            # 1. Features b√°sicas de pre√ßo\n",
    "            if 'price_change' not in df_work.columns:\n",
    "                df_work['price_change'] = df_work['√öLT. PRE√áO'].pct_change().fillna(0)\n",
    "            \n",
    "            if 'vol_ratio' not in df_work.columns and 'VOL.' in df_work.columns:\n",
    "                vol_ma = df_work['VOL.'].rolling(20, min_periods=1).mean()\n",
    "                df_work['vol_ratio'] = df_work['VOL.'] / vol_ma\n",
    "                df_work['vol_ratio'] = df_work['vol_ratio'].fillna(1)\n",
    "            \n",
    "            # 2. M√©dias m√≥veis simples\n",
    "            for period in [5, 10, 20, 50]:\n",
    "                col_name = f'sma_{period}'\n",
    "                if col_name not in df_work.columns:\n",
    "                    df_work[col_name] = df_work['√öLT. PRE√áO'].rolling(period, min_periods=1).mean()\n",
    "                \n",
    "                ratio_col = f'price_vs_sma_{period}'\n",
    "                if ratio_col not in df_work.columns:\n",
    "                    df_work[ratio_col] = df_work['√öLT. PRE√áO'] / df_work[col_name] - 1\n",
    "            \n",
    "            # 3. RSI\n",
    "            if 'rsi' not in df_work.columns:\n",
    "                delta = df_work['√öLT. PRE√áO'].diff()\n",
    "                gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=1).mean()\n",
    "                loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=1).mean()\n",
    "                rs = gain / (loss + 1e-10)\n",
    "                df_work['rsi'] = 100 - (100 / (1 + rs))\n",
    "                df_work['rsi'] = df_work['rsi'].fillna(50)\n",
    "            \n",
    "            # 4. Volatilidade\n",
    "            for period in [5, 10, 20]:\n",
    "                col_name = f'volatility_{period}'\n",
    "                if col_name not in df_work.columns:\n",
    "                    returns = df_work['√öLT. PRE√áO'].pct_change()\n",
    "                    df_work[col_name] = returns.rolling(period, min_periods=1).std().fillna(0.01)\n",
    "            \n",
    "            # 5. Momentum\n",
    "            for period in [3, 5, 10, 20]:\n",
    "                col_name = f'momentum_{period}'\n",
    "                if col_name not in df_work.columns:\n",
    "                    df_work[col_name] = df_work['√öLT. PRE√áO'].pct_change(periods=period).fillna(0)\n",
    "            \n",
    "            # 6. Features de lag se ainda precisarmos\n",
    "            current_features = [col for col in df_work.columns if col not in ['DATA', 'VOL.'] and df_work[col].dtype in ['int64', 'float64']]\n",
    "            \n",
    "            if len(current_features) < required_count:\n",
    "                needed = required_count - len(current_features)\n",
    "                for i in range(1, needed + 1):\n",
    "                    lag_col = f'price_lag_{i}'\n",
    "                    df_work[lag_col] = df_work['√öLT. PRE√áO'].shift(i).fillna(df_work['√öLT. PRE√áO'])\n",
    "            \n",
    "            # 7. Limpeza final\n",
    "            df_work = df_work.replace([np.inf, -np.inf], np.nan)\n",
    "            df_work = df_work.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "            \n",
    "            final_features = [col for col in df_work.columns if col not in ['DATA', 'VOL.'] and df_work[col].dtype in ['int64', 'float64']]\n",
    "            print(f\"‚úÖ Features criadas: {len(final_features)} total\")\n",
    "            \n",
    "            return df_work\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na cria√ß√£o de features: {e}\")\n",
    "            return df\n",
    "    \n",
    "    def __init__(self, config_manager):\n",
    "        self.config = config_manager\n",
    "        self.validator = TimeSeriesValidator(config_manager)\n",
    "        self.persistence = ModelPersistence(config_manager)\n",
    "        \n",
    "        # Scalers baseados na configura√ß√£o\n",
    "        self.scalers = {\n",
    "            'minmax': MinMaxScaler(),\n",
    "            'standard': StandardScaler(),\n",
    "            'robust': RobustScaler()\n",
    "        }\n",
    "        \n",
    "        self.feature_scaler = self.scalers['minmax']  # Padr√£o\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        \n",
    "        self.models = {}\n",
    "        self.model_configs = {}\n",
    "        self.trained_models = {}\n",
    "        \n",
    "        # Configurar modelos dispon√≠veis\n",
    "        self._setup_model_configs()\n",
    "    \n",
    "    def _setup_model_configs(self):\n",
    "        \"\"\"Configura modelos baseado na configura√ß√£o\"\"\"\n",
    "        enabled_models = self.config.get('ml_models.enabled_models', ['LSTM', 'RandomForest'])\n",
    "        \n",
    "        for model_name in enabled_models:\n",
    "            model_key = model_name.lower().replace('_', '')\n",
    "            self.model_configs[model_name] = self.config.get(f'ml_models.{model_key}', {})\n",
    "    \n",
    "    def prepare_features_advanced(self, df, required_features=20):\n",
    "        \"\"\"Prepara√ß√£o avan√ßada de features baseada na configura√ß√£o\"\"\"\n",
    "        print(f\"üîß Prepara√ß√£o avan√ßada de features para {len(df)} registros...\")\n",
    "        \n",
    "        df = df.copy()\n",
    "        \n",
    "        # Verificar colunas essenciais\n",
    "        if '√öLT. PRE√áO' not in df.columns or 'VOL.' not in df.columns:\n",
    "            print(\"‚ùå Colunas essenciais ausentes\")\n",
    "            return None\n",
    "        \n",
    "        # Features t√©cnicas configur√°veis\n",
    "        tech_config = self.config.get('features.technical_indicators', {})\n",
    "        \n",
    "        # SMAs\n",
    "        sma_periods = tech_config.get('sma_periods', [5, 10, 20, 50])\n",
    "        for period in sma_periods:\n",
    "            if len(df) > period:\n",
    "                df[f'sma_{period}'] = df['√öLT. PRE√áO'].rolling(window=period, min_periods=1).mean()\n",
    "                df[f'price_vs_sma_{period}'] = df['√öLT. PRE√áO'] / df[f'sma_{period}'] - 1\n",
    "            else:\n",
    "                df[f'sma_{period}'] = df['√öLT. PRE√áO']\n",
    "                df[f'price_vs_sma_{period}'] = 0\n",
    "        \n",
    "        # EMAs\n",
    "        ema_periods = tech_config.get('ema_periods', [5, 10, 20])\n",
    "        for period in ema_periods:\n",
    "            if len(df) > period:\n",
    "                df[f'ema_{period}'] = df['√öLT. PRE√áO'].ewm(span=period, min_periods=1).mean()\n",
    "                df[f'price_vs_ema_{period}'] = df['√öLT. PRE√áO'] / df[f'ema_{period}'] - 1\n",
    "            else:\n",
    "                df[f'ema_{period}'] = df['√öLT. PRE√áO']\n",
    "                df[f'price_vs_ema_{period}'] = 0\n",
    "        \n",
    "        # RSI\n",
    "        rsi_period = tech_config.get('rsi_period', 14)\n",
    "        if len(df) > rsi_period:\n",
    "            delta = df['√öLT. PRE√áO'].diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window=rsi_period, min_periods=1).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=rsi_period, min_periods=1).mean()\n",
    "            rs = gain / (loss + 1e-10)\n",
    "            df['rsi'] = 100 - (100 / (1 + rs))\n",
    "            df['rsi'] = df['rsi'].fillna(50)\n",
    "        else:\n",
    "            df['rsi'] = 50\n",
    "        \n",
    "        # MACD\n",
    "        macd_fast = tech_config.get('macd_fast', 12)\n",
    "        macd_slow = tech_config.get('macd_slow', 26)\n",
    "        macd_signal = tech_config.get('macd_signal', 9)\n",
    "        \n",
    "        if len(df) > macd_slow:\n",
    "            ema_fast = df['√öLT. PRE√áO'].ewm(span=macd_fast).mean()\n",
    "            ema_slow = df['√öLT. PRE√áO'].ewm(span=macd_slow).mean()\n",
    "            df['macd'] = ema_fast - ema_slow\n",
    "            df['macd_signal'] = df['macd'].ewm(span=macd_signal).mean()\n",
    "            df['macd_histogram'] = df['macd'] - df['macd_signal']\n",
    "        else:\n",
    "            df['macd'] = 0\n",
    "            df['macd_signal'] = 0\n",
    "            df['macd_histogram'] = 0\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bb_period = tech_config.get('bollinger_period', 20)\n",
    "        bb_std = tech_config.get('bollinger_std', 2)\n",
    "        \n",
    "        if len(df) > bb_period:\n",
    "            bb_ma = df['√öLT. PRE√áO'].rolling(window=bb_period).mean()\n",
    "            bb_std_val = df['√öLT. PRE√áO'].rolling(window=bb_period).std()\n",
    "            df['bb_upper'] = bb_ma + (bb_std_val * bb_std)\n",
    "            df['bb_lower'] = bb_ma - (bb_std_val * bb_std)\n",
    "            df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / bb_ma\n",
    "            df['bb_position'] = (df['√öLT. PRE√áO'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])\n",
    "        else:\n",
    "            df['bb_upper'] = df['√öLT. PRE√áO'] * 1.02\n",
    "            df['bb_lower'] = df['√öLT. PRE√áO'] * 0.98\n",
    "            df['bb_width'] = 0.04\n",
    "            df['bb_position'] = 0.5\n",
    "        \n",
    "        # Features customizadas\n",
    "        custom_config = self.config.get('features.custom_features', {})\n",
    "        \n",
    "        # Momentum de pre√ßo\n",
    "        momentum_periods = custom_config.get('price_momentum_periods', [5, 10, 20])\n",
    "        for period in momentum_periods:\n",
    "            if len(df) > period:\n",
    "                df[f'momentum_{period}'] = df['√öLT. PRE√áO'] / df['√öLT. PRE√áO'].shift(period) - 1\n",
    "            else:\n",
    "                df[f'momentum_{period}'] = 0\n",
    "        \n",
    "        # Momentum de volume\n",
    "        vol_momentum_periods = custom_config.get('volume_momentum_periods', [5, 10])\n",
    "        for period in vol_momentum_periods:\n",
    "            if len(df) > period:\n",
    "                df['vol_ma'] = df['VOL.'].rolling(window=period, min_periods=1).mean()\n",
    "                df[f'vol_momentum_{period}'] = df['VOL.'] / df['vol_ma'] - 1\n",
    "            else:\n",
    "                df[f'vol_momentum_{period}'] = 0\n",
    "        \n",
    "        # Volatilidade\n",
    "        vol_periods = custom_config.get('volatility_periods', [10, 20, 30])\n",
    "        for period in vol_periods:\n",
    "            if len(df) > period:\n",
    "                returns = df['√öLT. PRE√áO'].pct_change()\n",
    "                df[f'volatility_{period}'] = returns.rolling(window=period, min_periods=1).std() * np.sqrt(252)\n",
    "            else:\n",
    "                df[f'volatility_{period}'] = 0.01\n",
    "        \n",
    "        # Williams %R\n",
    "        williams_period = tech_config.get('williams_r_period', 14)\n",
    "        if len(df) > williams_period and all(col in df.columns for col in ['PRE√áO M√ÅX.', 'PRE√áO M√çN.']):\n",
    "            highest_high = df['PRE√áO M√ÅX.'].rolling(window=williams_period).max()\n",
    "            lowest_low = df['PRE√áO M√çN.'].rolling(window=williams_period).min()\n",
    "            df['williams_r'] = -100 * (highest_high - df['√öLT. PRE√áO']) / (highest_high - lowest_low)\n",
    "        else:\n",
    "            df['williams_r'] = -50\n",
    "        \n",
    "        # Stochastic\n",
    "        stoch_k_period = tech_config.get('stoch_k', 14)\n",
    "        stoch_d_period = tech_config.get('stoch_d', 3)\n",
    "        \n",
    "        if len(df) > stoch_k_period and all(col in df.columns for col in ['PRE√áO M√ÅX.', 'PRE√áO M√çN.']):\n",
    "            lowest_low = df['PRE√áO M√çN.'].rolling(window=stoch_k_period).min()\n",
    "            highest_high = df['PRE√áO M√ÅX.'].rolling(window=stoch_k_period).max()\n",
    "            df['stoch_k'] = 100 * (df['√öLT. PRE√áO'] - lowest_low) / (highest_high - lowest_low)\n",
    "            df['stoch_d'] = df['stoch_k'].rolling(window=stoch_d_period).mean()\n",
    "        else:\n",
    "            df['stoch_k'] = 50\n",
    "            df['stoch_d'] = 50\n",
    "        \n",
    "        # Limpeza final\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "        \n",
    "        print(f\"‚úÖ Features avan√ßadas preparadas: {df.shape}\")\n",
    "        return df.dropna()\n",
    "    \n",
    "    def create_lstm_model_advanced(self, input_shape, model_config):\n",
    "        \"\"\"Cria modelo LSTM avan√ßado baseado na configura√ß√£o\"\"\"\n",
    "        if not DEEP_LEARNING_AVAILABLE:\n",
    "            return None\n",
    "        \n",
    "        units = model_config.get('units', [64, 32])\n",
    "        dropout = model_config.get('dropout', 0.2)\n",
    "        recurrent_dropout = model_config.get('recurrent_dropout', 0.1)\n",
    "        learning_rate = model_config.get('learning_rate', 0.001)\n",
    "        optimizer_name = model_config.get('optimizer', 'adam')\n",
    "        bidirectional = model_config.get('bidirectional', False)\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Primeira camada LSTM\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(LSTM(\n",
    "                units[0], \n",
    "                return_sequences=len(units) > 1,\n",
    "                dropout=dropout,\n",
    "                recurrent_dropout=recurrent_dropout,\n",
    "                input_shape=input_shape\n",
    "            )))\n",
    "        else:\n",
    "            model.add(LSTM(\n",
    "                units[0], \n",
    "                return_sequences=len(units) > 1,\n",
    "                dropout=dropout,\n",
    "                recurrent_dropout=recurrent_dropout,\n",
    "                input_shape=input_shape\n",
    "            ))\n",
    "        \n",
    "        # Camadas LSTM adicionais\n",
    "        for i, unit_count in enumerate(units[1:], 1):\n",
    "            return_sequences = i < len(units) - 1\n",
    "            \n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(LSTM(\n",
    "                    unit_count,\n",
    "                    return_sequences=return_sequences,\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                )))\n",
    "            else:\n",
    "                model.add(LSTM(\n",
    "                    unit_count,\n",
    "                    return_sequences=return_sequences,\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                ))\n",
    "        \n",
    "        # Camadas densas\n",
    "        model.add(Dense(units[-1] // 2, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        # Configurar otimizador\n",
    "        optimizers = {\n",
    "            'adam': Adam(learning_rate=learning_rate),\n",
    "            'rmsprop': RMSprop(learning_rate=learning_rate),\n",
    "            'sgd': SGD(learning_rate=learning_rate)\n",
    "        }\n",
    "        \n",
    "        optimizer = optimizers.get(optimizer_name, Adam(learning_rate=learning_rate))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_gru_model(self, input_shape, model_config):\n",
    "        \"\"\"Cria modelo GRU baseado na configura√ß√£o\"\"\"\n",
    "        if not DEEP_LEARNING_AVAILABLE:\n",
    "            return None\n",
    "        \n",
    "        units = model_config.get('units', [64, 32])\n",
    "        dropout = model_config.get('dropout', 0.2)\n",
    "        recurrent_dropout = model_config.get('recurrent_dropout', 0.1)\n",
    "        learning_rate = model_config.get('learning_rate', 0.001)\n",
    "        optimizer_name = model_config.get('optimizer', 'adam')\n",
    "        bidirectional = model_config.get('bidirectional', True)\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Primeira camada GRU\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(GRU(\n",
    "                units[0], \n",
    "                return_sequences=len(units) > 1,\n",
    "                dropout=dropout,\n",
    "                recurrent_dropout=recurrent_dropout,\n",
    "                input_shape=input_shape\n",
    "            )))\n",
    "        else:\n",
    "            model.add(GRU(\n",
    "                units[0], \n",
    "                return_sequences=len(units) > 1,\n",
    "                dropout=dropout,\n",
    "                recurrent_dropout=recurrent_dropout,\n",
    "                input_shape=input_shape\n",
    "            ))\n",
    "        \n",
    "        # Camadas GRU adicionais\n",
    "        for i, unit_count in enumerate(units[1:], 1):\n",
    "            return_sequences = i < len(units) - 1\n",
    "            \n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(GRU(\n",
    "                    unit_count,\n",
    "                    return_sequences=return_sequences,\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                )))\n",
    "            else:\n",
    "                model.add(GRU(\n",
    "                    unit_count,\n",
    "                    return_sequences=return_sequences,\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                ))\n",
    "        \n",
    "        # Camadas densas\n",
    "        model.add(Dense(units[-1] // 2, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        # Configurar otimizador\n",
    "        optimizers = {\n",
    "            'adam': Adam(learning_rate=learning_rate),\n",
    "            'rmsprop': RMSprop(learning_rate=learning_rate),\n",
    "            'sgd': SGD(learning_rate=learning_rate)\n",
    "        }\n",
    "        \n",
    "        optimizer = optimizers.get(optimizer_name, Adam(learning_rate=learning_rate))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_cnn_lstm_model_advanced(self, input_shape, model_config):\n",
    "        \"\"\"Cria modelo CNN-LSTM avan√ßado\"\"\"\n",
    "        if not DEEP_LEARNING_AVAILABLE or input_shape[0] < 8:\n",
    "            return None\n",
    "        \n",
    "        conv_filters = model_config.get('conv_filters', [64, 32])\n",
    "        conv_kernel_size = model_config.get('conv_kernel_size', 3)\n",
    "        lstm_units = model_config.get('lstm_units', 50)\n",
    "        dropout = model_config.get('dropout', 0.2)\n",
    "        learning_rate = model_config.get('learning_rate', 0.001)\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Camadas convolucionais\n",
    "        for i, filters in enumerate(conv_filters):\n",
    "            if i == 0:\n",
    "                model.add(Conv1D(\n",
    "                    filters=filters,\n",
    "                    kernel_size=conv_kernel_size,\n",
    "                    activation='relu',\n",
    "                    input_shape=input_shape\n",
    "                ))\n",
    "            else:\n",
    "                model.add(Conv1D(\n",
    "                    filters=filters,\n",
    "                    kernel_size=conv_kernel_size,\n",
    "                    activation='relu'\n",
    "                ))\n",
    "            model.add(Dropout(dropout))\n",
    "        \n",
    "        # Camada LSTM\n",
    "        model.add(LSTM(lstm_units, dropout=dropout))\n",
    "        model.add(Dense(lstm_units // 2, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_sklearn_model(self, model_name, model_config):\n",
    "        \"\"\"Cria modelos scikit-learn baseados na configura√ß√£o\"\"\"\n",
    "        if not DEEP_LEARNING_AVAILABLE:\n",
    "            return None\n",
    "        \n",
    "        if model_name == 'RandomForest':\n",
    "            return RandomForestRegressor(\n",
    "                n_estimators=model_config.get('n_estimators', 200),\n",
    "                max_depth=model_config.get('max_depth', 15),\n",
    "                min_samples_split=model_config.get('min_samples_split', 5),\n",
    "                min_samples_leaf=model_config.get('min_samples_leaf', 2),\n",
    "                max_features=model_config.get('max_features', 'sqrt'),\n",
    "                random_state=model_config.get('random_state', 42),\n",
    "                n_jobs=model_config.get('n_jobs', -1)\n",
    "            )\n",
    "\n",
    "        elif model_name == 'CatBoost':                \n",
    "            catboost_config = model_config.copy()\n",
    "            catboost_config['use_best_model'] = False  # Desabilitar para evitar o erro\n",
    "                \n",
    "            return cb.CatBoostRegressor(\n",
    "                iterations=catboost_config.get('iterations', 500),\n",
    "                learning_rate=catboost_config.get('learning_rate', 0.05),\n",
    "                depth=catboost_config.get('depth', 6),\n",
    "                l2_leaf_reg=catboost_config.get('l2_leaf_reg', 3),\n",
    "                border_count=catboost_config.get('border_count', 128),\n",
    "                random_state=catboost_config.get('random_state', 42),\n",
    "                verbose=catboost_config.get('verbose', False),\n",
    "                early_stopping_rounds=catboost_config.get('early_stopping_rounds', 50),\n",
    "                use_best_model=False,  # Sempre False para evitar o erro\n",
    "                task_type=catboost_config.get('task_type', 'CPU')\n",
    "            )\n",
    "        \n",
    "        elif model_name == 'XGBoost':\n",
    "            return xgb.XGBRegressor(\n",
    "                n_estimators=model_config.get('n_estimators', 200),\n",
    "                learning_rate=model_config.get('learning_rate', 0.05),\n",
    "                max_depth=model_config.get('max_depth', 6),\n",
    "                min_child_weight=model_config.get('min_child_weight', 1),\n",
    "                subsample=model_config.get('subsample', 0.8),\n",
    "                colsample_bytree=model_config.get('colsample_bytree', 0.8),\n",
    "                tree_method=model_config.get('tree_method', 'hist'),\n",
    "                random_state=model_config.get('random_state', 42)\n",
    "            )\n",
    "        \n",
    "        elif model_name == 'LightGBM':\n",
    "            return lgb.LGBMRegressor(\n",
    "                n_estimators=model_config.get('n_estimators', 200),\n",
    "                learning_rate=model_config.get('learning_rate', 0.05),\n",
    "                max_depth=model_config.get('max_depth', 6),\n",
    "                num_leaves=model_config.get('num_leaves', 31),\n",
    "                subsample=model_config.get('subsample', 0.8),\n",
    "                colsample_bytree=model_config.get('colsample_bytree', 0.8),\n",
    "                random_state=model_config.get('random_state', 42),\n",
    "                n_jobs=model_config.get('n_jobs', -1)\n",
    "            )\n",
    "        \n",
    "        elif model_name == 'GradientBoosting':\n",
    "            return GradientBoostingRegressor(\n",
    "                n_estimators=model_config.get('n_estimators', 200),\n",
    "                learning_rate=model_config.get('learning_rate', 0.1),\n",
    "                max_depth=model_config.get('max_depth', 6),\n",
    "                min_samples_split=model_config.get('min_samples_split', 5),\n",
    "                min_samples_leaf=model_config.get('min_samples_leaf', 2),\n",
    "                subsample=model_config.get('subsample', 0.8),\n",
    "                random_state=model_config.get('random_state', 42)\n",
    "            )\n",
    "        \n",
    "        elif model_name == 'ExtraTrees':\n",
    "            return ExtraTreesRegressor(\n",
    "                n_estimators=model_config.get('n_estimators', 200),\n",
    "                max_depth=model_config.get('max_depth', 15),\n",
    "                min_samples_split=model_config.get('min_samples_split', 5),\n",
    "                min_samples_leaf=model_config.get('min_samples_leaf', 2),\n",
    "                max_features=model_config.get('max_features', 'sqrt'),\n",
    "                random_state=model_config.get('random_state', 42),\n",
    "                n_jobs=model_config.get('n_jobs', -1)\n",
    "            )\n",
    "        \n",
    "        elif model_name == 'SVR':\n",
    "            return SVR(\n",
    "                C=model_config.get('C', 100),\n",
    "                gamma=model_config.get('gamma', 'scale'),\n",
    "                kernel=model_config.get('kernel', 'rbf'),\n",
    "                epsilon=model_config.get('epsilon', 0.01)\n",
    "            )\n",
    "        \n",
    "        return None\n",
    "\n",
    "print(\"üöÄ WDO GERADOR v21 - PARTE 2/6 CARREGADA\")\n",
    "print(\"‚úÖ TimeSeriesValidator implementado\")\n",
    "print(\"‚úÖ AdvancedMLPredictor com novos modelos criado\")\n",
    "print(\"‚úÖ Valida√ß√£o cruzada temporal configurada\")\n",
    "print(\"‚úÖ Prepara√ß√£o avan√ßada de features baseada em config\")\n",
    "print(\"\\nüìã Pr√≥ximo: Execute a Parte 3/6 para continuar...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8092f637-12ef-4f4a-945e-640ab4a361e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ WDO GERADOR v21 - PARTE 3/6 CARREGADA\n",
      "‚úÖ SupportResistanceClusterAnalyzer implementado\n",
      "‚úÖ IntelligentModelTrainer com cache inteligente criado\n",
      "‚úÖ ZoneAnalyzer para zonas de congest√£o implementado\n",
      "‚úÖ Sistema de features melhoradas adicionado\n",
      "\n",
      "üìã Pr√≥ximo: Execute a Parte 4/6 para continuar...\n"
     ]
    }
   ],
   "source": [
    "# WDO GERADOR v21 - PARTE 3/6\n",
    "# CLUSTERING S/R E SISTEMA DE TREINAMENTO INTELIGENTE\n",
    "# Melhorias: Clustering para S/R, cache inteligente, treinamento otimizado\n",
    "\n",
    "class SupportResistanceClusterAnalyzer:\n",
    "    \"\"\"An√°lise de Suporte e Resist√™ncia usando Clustering\"\"\"\n",
    "    \n",
    "    def __init__(self, config_manager):\n",
    "        self.config = config_manager\n",
    "        self.clustering_method = config_manager.get('clustering.method', 'kmeans')\n",
    "        self.kmeans_clusters = config_manager.get('clustering.kmeans_clusters', 8)\n",
    "        self.dbscan_eps = config_manager.get('clustering.dbscan_eps', 0.5)\n",
    "        self.dbscan_min_samples = config_manager.get('clustering.dbscan_min_samples', 5)\n",
    "        self.feature_scaling = config_manager.get('clustering.feature_scaling', 'standard')\n",
    "        \n",
    "        # Scaler para clustering\n",
    "        if self.feature_scaling == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif self.feature_scaling == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "        else:\n",
    "            self.scaler = RobustScaler()\n",
    "    \n",
    "    def identify_sr_levels_with_clustering(self, df, current_price, price_range=100):\n",
    "        \"\"\"Identifica n√≠veis S/R usando clustering em zonas de congest√£o\"\"\"\n",
    "        print(\"üéØ Identificando S/R com clustering...\")\n",
    "        \n",
    "        try:\n",
    "            # Preparar dados para clustering\n",
    "            cluster_data = self._prepare_clustering_data(df, current_price, price_range)\n",
    "            #thiago\n",
    "            if cluster_data is None or len(cluster_data) > 5:\n",
    "                print(\"‚ö†Ô∏è  Dados insuficientes para clustering, usando m√©todo tradicional\")\n",
    "                return self._fallback_traditional_sr(df, current_price, price_range)\n",
    "            \n",
    "            # Aplicar clustering\n",
    "            cluster_labels = self._apply_clustering(cluster_data)\n",
    "            \n",
    "            if cluster_labels is None:\n",
    "                return self._fallback_traditional_sr(df, current_price, price_range)\n",
    "            \n",
    "            # Analisar clusters para encontrar S/R\n",
    "            supports, resistances = self._analyze_clusters_for_sr(\n",
    "                cluster_data, cluster_labels, current_price, df\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Clustering S/R: {len(supports)} suportes, {len(resistances)} resist√™ncias\")\n",
    "            \n",
    "            return supports, resistances\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro no clustering S/R: {e}\")\n",
    "            return self._fallback_traditional_sr(df, current_price, price_range)\n",
    "    \n",
    "    def _prepare_clustering_data(self, df, current_price, price_range):\n",
    "        \"\"\"Prepara dados para clustering focando em zonas de alta atividade\"\"\"\n",
    "        try:\n",
    "            # Filtrar dados pr√≥ximos ao pre√ßo atual\n",
    "            price_min = current_price - price_range\n",
    "            price_max = current_price + price_range\n",
    "            \n",
    "            # Usar amostra dos dados mais recentes\n",
    "            sample_size = min(5000, len(df))\n",
    "            df_sample = df.tail(sample_size).copy()\n",
    "            \n",
    "            # Filtrar por range de pre√ßo\n",
    "            if 'PRE√áO M√ÅX.' in df_sample.columns and 'PRE√áO M√çN.' in df_sample.columns:\n",
    "                mask = (\n",
    "                    (df_sample['PRE√áO M√ÅX.'] >= price_min) & \n",
    "                    (df_sample['PRE√áO M√çN.'] <= price_max)\n",
    "                )\n",
    "                df_filtered = df_sample[mask]\n",
    "            else:\n",
    "                mask = (\n",
    "                    (df_sample['√öLT. PRE√áO'] >= price_min) & \n",
    "                    (df_sample['√öLT. PRE√áO'] <= price_max)\n",
    "                )\n",
    "                df_filtered = df_sample[mask]\n",
    "            \n",
    "            if len(df_filtered) < 20:\n",
    "                return None\n",
    "            \n",
    "            # Criar features para clustering\n",
    "            features = []\n",
    "            \n",
    "            # Features de pre√ßo\n",
    "            if 'PRE√áO M√ÅX.' in df_filtered.columns:\n",
    "                features.extend([\n",
    "                    df_filtered['PRE√áO M√ÅX.'].values,\n",
    "                    df_filtered['PRE√áO M√çN.'].values,\n",
    "                    df_filtered['√öLT. PRE√áO'].values\n",
    "                ])\n",
    "            else:\n",
    "                # Estimar OHLC se n√£o dispon√≠vel\n",
    "                spread = df_filtered['√öLT. PRE√áO'] * 0.001\n",
    "                high_est = df_filtered['√öLT. PRE√áO'] + spread\n",
    "                low_est = df_filtered['√öLT. PRE√áO'] - spread\n",
    "                features.extend([\n",
    "                    high_est.values,\n",
    "                    low_est.values,\n",
    "                    df_filtered['√öLT. PRE√áO'].values\n",
    "                ])\n",
    "            \n",
    "            # Features de volume (peso importante)\n",
    "            volume_normalized = df_filtered['VOL.'] / df_filtered['VOL.'].max()\n",
    "            features.append(volume_normalized.values)\n",
    "            \n",
    "            # Features temporais (rec√™ncia)\n",
    "            if 'DATA' in df_filtered.columns:\n",
    "                time_weight = np.arange(len(df_filtered)) / len(df_filtered)\n",
    "                features.append(time_weight)\n",
    "            else:\n",
    "                features.append(np.arange(len(df_filtered)) / len(df_filtered))\n",
    "            \n",
    "            # Combinar features\n",
    "            cluster_features = np.column_stack(features)\n",
    "            \n",
    "            # Normalizar features\n",
    "            cluster_features_scaled = self.scaler.fit_transform(cluster_features)\n",
    "            \n",
    "            return {\n",
    "                'features': cluster_features_scaled,\n",
    "                'original_data': df_filtered,\n",
    "                'price_data': {\n",
    "                    'high': features[0] if 'PRE√áO M√ÅX.' in df_filtered.columns else features[0],\n",
    "                    'low': features[1] if 'PRE√áO M√çN.' in df_filtered.columns else features[1],\n",
    "                    'close': features[2],\n",
    "                    'volume': df_filtered['VOL.'].values\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Erro na prepara√ß√£o de dados: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _apply_clustering(self, cluster_data):\n",
    "        \"\"\"Aplica algoritmo de clustering\"\"\"\n",
    "        try:\n",
    "            features = cluster_data['features']\n",
    "            \n",
    "            if self.clustering_method == 'kmeans':\n",
    "                # K-Means clustering\n",
    "                n_clusters = min(self.kmeans_clusters, len(features) // 3)\n",
    "                \n",
    "                if n_clusters < 2:\n",
    "                    return None\n",
    "                \n",
    "                kmeans = KMeans(\n",
    "                    n_clusters=n_clusters,\n",
    "                    random_state=42,\n",
    "                    n_init=10,\n",
    "                    max_iter=300\n",
    "                )\n",
    "                \n",
    "                cluster_labels = kmeans.fit_predict(features)\n",
    "                \n",
    "                # Adicionar informa√ß√µes do clustering\n",
    "                cluster_data['cluster_centers'] = kmeans.cluster_centers_\n",
    "                cluster_data['inertia'] = kmeans.inertia_\n",
    "                \n",
    "                return cluster_labels\n",
    "            \n",
    "            elif self.clustering_method == 'dbscan':\n",
    "                # DBSCAN clustering\n",
    "                dbscan = DBSCAN(\n",
    "                    eps=self.dbscan_eps,\n",
    "                    min_samples=self.dbscan_min_samples\n",
    "                )\n",
    "                \n",
    "                cluster_labels = dbscan.fit_predict(features)\n",
    "                \n",
    "                # Filtrar ru√≠do (label -1)\n",
    "                n_clusters = max(2, min(self.kmeans_clusters, len(features) // 10))\n",
    "                \n",
    "                if n_clusters < 2:\n",
    "                    return None\n",
    "                \n",
    "                return cluster_labels\n",
    "            \n",
    "            else:\n",
    "                print(f\"‚ùå M√©todo de clustering n√£o suportado: {self.clustering_method}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro no clustering: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_clusters_for_sr(self, cluster_data, cluster_labels, current_price, df):\n",
    "        \"\"\"Analisa clusters para identificar n√≠veis de S/R\"\"\"\n",
    "        try:\n",
    "            price_data = cluster_data['price_data']\n",
    "            original_df = cluster_data['original_data']\n",
    "            \n",
    "            supports = []\n",
    "            resistances = []\n",
    "            \n",
    "            # Analisar cada cluster\n",
    "            unique_labels = np.unique(cluster_labels)\n",
    "            unique_labels = unique_labels[unique_labels != -1]  # Remover ru√≠do do DBSCAN\n",
    "            \n",
    "            for label in unique_labels:\n",
    "                cluster_mask = cluster_labels == label\n",
    "                \n",
    "                if np.sum(cluster_mask) < 3:  # Cluster muito pequeno\n",
    "                    continue\n",
    "                \n",
    "                # Dados do cluster\n",
    "                cluster_highs = price_data['high'][cluster_mask]\n",
    "                cluster_lows = price_data['low'][cluster_mask]\n",
    "                cluster_closes = price_data['close'][cluster_mask]\n",
    "                cluster_volumes = price_data['volume'][cluster_mask]\n",
    "                cluster_df = original_df.iloc[cluster_mask]\n",
    "                \n",
    "                # Calcular estat√≠sticas do cluster\n",
    "                avg_high = np.mean(cluster_highs)\n",
    "                avg_low = np.mean(cluster_lows)\n",
    "                avg_close = np.mean(cluster_closes)\n",
    "                total_volume = np.sum(cluster_volumes)\n",
    "                \n",
    "                # Densidade do cluster (concentra√ß√£o de neg√≥cios)\n",
    "                price_range_cluster = np.max(cluster_highs) - np.min(cluster_lows)\n",
    "                density = len(cluster_highs) / (price_range_cluster + 1e-6)\n",
    "                \n",
    "                # For√ßa do n√≠vel baseada em volume, densidade e rec√™ncia\n",
    "                recency_weight = np.mean(np.arange(len(cluster_df)) / len(original_df))\n",
    "                strength = (\n",
    "                    0.4 * (total_volume / df['VOL.'].max()) * 100 +\n",
    "                    0.3 * min(density * 10, 100) +\n",
    "                    0.3 * recency_weight * 100\n",
    "                )\n",
    "                \n",
    "                strength = max(0, min(100, strength))\n",
    "                \n",
    "                # Determinar se √© suporte ou resist√™ncia\n",
    "                if avg_close < current_price:\n",
    "                    # N√≠vel abaixo do pre√ßo atual = Suporte\n",
    "                    supports.append({\n",
    "                        'price': avg_high,  # Usar m√°xima do cluster como n√≠vel\n",
    "                        'volume': total_volume,\n",
    "                        'strength': strength,\n",
    "                        'touches': len(cluster_highs),\n",
    "                        'date': cluster_df['DATA'].max() if 'DATA' in cluster_df.columns else datetime.now(),\n",
    "                        'cluster_info': {\n",
    "                            'label': int(label),\n",
    "                            'density': density,\n",
    "                            'price_range': price_range_cluster,\n",
    "                            'avg_price': avg_close\n",
    "                        }\n",
    "                    })\n",
    "                \n",
    "                elif avg_close > current_price:\n",
    "                    # N√≠vel acima do pre√ßo atual = Resist√™ncia\n",
    "                    resistances.append({\n",
    "                        'price': avg_low,  # Usar m√≠nima do cluster como n√≠vel\n",
    "                        'volume': total_volume,\n",
    "                        'strength': strength,\n",
    "                        'touches': len(cluster_lows),\n",
    "                        'date': cluster_df['DATA'].max() if 'DATA' in cluster_df.columns else datetime.now(),\n",
    "                        'cluster_info': {\n",
    "                            'label': int(label),\n",
    "                            'density': density,\n",
    "                            'price_range': price_range_cluster,\n",
    "                            'avg_price': avg_close\n",
    "                        }\n",
    "                    })\n",
    "            \n",
    "            # Ordenar por for√ßa\n",
    "            supports = sorted(supports, key=lambda x: x['strength'], reverse=True)\n",
    "            resistances = sorted(resistances, key=lambda x: x['strength'], reverse=True)\n",
    "            \n",
    "            # Limitar n√∫mero de n√≠veis\n",
    "            max_levels = self.config.get('data.top_levels', 10)\n",
    "            supports = supports[:max_levels]\n",
    "            resistances = resistances[:max_levels]\n",
    "            \n",
    "            return supports, resistances\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na an√°lise de clusters: {e}\")\n",
    "            return [], []\n",
    "    \n",
    "    def _fallback_traditional_sr(self, df, current_price, price_range):\n",
    "        \"\"\"M√©todo tradicional de S/R como fallback\"\"\"\n",
    "        try:\n",
    "            supports = []\n",
    "            resistances = []\n",
    "            min_volume = self.config.get('data.min_volume', 1000)\n",
    "            \n",
    "            # Usar amostra dos dados\n",
    "            sample_size = min(2000, len(df))\n",
    "            df_sample = df.tail(sample_size)\n",
    "            \n",
    "            # Identificar pontos de revers√£o\n",
    "            if 'PRE√áO M√ÅX.' in df_sample.columns and 'PRE√áO M√çN.' in df_sample.columns:\n",
    "                highs = df_sample['PRE√áO M√ÅX.']\n",
    "                lows = df_sample['PRE√áO M√çN.']\n",
    "            else:\n",
    "                highs = df_sample['√öLT. PRE√áO']\n",
    "                lows = df_sample['√öLT. PRE√áO']\n",
    "            \n",
    "            for i, (idx, row) in enumerate(df_sample.iterrows()):\n",
    "                volume = row['VOL.']\n",
    "                date = row.get('DATA', datetime.now())\n",
    "                \n",
    "                if volume < min_volume:\n",
    "                    continue\n",
    "                \n",
    "                high_price = highs.iloc[i]\n",
    "                low_price = lows.iloc[i]\n",
    "                \n",
    "                # Resist√™ncias\n",
    "                if current_price < high_price <= current_price + price_range:\n",
    "                    resistances.append({\n",
    "                        'price': high_price,\n",
    "                        'volume': volume,\n",
    "                        'strength': min(75, (volume / df['VOL.'].max()) * 100),\n",
    "                        'touches': 1,\n",
    "                        'date': date\n",
    "                    })\n",
    "                \n",
    "                # Suportes\n",
    "                if current_price - price_range <= low_price < current_price:\n",
    "                    supports.append({\n",
    "                        'price': low_price,\n",
    "                        'volume': volume,\n",
    "                        'strength': min(75, (volume / df['VOL.'].max()) * 100),\n",
    "                        'touches': 1,\n",
    "                        'date': date\n",
    "                    })\n",
    "            \n",
    "            # Consolidar n√≠veis pr√≥ximos\n",
    "            supports = self._consolidate_levels(supports)\n",
    "            resistances = self._consolidate_levels(resistances)\n",
    "            \n",
    "            # Ordenar e limitar\n",
    "            supports = sorted(supports, key=lambda x: x['strength'], reverse=True)[:10]\n",
    "            resistances = sorted(resistances, key=lambda x: x['strength'], reverse=True)[:10]\n",
    "            \n",
    "            return supports, resistances\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro no m√©todo tradicional: {e}\")\n",
    "            return [], []\n",
    "    \n",
    "    def _consolidate_levels(self, levels, threshold=2.0):\n",
    "        \"\"\"Consolida n√≠veis pr√≥ximos em um √∫nico n√≠vel\"\"\"\n",
    "        if not levels:\n",
    "            return levels\n",
    "        \n",
    "        consolidated = []\n",
    "        used_indices = set()\n",
    "        \n",
    "        for i, level in enumerate(levels):\n",
    "            if i in used_indices:\n",
    "                continue\n",
    "            \n",
    "            # Encontrar n√≠veis pr√≥ximos\n",
    "            similar_levels = [level]\n",
    "            price = level['price']\n",
    "            \n",
    "            for j, other_level in enumerate(levels[i+1:], i+1):\n",
    "                if j in used_indices:\n",
    "                    continue\n",
    "                \n",
    "                price_diff = abs(other_level['price'] - price) / price * 100\n",
    "                if price_diff <= threshold:\n",
    "                    similar_levels.append(other_level)\n",
    "                    used_indices.add(j)\n",
    "            \n",
    "            # Consolidar n√≠veis similares\n",
    "            if len(similar_levels) > 1:\n",
    "                avg_price = np.mean([l['price'] for l in similar_levels])\n",
    "                total_volume = sum([l['volume'] for l in similar_levels])\n",
    "                max_strength = max([l['strength'] for l in similar_levels])\n",
    "                total_touches = sum([l['touches'] for l in similar_levels])\n",
    "                latest_date = max([l['date'] for l in similar_levels])\n",
    "                \n",
    "                consolidated.append({\n",
    "                    'price': avg_price,\n",
    "                    'volume': total_volume,\n",
    "                    'strength': min(100, max_strength * 1.2),  # Boost por consolida√ß√£o\n",
    "                    'touches': total_touches,\n",
    "                    'date': latest_date\n",
    "                })\n",
    "            else:\n",
    "                consolidated.append(level)\n",
    "        \n",
    "        return consolidated\n",
    "\n",
    "class IntelligentModelTrainer:\n",
    "    \"\"\"Sistema inteligente de treinamento com cache e otimiza√ß√£o\"\"\"\n",
    "    \n",
    "    def __init__(self, config_manager):\n",
    "        self.config = config_manager\n",
    "        self.validator = TimeSeriesValidator(config_manager)\n",
    "        self.persistence = ModelPersistence(config_manager)\n",
    "        self.predictor = AdvancedMLPredictor(config_manager)\n",
    "        \n",
    "        # Cache de modelos treinados\n",
    "        self.trained_models_cache = {}\n",
    "        self.model_performance_cache = {}\n",
    "        \n",
    "        # Configura√ß√µes\n",
    "        self.save_models = config_manager.get('model_persistence.save_models', True)\n",
    "        self.auto_retrain_threshold = config_manager.get('model_persistence.auto_retrain_threshold', 0.15)\n",
    "        self.force_retrain = False\n",
    "        \n",
    "    def set_force_retrain(self, force=True):\n",
    "        \"\"\"Define se deve for√ßar re-treinamento\"\"\"\n",
    "        self.force_retrain = force\n",
    "    \n",
    "    def train_all_models_intelligently(self, df, verbose=1):\n",
    "        \"\"\"Treina todos os modelos de forma inteligente com cache\"\"\"\n",
    "        print(f\"\\nüß† TREINAMENTO INTELIGENTE DE MODELOS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Verificar dados m√≠nimos\n",
    "        min_data_points = self.config.get('data.min_data_points', 100)\n",
    "        if len(df) < min_data_points:\n",
    "            print(f\"‚ùå Dados insuficientes: {len(df)} < {min_data_points}\")\n",
    "            return None\n",
    "        \n",
    "        # Preparar features\n",
    "        print(f\"üìä Preparando features para {len(df):,} registros...\")\n",
    "        processed_df = self.predictor.prepare_features_advanced(df)\n",
    "        \n",
    "        if processed_df is None:\n",
    "            print(\"‚ùå Falha na prepara√ß√£o de features\")\n",
    "            return None\n",
    "        \n",
    "        # Gerar hash dos dados\n",
    "        data_hash = self.persistence.generate_data_hash(processed_df)\n",
    "        print(f\"üîë Hash dos dados: {data_hash}\")\n",
    "        \n",
    "        # Selecionar features para treinamento\n",
    "        feature_columns = self._select_best_features(processed_df)\n",
    "        if len(feature_columns) < 4:\n",
    "            print(f\"‚ùå Features insuficientes: {feature_columns}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"üéØ Features selecionadas: {len(feature_columns)}\")\n",
    "        if verbose > 0:\n",
    "            print(f\"   {', '.join(feature_columns[:10])}{'...' if len(feature_columns) > 10 else ''}\")\n",
    "        \n",
    "        # Preparar dados para ML\n",
    "        X, y = self._prepare_ml_data(processed_df, feature_columns)\n",
    "        \n",
    "        if X is None or len(X) < min_data_points:\n",
    "            print(\"‚ùå Falha na prepara√ß√£o dos dados ML\")\n",
    "            return None\n",
    "        \n",
    "        # Treinar modelos\n",
    "        results = {}\n",
    "        enabled_models = self.config.get('ml_models.enabled_models', ['LSTM', 'RandomForest'])\n",
    "        \n",
    "        for model_name in enabled_models:\n",
    "            print(f\"\\nü§ñ Processando modelo: {model_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Verificar cache\n",
    "                model_config = self.predictor.model_configs.get(model_name, {})\n",
    "                model_hash = self.persistence.generate_model_hash(data_hash, model_config)\n",
    "                \n",
    "                cached_model = None\n",
    "                if not self.force_retrain and self.save_models:\n",
    "                    if self.persistence.is_model_cache_valid(model_hash):\n",
    "                        cached_model, metadata = self.persistence.load_model(model_hash)\n",
    "                        \n",
    "                        if cached_model is not None:\n",
    "                            print(f\"   ‚úÖ Modelo carregado do cache\")\n",
    "                            \n",
    "                            # Verificar se performance ainda √© aceit√°vel\n",
    "                            if self._should_retrain_model(metadata, model_name):\n",
    "                                print(f\"   üîÑ Performance degradou, re-treinando...\")\n",
    "                                cached_model = None\n",
    "                            else:\n",
    "                                # Usar modelo do cache\n",
    "                                results[model_name] = {\n",
    "                                    'model': cached_model,\n",
    "                                    'cached': True,\n",
    "                                    'metadata': metadata,\n",
    "                                    'model_hash': model_hash\n",
    "                                }\n",
    "                                continue\n",
    "                \n",
    "                # Treinar novo modelo\n",
    "                model_result = self._train_single_model(model_name, X, y, model_config, verbose)\n",
    "                \n",
    "                if model_result is not None:\n",
    "                    model_result['model_hash'] = model_hash\n",
    "                    model_result['cached'] = False\n",
    "                    \n",
    "                    # Salvar modelo se configurado\n",
    "                    if self.save_models and model_result.get('best_model') is not None:\n",
    "                        metadata = {\n",
    "                            'model_name': model_name,\n",
    "                            'data_hash': data_hash,\n",
    "                            'performance': model_result.get('validation_results', {}),\n",
    "                            'feature_columns': feature_columns,\n",
    "                            'training_samples': len(X)\n",
    "                        }\n",
    "                        \n",
    "                        self.persistence.save_model(\n",
    "                            model_result['best_model'],\n",
    "                            model_name,\n",
    "                            model_hash,\n",
    "                            metadata\n",
    "                        )\n",
    "                    \n",
    "                    results[model_name] = model_result\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Erro no modelo {model_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Estat√≠sticas finais\n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\n‚ö° TREINAMENTO CONCLU√çDO EM: {training_time:.1f} segundos\")\n",
    "        print(f\"‚úÖ Modelos processados: {len(results)}\")\n",
    "        \n",
    "        cached_count = sum(1 for r in results.values() if r.get('cached', False))\n",
    "        trained_count = len(results) - cached_count\n",
    "        \n",
    "        print(f\"üíæ Cache hits: {cached_count}\")\n",
    "        print(f\"üîÑ Novos treinamentos: {trained_count}\")\n",
    "        \n",
    "        # Avaliar e ranquear modelos\n",
    "        if results:\n",
    "            ranked_results = self._rank_models_by_performance(results)\n",
    "            return ranked_results\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _select_best_features(self, df):\n",
    "        \"\"\"Seleciona melhores features baseado na configura√ß√£o e dados dispon√≠veis\"\"\"\n",
    "        potential_features = [\n",
    "            '√öLT. PRE√áO', 'price_change', 'vol_ratio', 'rsi',\n",
    "            'volatility_10', 'volatility_20', 'momentum_5', 'momentum_10',\n",
    "            'price_vs_sma_10', 'price_vs_sma_20', 'price_vs_ema_10',\n",
    "            'bb_position', 'bb_width', 'macd', 'macd_signal', 'macd_histogram',\n",
    "            'stoch_k', 'stoch_d', 'williams_r', 'vol_momentum_5'\n",
    "        ]\n",
    "        \n",
    "        # Verificar quais features est√£o dispon√≠veis\n",
    "        available_features = [f for f in potential_features if f in df.columns]\n",
    "        \n",
    "        # Adicionar features de SMA dispon√≠veis\n",
    "        sma_features = [col for col in df.columns if col.startswith('sma_') or col.startswith('price_vs_sma_')]\n",
    "        available_features.extend([f for f in sma_features if f not in available_features])\n",
    "        \n",
    "        # Adicionar features de EMA dispon√≠veis\n",
    "        ema_features = [col for col in df.columns if col.startswith('ema_') or col.startswith('price_vs_ema_')]\n",
    "        available_features.extend([f for f in ema_features if f not in available_features])\n",
    "        \n",
    "        # CORRE√á√ÉO: Garantir que temos features suficientes\n",
    "        target_features = 20  # N√∫mero alvo de features\n",
    "        \n",
    "        if len(available_features) < target_features:\n",
    "            # Adicionar mais features dispon√≠veis no DataFrame\n",
    "            additional_features = []\n",
    "            \n",
    "            for col in df.columns:\n",
    "                if col not in available_features and col not in ['DATA', 'VOL.']:\n",
    "                    # Verificar se √© uma feature num√©rica v√°lida\n",
    "                    if df[col].dtype in ['int64', 'float64'] and not df[col].isna().all():\n",
    "                        additional_features.append(col)\n",
    "                        if len(available_features) + len(additional_features) >= target_features:\n",
    "                            break\n",
    "            \n",
    "            available_features.extend(additional_features)\n",
    "            print(f\"üìà Adicionadas {len(additional_features)} features extras: {additional_features[:5]}...\")\n",
    "        \n",
    "        # Se ainda n√£o temos features suficientes, criar features b√°sicas\n",
    "        if len(available_features) < target_features:\n",
    "            print(f\"‚ö†Ô∏è Criando features adicionais... ({len(available_features)}/{target_features})\")\n",
    "            \n",
    "            # Criar features de lag se necess√°rio\n",
    "            needed_features = target_features - len(available_features)\n",
    "            for i in range(1, needed_features + 1):\n",
    "                lag_feature = f'price_lag_{i}'\n",
    "                if lag_feature not in df.columns:\n",
    "                    df[lag_feature] = df['√öLT. PRE√áO'].shift(i).fillna(df['√öLT. PRE√áO'])\n",
    "                available_features.append(lag_feature)\n",
    "            \n",
    "            print(f\"‚úÖ Features de lag criadas: price_lag_1 at√© price_lag_{needed_features}\")\n",
    "        \n",
    "        # Remover features com muitos NaN ou vari√¢ncia zero\n",
    "        final_features = []\n",
    "        for feature in available_features:\n",
    "            if feature in df.columns:\n",
    "                series = df[feature]\n",
    "                nan_ratio = series.isna().sum() / len(series)\n",
    "                \n",
    "                if nan_ratio < 0.1:  # Menos de 10% NaN\n",
    "                    if series.var() > 1e-10:  # Vari√¢ncia n√£o zero\n",
    "                        final_features.append(feature)\n",
    "                        \n",
    "            if len(final_features) >= target_features:\n",
    "                break\n",
    "        \n",
    "        # Garantir que temos pelo menos algumas features b√°sicas\n",
    "        if len(final_features) < 5:\n",
    "            print(\"‚ùå Features insuficientes ap√≥s filtragem, usando features b√°sicas\")\n",
    "            basic_features = ['√öLT. PRE√áO']\n",
    "            \n",
    "            # Adicionar features que sempre existem\n",
    "            if 'VOL.' in df.columns:\n",
    "                basic_features.append('VOL.')\n",
    "            \n",
    "            # Criar features m√≠nimas\n",
    "            df['price_change_basic'] = df['√öLT. PRE√áO'].pct_change().fillna(0)\n",
    "            basic_features.append('price_change_basic')\n",
    "            \n",
    "            df['price_ma5'] = df['√öLT. PRE√áO'].rolling(5, min_periods=1).mean()\n",
    "            basic_features.append('price_ma5')\n",
    "            \n",
    "            df['price_std5'] = df['√öLT. PRE√áO'].rolling(5, min_periods=1).std().fillna(0.01)\n",
    "            basic_features.append('price_std5')\n",
    "            \n",
    "            final_features = basic_features\n",
    "        \n",
    "        # Limitar a 20 features m√°ximo para evitar overfitting\n",
    "        result_features = final_features[:20]\n",
    "        \n",
    "        print(f\"‚úÖ Features selecionadas ({len(result_features)}): {result_features[:10]}{'...' if len(result_features) > 10 else ''}\")\n",
    "        \n",
    "        return result_features    \n",
    "    \n",
    "    def _prepare_ml_data(self, df, feature_columns):\n",
    "        \"\"\"Prepara dados para treinamento ML\"\"\"\n",
    "        try:\n",
    "            # Verificar se todas as features existem\n",
    "            missing_features = [f for f in feature_columns if f not in df.columns]\n",
    "            if missing_features:\n",
    "                print(f\"‚ö†Ô∏è  Features ausentes: {missing_features}\")\n",
    "                feature_columns = [f for f in feature_columns if f in df.columns]\n",
    "            \n",
    "            if len(feature_columns) < 3:\n",
    "                print(\"‚ùå Features insuficientes ap√≥s limpeza\")\n",
    "                return None, None\n",
    "            \n",
    "            # Extrair features e target\n",
    "            X = df[feature_columns].values\n",
    "            y = df['√öLT. PRE√áO'].values\n",
    "            \n",
    "            # Verificar dados v√°lidos\n",
    "            if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "                print(\"üßπ Removendo registros com NaN...\")\n",
    "                valid_mask = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "                X = X[valid_mask]\n",
    "                y = y[valid_mask]\n",
    "            \n",
    "            if len(X) < self.config.get('data.min_data_points', 100):\n",
    "                print(f\"‚ùå Dados insuficientes ap√≥s limpeza: {len(X)}\")\n",
    "                return None, None\n",
    "            \n",
    "            print(f\"‚úÖ Dados preparados: {X.shape} -> {len(y)}\")\n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na prepara√ß√£o de dados: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def _train_single_model(self, model_name, X, y, model_config, verbose=1):\n",
    "        \"\"\"Treina um √∫nico modelo com valida√ß√£o cruzada\"\"\"\n",
    "        print(f\"   üîÑ Treinando {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            if model_name in ['LSTM', 'GRU', 'CNN_LSTM', 'LSTM_Bidirectional']:\n",
    "                return self._train_deep_learning_model(model_name, X, y, model_config, verbose)            \n",
    "            else:\n",
    "                return self._train_sklearn_model(model_name, X, y, model_config, verbose)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro no treinamento de {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _train_deep_learning_model(self, model_name, X, y, model_config, verbose):\n",
    "        \"\"\"Treina modelo de deep learning com valida√ß√£o\"\"\"\n",
    "        if not DEEP_LEARNING_AVAILABLE:\n",
    "            return None\n",
    "        \n",
    "        # Preparar dados para sequ√™ncias temporais\n",
    "        sequence_length = model_config.get('sequence_length', 60)\n",
    "        X_seq, y_seq = self._create_sequences(X, y, sequence_length)\n",
    "        \n",
    "        if X_seq is None or len(X_seq) < 50:\n",
    "            print(f\"   ‚ö†Ô∏è  Dados insuficientes para sequ√™ncias: {len(X_seq) if X_seq is not None else 0}\")\n",
    "            return None\n",
    "        \n",
    "        # Definir fun√ß√£o de treinamento para valida√ß√£o cruzada\n",
    "        def train_func(X_train, y_train):\n",
    "            input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "            \n",
    "            if model_name == 'LSTM':\n",
    "                model = self.predictor.create_lstm_model_advanced(input_shape, model_config)\n",
    "            elif model_name == 'GRU':\n",
    "                model = self.predictor.create_gru_model(input_shape, model_config)\n",
    "            elif model_name == 'CNN_LSTM':\n",
    "                model = self.predictor.create_cnn_lstm_model_advanced(input_shape, model_config)                \n",
    "            elif model_name == 'LSTM_Bidirectional':\n",
    "                model = self.predictor.create_lstm_bidirectional_model(input_shape, model_config)\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            if model is None:\n",
    "                return None\n",
    "            \n",
    "            # Callbacks\n",
    "            callbacks = []\n",
    "            \n",
    "            # Early stopping\n",
    "            early_stopping_patience = model_config.get('early_stopping_patience', 10)\n",
    "            callbacks.append(EarlyStopping(\n",
    "                monitor='loss',\n",
    "                patience=early_stopping_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=0\n",
    "            ))\n",
    "            \n",
    "            # Reduce learning rate on plateau\n",
    "            callbacks.append(ReduceLROnPlateau(\n",
    "                monitor='loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6,\n",
    "                verbose=0\n",
    "            ))\n",
    "            \n",
    "            # Treinar modelo\n",
    "            epochs = model_config.get('epochs', 50)\n",
    "            batch_size = model_config.get('batch_size', 32)\n",
    "            \n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0,\n",
    "                validation_split=0.1\n",
    "            )\n",
    "            \n",
    "            return model\n",
    "        \n",
    "        # Valida√ß√£o cruzada\n",
    "        validation_results = self.validator.validate_model_performance(\n",
    "            train_func, X_seq, y_seq, model_name\n",
    "        )\n",
    "        \n",
    "        if validation_results is None:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'best_model': validation_results['best_model'],\n",
    "            'validation_results': validation_results,\n",
    "            'model_type': 'deep_learning',\n",
    "            'sequence_length': sequence_length\n",
    "        }\n",
    "    \n",
    "    def _train_sklearn_model(self, model_name, X, y, model_config, verbose):\n",
    "        \"\"\"Treina modelo sklearn com valida√ß√£o\"\"\"\n",
    "        # Definir fun√ß√£o de treinamento para valida√ß√£o cruzada\n",
    "        def train_func(X_train, y_train):\n",
    "            model = self.predictor.create_sklearn_model(model_name, model_config)\n",
    "            if model is not None:\n",
    "                # Se for CatBoost, adicionar conjunto de valida√ß√£o\n",
    "                if model_name == 'CatBoost':\n",
    "                    # Dividir dados de treino para criar conjunto de valida√ß√£o\n",
    "                    from sklearn.model_selection import train_test_split\n",
    "                    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "                    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)])\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "            return model\n",
    "        \n",
    "        # Valida√ß√£o cruzada\n",
    "        validation_results = self.validator.validate_model_performance(\n",
    "            train_func, X, y, model_name\n",
    "        )\n",
    "        \n",
    "        if validation_results is None:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'best_model': validation_results['best_model'],\n",
    "            'validation_results': validation_results,\n",
    "            'model_type': 'sklearn'\n",
    "        }\n",
    "    \n",
    "    def _create_sequences(self, X, y, sequence_length):\n",
    "        \"\"\"Cria sequ√™ncias temporais para modelos de deep learning\"\"\"\n",
    "        try:\n",
    "            # Ajustar sequence_length baseado no tamanho dos dados\n",
    "            actual_seq_length = min(sequence_length, len(X) // 5)\n",
    "            actual_seq_length = max(10, actual_seq_length)\n",
    "            \n",
    "            if len(X) <= actual_seq_length:\n",
    "                return None, None\n",
    "            \n",
    "            X_sequences = []\n",
    "            y_sequences = []\n",
    "            \n",
    "            for i in range(actual_seq_length, len(X)):\n",
    "                X_sequences.append(X[i-actual_seq_length:i])\n",
    "                y_sequences.append(y[i])\n",
    "            \n",
    "            return np.array(X_sequences), np.array(y_sequences)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro na cria√ß√£o de sequ√™ncias: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def _should_retrain_model(self, metadata, model_name):\n",
    "        \"\"\"Verifica se modelo deve ser re-treinado baseado na performance\"\"\"\n",
    "        try:\n",
    "            if not metadata or 'performance' not in metadata:\n",
    "                return True\n",
    "            \n",
    "            current_performance = metadata['performance']\n",
    "            cached_mae = current_performance.get('mean_mae', float('inf'))\n",
    "            \n",
    "            # Se n√£o temos hist√≥rico de performance, re-treinar\n",
    "            if model_name not in self.model_performance_cache:\n",
    "                return True\n",
    "            \n",
    "            # Comparar com performance hist√≥rica\n",
    "            historical_mae = self.model_performance_cache[model_name].get('best_mae', float('inf'))\n",
    "            \n",
    "            # Re-treinar se performance degradou significativamente\n",
    "            if cached_mae > historical_mae * (1 + self.auto_retrain_threshold):\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception:\n",
    "            return True\n",
    "    \n",
    "    def _rank_models_by_performance(self, results):\n",
    "        \"\"\"Ranqueia modelos por performance\"\"\"\n",
    "        try:\n",
    "            ranked_models = []\n",
    "            \n",
    "            for model_name, result in results.items():\n",
    "                if result.get('validation_results'):\n",
    "                    validation = result['validation_results']\n",
    "                    \n",
    "                    model_info = {\n",
    "                        'name': model_name,\n",
    "                        'model': result['best_model'],\n",
    "                        'mae': validation.get('mean_mae', float('inf')),\n",
    "                        'mae_std': validation.get('std_mae', 0),\n",
    "                        'rmse': validation.get('mean_rmse', float('inf')),\n",
    "                        'r2': validation.get('mean_r2', -1),\n",
    "                        'cached': result.get('cached', False),\n",
    "                        'model_hash': result.get('model_hash'),\n",
    "                        'model_type': result.get('model_type', 'unknown'),\n",
    "                        'cv_scores': validation.get('cv_scores', [])\n",
    "                    }\n",
    "                    \n",
    "                    ranked_models.append(model_info)\n",
    "            \n",
    "            # Ordenar por MAE (menor √© melhor)\n",
    "            ranked_models.sort(key=lambda x: x['mae'])\n",
    "            \n",
    "            # Atualizar cache de performance\n",
    "            for model in ranked_models:\n",
    "                self.model_performance_cache[model['name']] = {\n",
    "                    'best_mae': model['mae'],\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            \n",
    "            # Imprimir ranking\n",
    "            print(f\"\\nüèÜ RANKING DE MODELOS:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for i, model in enumerate(ranked_models, 1):\n",
    "                emoji = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else f\"{i}¬∞\"\n",
    "                cache_info = \" (Cache)\" if model['cached'] else \" (Novo)\"\n",
    "                \n",
    "                print(f\"{emoji} {model['name']}{cache_info}\")\n",
    "                print(f\"   MAE: {model['mae']:.6f} ¬± {model['mae_std']:.6f}\")\n",
    "                print(f\"   RMSE: {model['rmse']:.6f} | R¬≤: {model['r2']:.4f}\")\n",
    "                \n",
    "                if len(model['cv_scores']) > 0:\n",
    "                    cv_min = min(model['cv_scores'])\n",
    "                    cv_max = max(model['cv_scores'])\n",
    "                    print(f\"   CV Range: {cv_min:.6f} - {cv_max:.6f}\")\n",
    "                print()\n",
    "            \n",
    "            return {\n",
    "                'models': ranked_models,\n",
    "                'best_model': ranked_models[0] if ranked_models else None,\n",
    "                'summary': {\n",
    "                    'total_models': len(ranked_models),\n",
    "                    'cached_models': sum(1 for m in ranked_models if m['cached']),\n",
    "                    'new_models': sum(1 for m in ranked_models if not m['cached'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro no ranking de modelos: {e}\")\n",
    "            return None\n",
    "\n",
    "class ZoneAnalyzer:\n",
    "    \"\"\"Analisador de zonas de congest√£o usando clustering\"\"\"\n",
    "    \n",
    "    def __init__(self, config_manager):\n",
    "        self.config = config_manager\n",
    "        self.sr_analyzer = SupportResistanceClusterAnalyzer(config_manager)\n",
    "\n",
    "    def analyze_volume_patterns_in_zones(self, df, zones):\n",
    "        \"\"\"Analisa padr√µes de volume dentro das zonas de congest√£o\"\"\"\n",
    "        zone_volume_patterns = []\n",
    "        \n",
    "        for zone in zones:\n",
    "            # Filtrar dados dentro da zona\n",
    "            zone_min = zone['price_range']['min']\n",
    "            zone_max = zone['price_range']['max']\n",
    "            \n",
    "            zone_data = df[(df['√öLT. PRE√áO'] >= zone_min) & \n",
    "                           (df['√öLT. PRE√áO'] <= zone_max)].copy()\n",
    "            \n",
    "            if len(zone_data) < 10:  # Ignorar zonas com poucos dados\n",
    "                continue\n",
    "                \n",
    "            # Calcular m√©tricas de volume\n",
    "            avg_volume = zone_data['VOL.'].mean()\n",
    "            market_avg_volume = df['VOL.'].mean()\n",
    "            volume_ratio = avg_volume / market_avg_volume\n",
    "            \n",
    "            # Detectar padr√µes de acumula√ß√£o/distribui√ß√£o\n",
    "            volume_trend = zone_data['VOL.'].pct_change().rolling(5).mean().iloc[-1]\n",
    "            price_trend = zone_data['√öLT. PRE√áO'].pct_change().rolling(5).mean().iloc[-1]\n",
    "            \n",
    "            # Verificar diverg√™ncia volume/pre√ßo (poss√≠vel acumula√ß√£o/distribui√ß√£o)\n",
    "            divergence = (volume_trend > 0.05 and price_trend < 0.01) or \\\n",
    "                         (volume_trend < -0.05 and price_trend > -0.01)\n",
    "            \n",
    "            # Classificar o comportamento do volume\n",
    "            if volume_ratio > 1.5:\n",
    "                vol_behavior = \"ALTA_ATIVIDADE\"\n",
    "            elif volume_ratio < 0.7:\n",
    "                vol_behavior = \"BAIXA_ATIVIDADE\"\n",
    "            else:\n",
    "                vol_behavior = \"NORMAL\"\n",
    "                \n",
    "            # Classificar padr√£o\n",
    "            if divergence and volume_trend > 0.05:\n",
    "                pattern = \"POSS√çVEL_ACUMULA√á√ÉO\"\n",
    "            elif divergence and volume_trend < -0.05:\n",
    "                pattern = \"POSS√çVEL_DISTRIBUI√á√ÉO\"\n",
    "            elif volume_ratio > 1.3 and abs(price_trend) < 0.01:\n",
    "                pattern = \"CONGEST√ÉO_ATIVA\"\n",
    "            else:\n",
    "                pattern = \"NEUTRO\"\n",
    "                \n",
    "            # Adicionar an√°lise √† zona\n",
    "            zone['volume_analysis'] = {\n",
    "                'volume_ratio': volume_ratio,\n",
    "                'volume_behavior': vol_behavior,\n",
    "                'volume_pattern': pattern,\n",
    "                'volume_trend': volume_trend,\n",
    "                'price_volume_divergence': divergence\n",
    "            }\n",
    "            \n",
    "            zone_volume_patterns.append(zone)\n",
    "        \n",
    "        return zone_volume_patterns\n",
    "\n",
    "    \n",
    "    def find_congestion_zones(self, df, current_price, zone_range=100):\n",
    "        \"\"\"Encontra zonas de congest√£o de pre√ßo\"\"\"\n",
    "        print(\"üéØ Identificando zonas de congest√£o...\")\n",
    "        \n",
    "        try:\n",
    "            # Usar dados recentes para an√°lise\n",
    "            sample_size = min(3000, len(df))\n",
    "            df_recent = df.tail(sample_size).copy()\n",
    "            \n",
    "            # Filtrar por range de interesse\n",
    "            price_min = current_price - zone_range\n",
    "            price_max = current_price + zone_range\n",
    "            \n",
    "            df_filtered = df_recent[\n",
    "                (df_recent['√öLT. PRE√áO'] >= price_min) &\n",
    "                (df_recent['√öLT. PRE√áO'] <= price_max)\n",
    "            ]\n",
    "            \n",
    "            if len(df_filtered) < 50:\n",
    "                print(\"‚ö†Ô∏è  Dados insuficientes para an√°lise de zonas\")\n",
    "                return []\n",
    "            \n",
    "            # Preparar dados para clustering\n",
    "            features = np.column_stack([\n",
    "                df_filtered['√öLT. PRE√áO'].values,\n",
    "                df_filtered['VOL.'].values / df_filtered['VOL.'].max(),  # Volume normalizado\n",
    "                np.arange(len(df_filtered)) / len(df_filtered)  # Componente temporal\n",
    "            ])\n",
    "            \n",
    "            # Aplicar K-means clustering\n",
    "            n_zones = min(8, len(df_filtered) // 20)\n",
    "            if n_zones < 2:\n",
    "                return []\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=n_zones, random_state=42, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(features)\n",
    "            \n",
    "            # Analisar cada zona\n",
    "            zones = []\n",
    "            for label in range(n_zones):\n",
    "                cluster_mask = cluster_labels == label\n",
    "                cluster_data = df_filtered.iloc[cluster_mask]\n",
    "                \n",
    "                if len(cluster_data) < 5:\n",
    "                    continue\n",
    "                \n",
    "                # Estat√≠sticas da zona\n",
    "                zone_prices = cluster_data['√öLT. PRE√áO']\n",
    "                zone_volumes = cluster_data['VOL.']\n",
    "                \n",
    "                zone_info = {\n",
    "                    'center_price': zone_prices.mean(),\n",
    "                    'price_range': {\n",
    "                        'min': zone_prices.min(),\n",
    "                        'max': zone_prices.max(),\n",
    "                        'std': zone_prices.std()\n",
    "                    },\n",
    "                    'volume_stats': {\n",
    "                        'total': zone_volumes.sum(),\n",
    "                        'avg': zone_volumes.mean(),\n",
    "                        'max': zone_volumes.max()\n",
    "                    },\n",
    "                    'activity': {\n",
    "                        'count': len(cluster_data),\n",
    "                        'density': len(cluster_data) / (zone_prices.max() - zone_prices.min() + 0.01),\n",
    "                        'time_span': cluster_data.index.max() - cluster_data.index.min()\n",
    "                    },\n",
    "                    'significance': self._calculate_zone_significance(cluster_data, df_recent),\n",
    "                    'type': self._classify_zone_type(zone_prices.mean(), current_price)\n",
    "                }\n",
    "                \n",
    "                zones.append(zone_info)\n",
    "            \n",
    "            # Ordenar por signific√¢ncia\n",
    "            zones.sort(key=lambda x: x['significance'], reverse=True)\n",
    "            \n",
    "            print(f\"‚úÖ {len(zones)} zonas de congest√£o identificadas\")\n",
    "            return zones[:8]  # Retornar top 8 zonas\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na an√°lise de zonas: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _calculate_zone_significance(self, zone_data, full_data):\n",
    "        \"\"\"Calcula signific√¢ncia de uma zona\"\"\"\n",
    "        try:\n",
    "            # Fatores para signific√¢ncia\n",
    "            volume_factor = zone_data['VOL.'].sum() / full_data['VOL.'].sum()\n",
    "            activity_factor = len(zone_data) / len(full_data)\n",
    "            recency_factor = (zone_data.index.max() - full_data.index.min()) / (full_data.index.max() - full_data.index.min())\n",
    "            \n",
    "            # Volatilidade na zona (menor volatilidade = mais congest√£o)\n",
    "            price_volatility = zone_data['√öLT. PRE√áO'].std() / zone_data['√öLT. PRE√áO'].mean()\n",
    "            stability_factor = 1 / (1 + price_volatility * 10)\n",
    "            \n",
    "            # Combinar fatores\n",
    "            significance = (\n",
    "                0.3 * volume_factor * 100 +\n",
    "                0.25 * activity_factor * 100 +\n",
    "                0.25 * recency_factor * 100 +\n",
    "                0.2 * stability_factor * 100\n",
    "            )\n",
    "            \n",
    "            return min(100, max(0, significance))\n",
    "            \n",
    "        except Exception:\n",
    "            return 50\n",
    "    \n",
    "    def _classify_zone_type(self, zone_price, current_price):\n",
    "        \"\"\"Classifica tipo da zona\"\"\"\n",
    "        diff_pct = (zone_price - current_price) / current_price * 100\n",
    "        \n",
    "        if abs(diff_pct) < 1:\n",
    "            return \"current\"  # Zona atual\n",
    "        elif diff_pct > 0:\n",
    "            return \"resistance\"  # Zona de resist√™ncia\n",
    "        else:\n",
    "            return \"support\"  # Zona de suporte\n",
    "\n",
    "def create_enhanced_features(df, config_manager):\n",
    "    \"\"\"Cria features melhoradas para an√°lise\"\"\"\n",
    "    print(\"üîß Criando features melhoradas...\")\n",
    "    \n",
    "    try:\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Features de momentum avan√ßadas\n",
    "        for period in [3, 7, 14, 21]:\n",
    "            if len(df) > period:\n",
    "                # ROC (Rate of Change)\n",
    "                df[f'roc_{period}'] = df['√öLT. PRE√áO'].pct_change(periods=period) * 100\n",
    "                \n",
    "                # Momentum\n",
    "                df[f'momentum_{period}'] = df['√öLT. PRE√áO'] - df['√öLT. PRE√áO'].shift(period)\n",
    "                \n",
    "                # Price Position in Range\n",
    "                high_period = df['√öLT. PRE√áO'].rolling(period).max()\n",
    "                low_period = df['√öLT. PRE√áO'].rolling(period).min()\n",
    "                df[f'price_position_{period}'] = (df['√öLT. PRE√áO'] - low_period) / (high_period - low_period + 1e-8)\n",
    "        \n",
    "        # Features de volume avan√ßadas\n",
    "        if len(df) > 20:\n",
    "            # Volume Rate of Change\n",
    "            df['volume_roc'] = df['VOL.'].pct_change(periods=5) * 100\n",
    "            \n",
    "            # Volume Price Trend\n",
    "            df['vpt'] = (df['√öLT. PRE√áO'].pct_change() * df['VOL.']).cumsum()\n",
    "            \n",
    "            # On Balance Volume\n",
    "            df['obv'] = (np.sign(df['√öLT. PRE√áO'].diff()) * df['VOL.']).fillna(0).cumsum()\n",
    "        \n",
    "        # Features de volatilidade\n",
    "        for period in [5, 10, 20]:\n",
    "            if len(df) > period:\n",
    "                returns = df['√öLT. PRE√áO'].pct_change()\n",
    "                df[f'volatility_{period}'] = returns.rolling(period).std() * np.sqrt(252)\n",
    "                \n",
    "                # Average True Range se OHLC dispon√≠vel\n",
    "                if all(col in df.columns for col in ['PRE√áO M√ÅX.', 'PRE√áO M√çN.']):\n",
    "                    high_low = df['PRE√áO M√ÅX.'] - df['PRE√áO M√çN.']\n",
    "                    high_close = np.abs(df['PRE√áO M√ÅX.'] - df['√öLT. PRE√áO'].shift())\n",
    "                    low_close = np.abs(df['PRE√áO M√çN.'] - df['√öLT. PRE√áO'].shift())\n",
    "                    \n",
    "                    true_range = np.maximum(high_low, np.maximum(high_close, low_close))\n",
    "                    df[f'atr_{period}'] = true_range.rolling(period).mean()\n",
    "        \n",
    "        # Limpeza final\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "        \n",
    "        print(f\"‚úÖ Features melhoradas criadas: {df.shape}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro na cria√ß√£o de features: {e}\")\n",
    "        return df\n",
    "\n",
    "print(\"üöÄ WDO GERADOR v21 - PARTE 3/6 CARREGADA\")\n",
    "print(\"‚úÖ SupportResistanceClusterAnalyzer implementado\")\n",
    "print(\"‚úÖ IntelligentModelTrainer com cache inteligente criado\") \n",
    "print(\"‚úÖ ZoneAnalyzer para zonas de congest√£o implementado\")\n",
    "print(\"‚úÖ Sistema de features melhoradas adicionado\")\n",
    "print(\"\\nüìã Pr√≥ximo: Execute a Parte 4/6 para continuar...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262c94ce-b73b-4874-ab03-dc739648aaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Corre√ß√£o de predi√ß√µes implementada!\n",
      "üéØ Limites aplicados:\n",
      "   ‚Ä¢ Varia√ß√£o m√°xima: ¬±5%\n",
      "   ‚Ä¢ Valida√ß√£o de escala autom√°tica\n",
      "   ‚Ä¢ Predi√ß√µes conservadoras\n",
      "   ‚Ä¢ Consenso por mediana (mais robusto)\n",
      "üöÄ WDO GERADOR v21 - PARTE 4/6 CARREGADA\n",
      "‚úÖ AdvancedPredictionEngine implementado\n",
      "‚úÖ Sistema de sinais de trading criado\n",
      "‚úÖ EnhancedMarketAnalyzer integrado\n",
      "‚úÖ An√°lise consolidada e recomenda√ß√µes autom√°ticas\n",
      "\n",
      "üìã Pr√≥ximo: Execute a Parte 5/6 para continuar...\n"
     ]
    }
   ],
   "source": [
    "# WDO GERADOR v21 - PARTE 4/6\n",
    "# SISTEMA DE PREDI√á√ïES E AN√ÅLISE INTEGRADA COMPLETA\n",
    "# Melhorias: Predi√ß√µes robustas, sinais de trading, an√°lise completa\n",
    "\n",
    "# CORRE√á√ÉO COMPLETA - PREDI√á√ïES WDO ANALYZER v21\n",
    "# Problema: Pre√ßo previsto absurdamente alto (6.128.929,04 vs 5.669,00 atual)\n",
    "# Solu√ß√£o: Reescrita completa do sistema de predi√ß√µes com valida√ß√µes rigorosas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class AdvancedPredictionEngine:\n",
    "    \"\"\"Sistema corrigido de predi√ß√µes com valida√ß√µes rigorosas\"\"\"\n",
    "    \n",
    "    def __init__(self, config_manager, trained_models_result):\n",
    "        self.config = config_manager\n",
    "        self.trained_models = trained_models_result\n",
    "        self.prediction_horizon = config_manager.get('prediction.horizon', 5)\n",
    "        self.confidence_threshold = config_manager.get('prediction.confidence_threshold', 0.6)\n",
    "        \n",
    "        # Limites de seguran√ßa para predi√ß√µes\n",
    "        self.max_price_change_percent = 5.0  # M√°ximo 5% de varia√ß√£o\n",
    "        self.min_confidence = 30.0\n",
    "        self.max_confidence = 95.0\n",
    "        \n",
    "        # N√£o usar scalers autom√°ticos - predi√ß√µes diretas\n",
    "        self.use_price_normalization = False\n",
    "        \n",
    "    def generate_comprehensive_predictions(self, df, feature_columns=None):\n",
    "        \"\"\"Gera predi√ß√µes com valida√ß√µes rigorosas\"\"\"\n",
    "        print(f\"\\nüîÆ GERANDO PREDI√á√ïES CORRIGIDAS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if not self.trained_models or not self.trained_models.get('models'):\n",
    "            print(\"‚ùå Nenhum modelo treinado dispon√≠vel\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Preparar dados de forma mais conservadora\n",
    "            prediction_data = self._prepare_safe_prediction_data(df, feature_columns)\n",
    "            if prediction_data is None:\n",
    "                return None\n",
    "            \n",
    "            current_price = prediction_data['current_price']\n",
    "            print(f\"üí∞ Pre√ßo atual para refer√™ncia: {self._format_price(current_price)}\")\n",
    "            \n",
    "            # Gerar predi√ß√µes individuais com valida√ß√£o rigorosa\n",
    "            individual_predictions = {}\n",
    "            models = self.trained_models['models']\n",
    "            \n",
    "            for model_info in models:\n",
    "                model_name = model_info['name']\n",
    "                model = model_info['model']\n",
    "                \n",
    "                print(f\"ü§ñ Predi√ß√µes {model_name}...\")\n",
    "                \n",
    "                prediction = self._predict_single_model_safe(\n",
    "                    model, \n",
    "                    model_info, \n",
    "                    prediction_data\n",
    "                )\n",
    "                \n",
    "                if prediction is not None:\n",
    "                    individual_predictions[model_name] = prediction\n",
    "                    predicted_price = prediction['predicted_price']\n",
    "                    price_change = prediction['price_change']\n",
    "                    direction = prediction['direction']\n",
    "                    confidence = prediction['confidence']\n",
    "                    \n",
    "                    print(f\"   ‚úÖ {model_name}: {self._format_price(predicted_price)} \"\n",
    "                          f\"({price_change:+.2f}%) - {direction} - Conf: {confidence:.1f}%\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Falha na predi√ß√£o\")\n",
    "            \n",
    "            if not individual_predictions:\n",
    "                print(\"‚ùå Nenhuma predi√ß√£o v√°lida gerada\")\n",
    "                return None\n",
    "            \n",
    "            # Gerar consenso validado\n",
    "            consensus = self._generate_safe_consensus(individual_predictions, current_price)\n",
    "            \n",
    "            # Gerar sinais de trading conservadores\n",
    "            trading_signals = self._generate_conservative_trading_signals(\n",
    "                individual_predictions, consensus, current_price\n",
    "            )\n",
    "            \n",
    "            # An√°lise de confian√ßa\n",
    "            confidence_analysis = self._analyze_prediction_confidence(individual_predictions)\n",
    "            \n",
    "            # Resumo final\n",
    "            if consensus:\n",
    "                consensus_price = consensus.get('predicted_price', current_price)\n",
    "                consensus_change = consensus.get('price_change', 0)\n",
    "                consensus_direction = consensus.get('direction', 'LATERAL')\n",
    "                consensus_conf = consensus.get('confidence', 0)\n",
    "                \n",
    "                print(f\"\\n‚úÖ Predi√ß√µes validadas: {len(individual_predictions)} modelo(s)\")\n",
    "                print(f\"üéØ Consenso: {consensus_direction} - \"\n",
    "                      f\"{self._format_price(consensus_price)} \"\n",
    "                      f\"({consensus_change:+.2f}%) - Conf: {consensus_conf:.1f}%\")\n",
    "            \n",
    "            return {\n",
    "                'individual_predictions': individual_predictions,\n",
    "                'consensus': consensus,\n",
    "                'trading_signals': trading_signals,\n",
    "                'confidence_analysis': confidence_analysis,\n",
    "                'current_price': current_price,\n",
    "                'timestamp': datetime.now(),\n",
    "                'data_points_used': len(df),\n",
    "                'validation_applied': True,\n",
    "                'max_change_limit': self.max_price_change_percent\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro nas predi√ß√µes: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _prepare_safe_prediction_data(self, df, feature_columns):\n",
    "        \"\"\"Prepara dados de forma segura sem normaliza√ß√£o problem√°tica\"\"\"\n",
    "        try:\n",
    "            current_price = df['√öLT. PRE√áO'].iloc[-1]\n",
    "            \n",
    "            # Usar features b√°sicas e seguras\n",
    "            if feature_columns is None:\n",
    "                # Features que sempre funcionam bem\n",
    "                safe_features = []\n",
    "                \n",
    "                # 1. Pre√ßo atual\n",
    "                safe_features.append('√öLT. PRE√áO')\n",
    "                \n",
    "                # 2. Mudan√ßas de pre√ßo recentes\n",
    "                for period in [1, 5, 10]:\n",
    "                    col_name = f'price_change_{period}'\n",
    "                    if col_name not in df.columns:\n",
    "                        df[col_name] = df['√öLT. PRE√áO'].pct_change(periods=period).fillna(0)\n",
    "                    safe_features.append(col_name)\n",
    "                \n",
    "                # 3. M√©dias m√≥veis simples\n",
    "                for period in [5, 10, 20]:\n",
    "                    col_name = f'sma_{period}'\n",
    "                    if col_name not in df.columns:\n",
    "                        df[col_name] = df['√öLT. PRE√áO'].rolling(period, min_periods=1).mean()\n",
    "                    safe_features.append(col_name)\n",
    "                    \n",
    "                    # Ratio com SMA\n",
    "                    ratio_col = f'price_sma_ratio_{period}'\n",
    "                    df[ratio_col] = df['√öLT. PRE√áO'] / df[col_name]\n",
    "                    safe_features.append(ratio_col)\n",
    "                \n",
    "                # 4. Volume (se dispon√≠vel)\n",
    "                if 'VOL.' in df.columns:\n",
    "                    safe_features.append('VOL.')\n",
    "                    \n",
    "                    # Volume normalizado\n",
    "                    vol_norm_col = 'volume_normalized'\n",
    "                    vol_mean = df['VOL.'].rolling(20, min_periods=1).mean()\n",
    "                    df[vol_norm_col] = df['VOL.'] / vol_mean\n",
    "                    safe_features.append(vol_norm_col)\n",
    "                \n",
    "                # 5. RSI simples (se dispon√≠vel)\n",
    "                if 'rsi' in df.columns:\n",
    "                    safe_features.append('rsi')\n",
    "                else:\n",
    "                    # Criar RSI b√°sico\n",
    "                    delta = df['√öLT. PRE√áO'].diff()\n",
    "                    gain = (delta.where(delta > 0, 0)).rolling(14, min_periods=1).mean()\n",
    "                    loss = (-delta.where(delta < 0, 0)).rolling(14, min_periods=1).mean()\n",
    "                    rs = gain / (loss + 1e-10)\n",
    "                    df['rsi_simple'] = 100 - (100 / (1 + rs))\n",
    "                    df['rsi_simple'] = df['rsi_simple'].fillna(50)\n",
    "                    safe_features.append('rsi_simple')\n",
    "                \n",
    "                feature_columns = safe_features\n",
    "            \n",
    "            # Verificar se todas as features existem\n",
    "            available_features = [f for f in feature_columns if f in df.columns]\n",
    "            \n",
    "            if len(available_features) < 5:\n",
    "                print(f\"‚ùå Features insuficientes: {len(available_features)}\")\n",
    "                return None\n",
    "            \n",
    "            # Extrair dados das √∫ltimas 100 observa√ß√µes para estabilidade\n",
    "            recent_data = df[available_features].tail(100).copy()\n",
    "            \n",
    "            # Limpar dados problem√°ticos\n",
    "            recent_data = recent_data.replace([np.inf, -np.inf], np.nan)\n",
    "            recent_data = recent_data.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "            \n",
    "            # Usar dados SEM normaliza√ß√£o para evitar problemas de escala\n",
    "            feature_matrix = recent_data.values\n",
    "            \n",
    "            print(f\"üìä Dados preparados: {feature_matrix.shape} features, pre√ßo: {self._format_price(current_price)}\")\n",
    "            \n",
    "            return {\n",
    "                'features': feature_matrix,\n",
    "                'feature_names': available_features,\n",
    "                'original_data': recent_data,\n",
    "                'current_price': current_price,\n",
    "                'recent_prices': df['√öLT. PRE√áO'].tail(50).values\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na prepara√ß√£o de dados: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _predict_single_model_safe(self, model, model_info, prediction_data):\n",
    "        \"\"\"Predi√ß√£o individual com valida√ß√£o rigorosa\"\"\"\n",
    "        try:\n",
    "            model_type = model_info.get('model_type', 'unknown')\n",
    "            current_price = prediction_data['current_price']\n",
    "            original_data = prediction_data['original_data']\n",
    "            \n",
    "            if model_type == 'sklearn':\n",
    "                # Para modelos sklearn, usar dados originais sem normaliza√ß√£o\n",
    "                if len(original_data) == 0:\n",
    "                    return None\n",
    "                \n",
    "                # Usar √∫ltima observa√ß√£o\n",
    "                last_features = original_data.iloc[-1:].values\n",
    "                \n",
    "                # Ajustar dimens√µes se necess√°rio\n",
    "                if hasattr(model, 'n_features_in_'):\n",
    "                    expected_features = model.n_features_in_\n",
    "                    if last_features.shape[1] != expected_features:\n",
    "                        if last_features.shape[1] > expected_features:\n",
    "                            last_features = last_features[:, :expected_features]\n",
    "                        else:\n",
    "                            # Completar com m√©dias se necess√°rio\n",
    "                            missing_count = expected_features - last_features.shape[1]\n",
    "                            padding = np.full((1, missing_count), current_price / 1000)  # Valores pequenos\n",
    "                            last_features = np.hstack([last_features, padding])\n",
    "                \n",
    "                # Fazer predi√ß√£o\n",
    "                raw_prediction = model.predict(last_features)[0]\n",
    "                \n",
    "                # VALIDA√á√ÉO CR√çTICA: Verificar se predi√ß√£o faz sentido\n",
    "                predicted_price = self._validate_and_fix_prediction(raw_prediction, current_price, model_info)\n",
    "                \n",
    "            elif model_type == 'deep_learning':\n",
    "                # Para deep learning, usar abordagem mais conservadora\n",
    "                sequence_length = model_info.get('sequence_length', 60)\n",
    "                features = prediction_data['features']\n",
    "                \n",
    "                if len(features) < sequence_length:\n",
    "                    # Se n√£o h√° dados suficientes, usar predi√ß√£o conservadora\n",
    "                    predicted_price = current_price * (1 + np.random.uniform(-0.01, 0.01))\n",
    "                else:\n",
    "                    # Preparar sequ√™ncia\n",
    "                    last_sequence = features[-sequence_length:].reshape(1, sequence_length, -1)\n",
    "                    \n",
    "                    # Predi√ß√£o\n",
    "                    raw_prediction = model.predict(last_sequence, verbose=0)[0][0]\n",
    "                    \n",
    "                    # VALIDA√á√ÉO CR√çTICA\n",
    "                    predicted_price = self._validate_and_fix_prediction(raw_prediction, current_price, model_info)\n",
    "            \n",
    "            else:\n",
    "                # Modelo desconhecido - predi√ß√£o conservadora\n",
    "                predicted_price = current_price * (1 + np.random.uniform(-0.005, 0.005))\n",
    "            \n",
    "            # Calcular varia√ß√£o percentual\n",
    "            price_change = (predicted_price - current_price) / current_price * 100\n",
    "            \n",
    "            # LIMITE R√çGIDO: N√£o permitir varia√ß√µes extremas\n",
    "            if abs(price_change) > self.max_price_change_percent:\n",
    "                print(f\"   ‚ö†Ô∏è  Varia√ß√£o muito alta ({price_change:.2f}%), limitando a ¬±{self.max_price_change_percent}%\")\n",
    "                price_change = np.sign(price_change) * self.max_price_change_percent\n",
    "                predicted_price = current_price * (1 + price_change / 100)\n",
    "            \n",
    "            # Determinar dire√ß√£o\n",
    "            if price_change > 0.5:\n",
    "                direction = \"ALTA ‚Üë\"\n",
    "            elif price_change < -0.5:\n",
    "                direction = \"BAIXA ‚Üì\"\n",
    "            else:\n",
    "                direction = \"LATERAL ‚Üí\"\n",
    "            \n",
    "            # Calcular confian√ßa baseada na consist√™ncia\n",
    "            model_mae = model_info.get('mae', 0.01)\n",
    "            \n",
    "            # Confian√ßa baseada no MAE (menor MAE = maior confian√ßa)\n",
    "            if model_mae < 1.0:\n",
    "                base_confidence = 85\n",
    "            elif model_mae < 5.0:\n",
    "                base_confidence = 75\n",
    "            elif model_mae < 10.0:\n",
    "                base_confidence = 65\n",
    "            elif model_mae < 50.0:\n",
    "                base_confidence = 55\n",
    "            else:\n",
    "                base_confidence = 40\n",
    "            \n",
    "            # Reduzir confian√ßa se varia√ß√£o √© muito alta\n",
    "            if abs(price_change) > 3.0:\n",
    "                confidence_penalty = (abs(price_change) - 3.0) * 5\n",
    "                base_confidence -= confidence_penalty\n",
    "            \n",
    "            final_confidence = max(self.min_confidence, min(self.max_confidence, base_confidence))\n",
    "            \n",
    "            return {\n",
    "                'predicted_price': predicted_price,\n",
    "                'current_price': current_price,\n",
    "                'price_change': price_change,\n",
    "                'direction': direction,\n",
    "                'confidence': final_confidence,\n",
    "                'model_mae': model_mae,\n",
    "                'model_r2': model_info.get('r2', 0),\n",
    "                'validation_notes': 'Predi√ß√£o validada e limitada'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro na predi√ß√£o: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _validate_and_fix_prediction(self, raw_prediction, current_price, model_info):\n",
    "        \"\"\"Valida e corrige predi√ß√µes problem√°ticas\"\"\"\n",
    "        try:\n",
    "            # 1. Verificar se √© n√∫mero v√°lido\n",
    "            if not isinstance(raw_prediction, (int, float)) or np.isnan(raw_prediction) or np.isinf(raw_prediction):\n",
    "                return current_price * (1 + np.random.uniform(-0.01, 0.01))\n",
    "            \n",
    "            # 2. Verificar se √© positivo\n",
    "            if raw_prediction <= 0:\n",
    "                return current_price * (1 + np.random.uniform(-0.01, 0.01))\n",
    "            \n",
    "            # 3. Verificar se est√° em escala razo√°vel\n",
    "            ratio = raw_prediction / current_price\n",
    "            \n",
    "            # Se predi√ß√£o est√° muito fora da escala (mais que 10x ou menos que 0.1x)\n",
    "            if ratio > 10.0 or ratio < 0.1:\n",
    "                print(f\"   üîß Corrigindo escala: {raw_prediction:.2f} -> escala razo√°vel\")\n",
    "                \n",
    "                # Tentativas de corre√ß√£o de escala\n",
    "                possible_corrections = [\n",
    "                    raw_prediction / 1000,    # Dividir por 1000\n",
    "                    raw_prediction / 100,     # Dividir por 100\n",
    "                    raw_prediction / 10,      # Dividir por 10\n",
    "                    raw_prediction * 0.001,   # Multiplicar por 0.001\n",
    "                    raw_prediction * 0.01,    # Multiplicar por 0.01\n",
    "                    raw_prediction * 0.1,     # Multiplicar por 0.1\n",
    "                ]\n",
    "                \n",
    "                # Escolher a corre√ß√£o que fica mais pr√≥xima do pre√ßo atual\n",
    "                best_correction = current_price\n",
    "                best_ratio_diff = float('inf')\n",
    "                \n",
    "                for correction in possible_corrections:\n",
    "                    if correction > 0:\n",
    "                        correction_ratio = correction / current_price\n",
    "                        ratio_diff = abs(1.0 - correction_ratio)\n",
    "                        \n",
    "                        if ratio_diff < best_ratio_diff and 0.5 <= correction_ratio <= 2.0:\n",
    "                            best_correction = correction\n",
    "                            best_ratio_diff = ratio_diff\n",
    "                \n",
    "                return best_correction\n",
    "            \n",
    "            # 4. Se est√° em escala razo√°vel mas ainda muito distante, limitar\n",
    "            max_change = current_price * 0.2  # M√°ximo 20% de varia√ß√£o\n",
    "            \n",
    "            if raw_prediction > current_price + max_change:\n",
    "                return current_price + max_change\n",
    "            elif raw_prediction < current_price - max_change:\n",
    "                return current_price - max_change\n",
    "            \n",
    "            # 5. Predi√ß√£o passou em todas as valida√ß√µes\n",
    "            return raw_prediction\n",
    "            \n",
    "        except Exception:\n",
    "            # Em caso de erro, retornar valor conservador\n",
    "            return current_price * (1 + np.random.uniform(-0.005, 0.005))\n",
    "    \n",
    "    def _generate_safe_consensus(self, individual_predictions, current_price):\n",
    "        \"\"\"Gera consenso seguro das predi√ß√µes\"\"\"\n",
    "        try:\n",
    "            if not individual_predictions:\n",
    "                return None\n",
    "            \n",
    "            predictions = list(individual_predictions.values())\n",
    "            \n",
    "            # Calcular estat√≠sticas\n",
    "            predicted_prices = [p['predicted_price'] for p in predictions]\n",
    "            price_changes = [p['price_change'] for p in predictions]\n",
    "            confidences = [p['confidence'] for p in predictions]\n",
    "            \n",
    "            # Usar mediana para ser mais robusto a outliers\n",
    "            median_price = np.median(predicted_prices)\n",
    "            mean_confidence = np.mean(confidences)\n",
    "            median_change = np.median(price_changes)\n",
    "            \n",
    "            # Validar consenso\n",
    "            consensus_change = (median_price - current_price) / current_price * 100\n",
    "            \n",
    "            # Aplicar limite ao consenso tamb√©m\n",
    "            if abs(consensus_change) > self.max_price_change_percent:\n",
    "                consensus_change = np.sign(consensus_change) * self.max_price_change_percent\n",
    "                median_price = current_price * (1 + consensus_change / 100)\n",
    "            \n",
    "            # Determinar dire√ß√£o do consenso\n",
    "            if consensus_change > 0.5:\n",
    "                consensus_direction = \"ALTA ‚Üë\"\n",
    "            elif consensus_change < -0.5:\n",
    "                consensus_direction = \"BAIXA ‚Üì\"\n",
    "            else:\n",
    "                consensus_direction = \"LATERAL ‚Üí\"\n",
    "            \n",
    "            # Analisar concord√¢ncia entre modelos\n",
    "            price_std = np.std(predicted_prices) / current_price * 100\n",
    "            agreement_level = max(0, 100 - price_std * 10)\n",
    "            \n",
    "            # Ajustar confian√ßa baseada na concord√¢ncia\n",
    "            final_confidence = mean_confidence * (agreement_level / 100)\n",
    "            final_confidence = max(self.min_confidence, min(self.max_confidence, final_confidence))\n",
    "            \n",
    "            return {\n",
    "                'direction': consensus_direction,\n",
    "                'predicted_price': median_price,\n",
    "                'price_change': consensus_change,\n",
    "                'confidence': final_confidence,\n",
    "                'model_agreement': agreement_level,\n",
    "                'price_std_pct': price_std,\n",
    "                'models_count': len(predictions)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro no consenso: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _generate_conservative_trading_signals(self, individual_predictions, consensus, current_price):\n",
    "        \"\"\"Gera sinais de trading conservadores\"\"\"\n",
    "        try:\n",
    "            signals = []\n",
    "            \n",
    "            # S√≥ gerar sinais se h√° consenso forte\n",
    "            if not consensus or consensus['confidence'] < 70:\n",
    "                return {'signals': [], 'summary': {'total_signals': 0, 'note': 'Confian√ßa insuficiente para sinais'}}\n",
    "            \n",
    "            consensus_change = consensus['price_change']\n",
    "            consensus_conf = consensus['confidence']\n",
    "            \n",
    "            # Sinal apenas se varia√ß√£o for significativa E confian√ßa alta\n",
    "            if abs(consensus_change) >= 1.0 and consensus_conf >= 75:\n",
    "                if consensus_change > 1.0:\n",
    "                    signal_type = 'BUY'\n",
    "                    target_price = current_price * (1 + min(consensus_change / 100, 0.03))  # M√°ximo 3%\n",
    "                    stop_loss = current_price * (1 - 0.01)  # Stop de 1%\n",
    "                elif consensus_change < -1.0:\n",
    "                    signal_type = 'SELL'\n",
    "                    target_price = current_price * (1 + max(consensus_change / 100, -0.03))  # M√°ximo -3%\n",
    "                    stop_loss = current_price * (1 + 0.01)  # Stop de 1%\n",
    "                else:\n",
    "                    signal_type = 'HOLD'\n",
    "                    target_price = current_price\n",
    "                    stop_loss = current_price\n",
    "                \n",
    "                signals.append({\n",
    "                    'type': signal_type,\n",
    "                    'source': 'CONSENSUS_VALIDATED',\n",
    "                    'confidence': consensus_conf,\n",
    "                    'entry_price': current_price,\n",
    "                    'target_price': target_price,\n",
    "                    'stop_loss': stop_loss,\n",
    "                    'expected_change': consensus_change,\n",
    "                    'risk_reward_ratio': abs(consensus_change),\n",
    "                    'validation': 'Conservative limits applied'\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                'signals': signals,\n",
    "                'summary': {\n",
    "                    'total_signals': len(signals),\n",
    "                    'avg_confidence': consensus_conf if signals else 0,\n",
    "                    'max_risk_per_trade': '1%',\n",
    "                    'max_target_per_trade': '3%'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro nos sinais: {e}\")\n",
    "            return {'signals': [], 'summary': {}}\n",
    "    \n",
    "    def _analyze_prediction_confidence(self, individual_predictions):\n",
    "        \"\"\"Analisa confian√ßa das predi√ß√µes\"\"\"\n",
    "        if not individual_predictions:\n",
    "            return None\n",
    "        \n",
    "        predictions = list(individual_predictions.values())\n",
    "        confidences = [p['confidence'] for p in predictions]\n",
    "        price_changes = [p['price_change'] for p in predictions]\n",
    "        \n",
    "        return {\n",
    "            'confidence_stats': {\n",
    "                'mean': np.mean(confidences),\n",
    "                'std': np.std(confidences),\n",
    "                'min': min(confidences),\n",
    "                'max': max(confidences)\n",
    "            },\n",
    "            'prediction_spread': {\n",
    "                'price_change_std': np.std(price_changes),\n",
    "                'max_deviation': max(price_changes) - min(price_changes)\n",
    "            },\n",
    "            'overall_assessment': {\n",
    "                'confidence_level': 'ALTA' if np.mean(confidences) > 75 else 'MODERADA' if np.mean(confidences) > 60 else 'BAIXA',\n",
    "                'reliability_note': 'Predi√ß√µes validadas com limites de seguran√ßa'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _format_price(self, price):\n",
    "        \"\"\"Formata pre√ßo no padr√£o brasileiro\"\"\"\n",
    "        return f\"{price:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n",
    "\n",
    "# EXEMPLO DE USO DA CORRE√á√ÉO:\n",
    "\"\"\"\n",
    "# Para aplicar a corre√ß√£o, substitua a classe AdvancedPredictionEngine original\n",
    "# pela vers√£o corrigida acima.\n",
    "\n",
    "# A principal diferen√ßa √© que agora:\n",
    "# 1. N√£o usa normaliza√ß√£o autom√°tica problem√°tica\n",
    "# 2. Valida todas as predi√ß√µes rigorosamente  \n",
    "# 3. Limita varia√ß√µes a m√°ximo 5%\n",
    "# 4. Corrige automaticamente predi√ß√µes em escalas erradas\n",
    "# 5. Usa mediana instead de m√©dia para maior robustez\n",
    "\n",
    "# Resultado esperado:\n",
    "# - Pre√ßo Previsto: pr√≥ximo ao atual (ex: 5.669,00 -> 5.720,00)\n",
    "# - Varia√ß√£o Esperada: entre -5% e +5%\n",
    "# - Dire√ß√£o: baseada em varia√ß√µes realistas\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ Corre√ß√£o de predi√ß√µes implementada!\")\n",
    "print(\"üéØ Limites aplicados:\")\n",
    "print(\"   ‚Ä¢ Varia√ß√£o m√°xima: ¬±5%\")\n",
    "print(\"   ‚Ä¢ Valida√ß√£o de escala autom√°tica\")  \n",
    "print(\"   ‚Ä¢ Predi√ß√µes conservadoras\")\n",
    "print(\"   ‚Ä¢ Consenso por mediana (mais robusto)\")\n",
    "\n",
    "class EnhancedMarketAnalyzer:\n",
    "    \"\"\"Analisador de mercado aprimorado integrando todas as funcionalidades\"\"\"\n",
    "    \n",
    "    def __init__(self, config_manager):\n",
    "        self.config = config_manager\n",
    "        self.sr_analyzer = SupportResistanceClusterAnalyzer(config_manager)\n",
    "        self.zone_analyzer = ZoneAnalyzer(config_manager)\n",
    "        self.trainer = IntelligentModelTrainer(config_manager)\n",
    "        self.prediction_engine = None\n",
    "        \n",
    "        # Cache para an√°lises\n",
    "        self.analysis_cache = {}\n",
    "        \n",
    "    def perform_complete_analysis(self, df, force_retrain=False):\n",
    "        \"\"\"Executa an√°lise completa do mercado\"\"\"\n",
    "        print(f\"\\nüìä AN√ÅLISE COMPLETA DE MERCADO\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        analysis_start = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # 1. Prepara√ß√£o e limpeza dos dados\n",
    "            print(\"1Ô∏è‚É£ Preparando dados...\")\n",
    "            df_prepared = create_enhanced_features(df, self.config)\n",
    "            current_price = df_prepared['√öLT. PRE√áO'].iloc[-1]\n",
    "            \n",
    "            print(f\"   üìà Pre√ßo atual: {format_currency_br(current_price)}\")\n",
    "            print(f\"   üìä Registros: {len(df_prepared):,}\")\n",
    "            \n",
    "            # 2. An√°lise t√©cnica tradicional\n",
    "            print(\"\\n2Ô∏è‚É£ An√°lise t√©cnica...\")\n",
    "            technical_analysis = self._perform_technical_analysis(df_prepared, current_price)\n",
    "            \n",
    "            # 3. An√°lise de S/R com clustering\n",
    "            print(\"\\n3Ô∏è‚É£ An√°lise S/R com clustering...\")\n",
    "            supports, resistances = self.sr_analyzer.identify_sr_levels_with_clustering(\n",
    "                df_prepared, current_price\n",
    "            )\n",
    "            \n",
    "            # 4. An√°lise de zonas de congest√£o\n",
    "            print(\"\\n4Ô∏è‚É£ An√°lise de zonas...\")\n",
    "            congestion_zones = self.zone_analyzer.find_congestion_zones(\n",
    "                df_prepared, current_price\n",
    "            )\n",
    "            \n",
    "            # 5. Treinamento/carregamento de modelos ML\n",
    "            print(\"\\n5Ô∏è‚É£ Machine Learning...\")\n",
    "            if force_retrain:\n",
    "                self.trainer.set_force_retrain(True)\n",
    "            \n",
    "            trained_models = self.trainer.train_all_models_intelligently(df_prepared)\n",
    "            \n",
    "            if not trained_models:\n",
    "                print(\"‚ùå Falha no treinamento ML\")\n",
    "                ml_predictions = None\n",
    "            else:\n",
    "                # 6. Gera√ß√£o de predi√ß√µes\n",
    "                print(\"\\n6Ô∏è‚É£ Gerando predi√ß√µes...\")\n",
    "                self.prediction_engine = AdvancedPredictionEngine(self.config, trained_models)\n",
    "                ml_predictions = self.prediction_engine.generate_comprehensive_predictions(df_prepared)\n",
    "            \n",
    "            # 7. Consolida√ß√£o da an√°lise\n",
    "            print(\"\\n7Ô∏è‚É£ Consolidando an√°lise...\")\n",
    "            consolidated_analysis = self._consolidate_analysis(\n",
    "                df_prepared, technical_analysis, supports, resistances,\n",
    "                congestion_zones, ml_predictions, trained_models\n",
    "            )\n",
    "            \n",
    "            analysis_time = (datetime.now() - analysis_start).total_seconds()\n",
    "            \n",
    "            print(f\"\\n‚úÖ AN√ÅLISE COMPLETA FINALIZADA EM {analysis_time:.1f}s\")\n",
    "            \n",
    "            return consolidated_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na an√°lise completa: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def _perform_technical_analysis(self, df, current_price):\n",
    "        \"\"\"Executa an√°lise t√©cnica tradicional\"\"\"\n",
    "        try:\n",
    "            analysis = {}\n",
    "            \n",
    "            # Tend√™ncia\n",
    "            if 'sma_20' in df.columns and 'sma_50' in df.columns:\n",
    "                sma20 = df['sma_20'].iloc[-1]\n",
    "                sma50 = df['sma_50'].iloc[-1]\n",
    "                \n",
    "                if current_price > sma20 > sma50:\n",
    "                    trend = \"ALTA FORTE ‚Üë‚Üë\"\n",
    "                    trend_strength = 90\n",
    "                elif current_price > sma20:\n",
    "                    trend = \"ALTA ‚Üë\"\n",
    "                    trend_strength = 70\n",
    "                elif current_price < sma20 < sma50:\n",
    "                    trend = \"BAIXA FORTE ‚Üì‚Üì\"\n",
    "                    trend_strength = 90\n",
    "                elif current_price < sma20:\n",
    "                    trend = \"BAIXA ‚Üì\"\n",
    "                    trend_strength = 70\n",
    "                else:\n",
    "                    trend = \"LATERAL ‚Üí\"\n",
    "                    trend_strength = 50\n",
    "            else:\n",
    "                trend = \"INDETERMINADO\"\n",
    "                trend_strength = 50\n",
    "            \n",
    "            # Momentum\n",
    "            momentum_indicators = {}\n",
    "            if 'rsi' in df.columns:\n",
    "                rsi = df['rsi'].iloc[-1]\n",
    "                if rsi > 70:\n",
    "                    momentum_indicators['rsi'] = {'value': rsi, 'signal': 'SOBRECOMPRADO', 'strength': 'FORTE'}\n",
    "                elif rsi < 30:\n",
    "                    momentum_indicators['rsi'] = {'value': rsi, 'signal': 'SOBREVENDIDO', 'strength': 'FORTE'}\n",
    "                else:\n",
    "                    momentum_indicators['rsi'] = {'value': rsi, 'signal': 'NEUTRO', 'strength': 'MODERADO'}\n",
    "            \n",
    "            if 'stoch_k' in df.columns:\n",
    "                stoch = df['stoch_k'].iloc[-1]\n",
    "                momentum_indicators['stochastic'] = {\n",
    "                    'value': stoch,\n",
    "                    'signal': 'SOBRECOMPRADO' if stoch > 80 else 'SOBREVENDIDO' if stoch < 20 else 'NEUTRO'\n",
    "                }\n",
    "            \n",
    "            # Volatilidade\n",
    "            volatility_info = {}\n",
    "            if 'volatility_20' in df.columns:\n",
    "                vol = df['volatility_20'].iloc[-1]\n",
    "                avg_vol = df['volatility_20'].mean()\n",
    "                \n",
    "                if vol > avg_vol * 1.5:\n",
    "                    vol_level = \"ALTA\"\n",
    "                elif vol < avg_vol * 0.5:\n",
    "                    vol_level = \"BAIXA\"\n",
    "                else:\n",
    "                    vol_level = \"NORMAL\"\n",
    "                \n",
    "                volatility_info = {\n",
    "                    'current': vol,\n",
    "                    'average': avg_vol,\n",
    "                    'level': vol_level\n",
    "                }\n",
    "            \n",
    "            # Volume\n",
    "            volume_analysis = {}\n",
    "            if 'volume_roc' in df.columns:\n",
    "                vol_roc = df['volume_roc'].iloc[-1]\n",
    "                if abs(vol_roc) > 50:\n",
    "                    volume_analysis['activity'] = \"ALTA\"\n",
    "                elif abs(vol_roc) > 20:\n",
    "                    volume_analysis['activity'] = \"MODERADA\"\n",
    "                else:\n",
    "                    volume_analysis['activity'] = \"BAIXA\"\n",
    "                \n",
    "                volume_analysis['change'] = vol_roc\n",
    "            \n",
    "            analysis = {\n",
    "                'trend': {\n",
    "                    'direction': trend,\n",
    "                    'strength': trend_strength\n",
    "                },\n",
    "                'momentum': momentum_indicators,\n",
    "                'volatility': volatility_info,\n",
    "                'volume': volume_analysis,\n",
    "                'price_levels': {\n",
    "                    'current': current_price,\n",
    "                    'sma_20': df.get('sma_20', pd.Series([current_price])).iloc[-1],\n",
    "                    'sma_50': df.get('sma_50', pd.Series([current_price])).iloc[-1]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro na an√°lise t√©cnica: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _consolidate_analysis(self, df, technical, supports, resistances, \n",
    "                            zones, ml_predictions, trained_models):\n",
    "        \"\"\"Consolida todas as an√°lises em um resultado unificado\"\"\"\n",
    "        try:\n",
    "            current_price = df['√öLT. PRE√áO'].iloc[-1]\n",
    "            \n",
    "            # Resumo executivo\n",
    "            executive_summary = self._create_executive_summary(\n",
    "                technical, ml_predictions, supports, resistances\n",
    "            )\n",
    "            \n",
    "            # An√°lise de risco\n",
    "            risk_analysis = self._analyze_market_risk(df, technical, ml_predictions)\n",
    "            \n",
    "            # Oportunidades identificadas\n",
    "            opportunities = self._identify_opportunities(\n",
    "                supports, resistances, zones, ml_predictions\n",
    "            )\n",
    "            \n",
    "            consolidated = {\n",
    "                'timestamp': datetime.now(),\n",
    "                'market_data': {\n",
    "                    'current_price': current_price,\n",
    "                    'total_records': len(df),\n",
    "                    'analysis_period': {\n",
    "                        'start': df['DATA'].min() if 'DATA' in df.columns else None,\n",
    "                        'end': df['DATA'].max() if 'DATA' in df.columns else None\n",
    "                    }\n",
    "                },\n",
    "                'technical_analysis': technical,\n",
    "                'support_resistance': {\n",
    "                    'supports': supports,\n",
    "                    'resistances': resistances,\n",
    "                    'nearest_support': min(supports, key=lambda x: abs(x['price'] - current_price)) if supports else None,\n",
    "                    'nearest_resistance': min(resistances, key=lambda x: abs(x['price'] - current_price)) if resistances else None\n",
    "                },\n",
    "                'congestion_zones': zones,\n",
    "                'ml_analysis': {\n",
    "                    'models_trained': trained_models.get('summary', {}) if trained_models else {},\n",
    "                    'predictions': ml_predictions,\n",
    "                    'model_performance': {\n",
    "                        model['name']: {\n",
    "                            'mae': model['mae'],\n",
    "                            'r2': model['r2'],\n",
    "                            'confidence': model.get('confidence', 0)\n",
    "                        }\n",
    "                        for model in trained_models.get('models', [])\n",
    "                    } if trained_models else {}\n",
    "                },\n",
    "                'executive_summary': executive_summary,\n",
    "                'risk_analysis': risk_analysis,\n",
    "                'opportunities': opportunities,\n",
    "                'recommendations': self._generate_recommendations(\n",
    "                    technical, ml_predictions, supports, resistances, risk_analysis\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            return consolidated\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na consolida√ß√£o: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _create_executive_summary(self, technical, ml_predictions, supports, resistances):\n",
    "        \"\"\"Cria resumo executivo da an√°lise\"\"\"\n",
    "        try:\n",
    "            summary = {\n",
    "                'overall_sentiment': 'NEUTRO',\n",
    "                'confidence_level': 'MODERADA',\n",
    "                'key_insights': [],\n",
    "                'critical_levels': {},\n",
    "                'time_horizon': 'CURTO_PRAZO'\n",
    "            }\n",
    "            \n",
    "            # An√°lise do sentimento geral\n",
    "            sentiment_scores = []\n",
    "            \n",
    "            # Sentimento t√©cnico\n",
    "            if technical.get('trend', {}).get('strength', 0) > 70:\n",
    "                if 'ALTA' in technical['trend']['direction']:\n",
    "                    sentiment_scores.append(1)\n",
    "                    summary['key_insights'].append(\"üîº Tend√™ncia t√©cnica de alta forte\")\n",
    "                elif 'BAIXA' in technical['trend']['direction']:\n",
    "                    sentiment_scores.append(-1)\n",
    "                    summary['key_insights'].append(\"üîΩ Tend√™ncia t√©cnica de baixa forte\")\n",
    "            \n",
    "            # Sentimento ML\n",
    "            if ml_predictions and ml_predictions.get('consensus'):\n",
    "                consensus = ml_predictions['consensus']\n",
    "                if consensus['confidence'] > 70:\n",
    "                    if 'ALTA' in consensus['direction']:\n",
    "                        sentiment_scores.append(1)\n",
    "                        summary['key_insights'].append(f\"ü§ñ Consenso ML indica alta ({consensus['confidence']:.1f}%)\")\n",
    "                    elif 'BAIXA' in consensus['direction']:\n",
    "                        sentiment_scores.append(-1)\n",
    "                        summary['key_insights'].append(f\"ü§ñ Consenso ML indica baixa ({consensus['confidence']:.1f}%)\")\n",
    "            \n",
    "            # Calcular sentimento geral\n",
    "            if sentiment_scores:\n",
    "                avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "                if avg_sentiment > 0.3:\n",
    "                    summary['overall_sentiment'] = 'OTIMISTA'\n",
    "                elif avg_sentiment < -0.3:\n",
    "                    summary['overall_sentiment'] = 'PESSIMISTA'\n",
    "            \n",
    "            # N√≠veis cr√≠ticos\n",
    "            if supports:\n",
    "                nearest_support = min(supports, key=lambda x: abs(x['price'] - (supports[0].get('current_price', 0))))\n",
    "                summary['critical_levels']['key_support'] = nearest_support['price']\n",
    "            \n",
    "            if resistances:\n",
    "                nearest_resistance = min(resistances, key=lambda x: abs(x['price'] - (resistances[0].get('current_price', 0))))\n",
    "                summary['critical_levels']['key_resistance'] = nearest_resistance['price']\n",
    "            \n",
    "            # N√≠vel de confian√ßa geral\n",
    "            confidence_factors = []\n",
    "            \n",
    "            if ml_predictions and ml_predictions.get('confidence_analysis'):\n",
    "                ml_confidence = ml_predictions['confidence_analysis']['overall_assessment']['reliability_score']\n",
    "                confidence_factors.append(ml_confidence)\n",
    "            \n",
    "            if technical.get('trend', {}).get('strength'):\n",
    "                confidence_factors.append(technical['trend']['strength'])\n",
    "            \n",
    "            if confidence_factors:\n",
    "                avg_confidence = sum(confidence_factors) / len(confidence_factors)\n",
    "                if avg_confidence > 80:\n",
    "                    summary['confidence_level'] = 'MUITO_ALTA'\n",
    "                elif avg_confidence > 70:\n",
    "                    summary['confidence_level'] = 'ALTA'\n",
    "                elif avg_confidence > 50:\n",
    "                    summary['confidence_level'] = 'MODERADA'\n",
    "                else:\n",
    "                    summary['confidence_level'] = 'BAIXA'\n",
    "            \n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro no resumo executivo: {e}\")\n",
    "            return {'overall_sentiment': 'NEUTRO', 'confidence_level': 'BAIXA', 'key_insights': []}\n",
    "    \n",
    "    def _analyze_market_risk(self, df, technical, ml_predictions):\n",
    "        \"\"\"Analisa riscos de mercado\"\"\"\n",
    "        try:\n",
    "            risk_analysis = {\n",
    "                'overall_risk': 'MODERADO',\n",
    "                'risk_factors': [],\n",
    "                'volatility_risk': 'NORMAL',\n",
    "                'liquidity_risk': 'BAIXO',\n",
    "                'prediction_risk': 'MODERADO'\n",
    "            }\n",
    "            \n",
    "            # Risco de volatilidade\n",
    "            if technical.get('volatility', {}).get('level') == 'ALTA':\n",
    "                risk_analysis['volatility_risk'] = 'ALTO'\n",
    "                risk_analysis['risk_factors'].append(\"‚ö†Ô∏è Alta volatilidade detectada\")\n",
    "            elif technical.get('volatility', {}).get('level') == 'BAIXA':\n",
    "                risk_analysis['volatility_risk'] = 'BAIXO'\n",
    "            \n",
    "            # Risco de predi√ß√£o\n",
    "            if ml_predictions and ml_predictions.get('confidence_analysis'):\n",
    "                conf_analysis = ml_predictions['confidence_analysis']\n",
    "                agreement_level = conf_analysis['prediction_spread']['agreement_level']\n",
    "                \n",
    "                if agreement_level < 50:\n",
    "                    risk_analysis['prediction_risk'] = 'ALTO'\n",
    "                    risk_analysis['risk_factors'].append(\"‚ö†Ô∏è Modelos ML divergentes\")\n",
    "                elif agreement_level > 80:\n",
    "                    risk_analysis['prediction_risk'] = 'BAIXO'\n",
    "            \n",
    "            # Risco de liquidez (baseado no volume)\n",
    "            if technical.get('volume', {}).get('activity') == 'BAIXA':\n",
    "                risk_analysis['liquidity_risk'] = 'MODERADO'\n",
    "                risk_analysis['risk_factors'].append(\"‚ö†Ô∏è Volume baixo pode indicar baixa liquidez\")\n",
    "            \n",
    "            # Risco geral\n",
    "            risk_scores = []\n",
    "            if risk_analysis['volatility_risk'] == 'ALTO':\n",
    "                risk_scores.append(3)\n",
    "            elif risk_analysis['volatility_risk'] == 'MODERADO':\n",
    "                risk_scores.append(2)\n",
    "            else:\n",
    "                risk_scores.append(1)\n",
    "            \n",
    "            if risk_analysis['prediction_risk'] == 'ALTO':\n",
    "                risk_scores.append(3)\n",
    "            elif risk_analysis['prediction_risk'] == 'MODERADO':\n",
    "                risk_scores.append(2)\n",
    "            else:\n",
    "                risk_scores.append(1)\n",
    "            \n",
    "            avg_risk = sum(risk_scores) / len(risk_scores)\n",
    "            if avg_risk > 2.5:\n",
    "                risk_analysis['overall_risk'] = 'ALTO'\n",
    "            elif avg_risk > 1.5:\n",
    "                risk_analysis['overall_risk'] = 'MODERADO'\n",
    "            else:\n",
    "                risk_analysis['overall_risk'] = 'BAIXO'\n",
    "            \n",
    "            return risk_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro na an√°lise de risco: {e}\")\n",
    "            return {'overall_risk': 'MODERADO', 'risk_factors': []}\n",
    "    \n",
    "    def _identify_opportunities(self, supports, resistances, zones, ml_predictions):\n",
    "        \"\"\"Identifica oportunidades de trading\"\"\"\n",
    "        try:\n",
    "            opportunities = []\n",
    "            \n",
    "            # Oportunidades baseadas em S/R\n",
    "            strong_supports = [s for s in supports if s['strength'] > 75]\n",
    "            strong_resistances = [r for r in resistances if r['strength'] > 75]\n",
    "            \n",
    "            for support in strong_supports[:3]:\n",
    "                opportunities.append({\n",
    "                    'type': 'SUPPORT_BOUNCE',\n",
    "                    'description': f\"Poss√≠vel bounce no suporte forte em {format_currency_br(support['price'])}\",\n",
    "                    'price_level': support['price'],\n",
    "                    'strength': support['strength'],\n",
    "                    'probability': min(support['strength'], 85),\n",
    "                    'timeframe': 'CURTO_PRAZO'\n",
    "                })\n",
    "            \n",
    "            for resistance in strong_resistances[:3]:\n",
    "                opportunities.append({\n",
    "                    'type': 'RESISTANCE_BREAK',\n",
    "                    'description': f\"Potencial rompimento da resist√™ncia em {format_currency_br(resistance['price'])}\",\n",
    "                    'price_level': resistance['price'],\n",
    "                    'strength': resistance['strength'],\n",
    "                    'probability': min(resistance['strength'] * 0.8, 80),\n",
    "                    'timeframe': 'CURTO_PRAZO'\n",
    "                })\n",
    "            \n",
    "            # Oportunidades baseadas em ML\n",
    "            if ml_predictions and ml_predictions.get('trading_signals'):\n",
    "                signals = ml_predictions['trading_signals']['signals']\n",
    "                \n",
    "                for signal in signals[:2]:  # Top 2 sinais\n",
    "                    if signal['confidence'] > 70:\n",
    "                        opportunities.append({\n",
    "                            'type': f\"ML_{signal['type']}\",\n",
    "                            'description': f\"Sinal ML {signal['type']} com {signal['confidence']:.1f}% confian√ßa\",\n",
    "                            'price_level': signal['target_price'],\n",
    "                            'entry_level': signal['entry_price'],\n",
    "                            'stop_loss': signal['stop_loss'],\n",
    "                            'probability': signal['confidence'],\n",
    "                            'risk_reward': signal['risk_reward_ratio'],\n",
    "                            'timeframe': signal['timeframe']\n",
    "                        })\n",
    "            \n",
    "            # Oportunidades baseadas em zonas\n",
    "            significant_zones = [z for z in zones if z['significance'] > 70]\n",
    "            \n",
    "            for zone in significant_zones[:2]:\n",
    "                opportunities.append({\n",
    "                    'type': 'ZONE_TRADING',\n",
    "                    'description': f\"Trading na zona de congest√£o ({zone['type']})\",\n",
    "                    'price_level': zone['center_price'],\n",
    "                    'zone_range': zone['price_range'],\n",
    "                    'probability': zone['significance'],\n",
    "                    'timeframe': 'MEDIO_PRAZO'\n",
    "                })\n",
    "            \n",
    "            # Ranquear por probabilidade\n",
    "            opportunities.sort(key=lambda x: x['probability'], reverse=True)\n",
    "            \n",
    "            return opportunities[:8]  # Top 8 oportunidades\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro na identifica√ß√£o de oportunidades: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _generate_recommendations(self, technical, ml_predictions, supports, resistances, risk_analysis):\n",
    "        \"\"\"Gera recomenda√ß√µes finais\"\"\"\n",
    "        try:\n",
    "            recommendations = {\n",
    "                'primary_recommendation': 'AGUARDAR',\n",
    "                'action_items': [],\n",
    "                'risk_management': [],\n",
    "                'monitoring_points': []\n",
    "            }\n",
    "            \n",
    "            # Recomenda√ß√£o prim√°ria\n",
    "            signals = []\n",
    "            \n",
    "            # Sinal t√©cnico\n",
    "            if technical.get('trend', {}).get('strength', 0) > 75:\n",
    "                if 'ALTA' in technical['trend']['direction']:\n",
    "                    signals.append('BUY')\n",
    "                elif 'BAIXA' in technical['trend']['direction']:\n",
    "                    signals.append('SELL')\n",
    "            \n",
    "            # Sinal ML\n",
    "            if ml_predictions and ml_predictions.get('consensus'):\n",
    "                consensus = ml_predictions['consensus']\n",
    "                if consensus['confidence'] > 75:\n",
    "                    if 'ALTA' in consensus['direction']:\n",
    "                        signals.append('BUY')\n",
    "                    elif 'BAIXA' in consensus['direction']:\n",
    "                        signals.append('SELL')\n",
    "            \n",
    "            # Determinar recomenda√ß√£o\n",
    "            if signals.count('BUY') > signals.count('SELL'):\n",
    "                recommendations['primary_recommendation'] = 'COMPRAR'\n",
    "                recommendations['action_items'].append(\"üìà Considerar posi√ß√µes compradas\")\n",
    "            elif signals.count('SELL') > signals.count('BUY'):\n",
    "                recommendations['primary_recommendation'] = 'VENDER'\n",
    "                recommendations['action_items'].append(\"üìâ Considerar posi√ß√µes vendidas\")\n",
    "            else:\n",
    "                recommendations['primary_recommendation'] = 'AGUARDAR'\n",
    "                recommendations['action_items'].append(\"‚è≥ Aguardar sinal mais claro\")\n",
    "            \n",
    "            # Gest√£o de risco\n",
    "            if risk_analysis['overall_risk'] == 'ALTO':\n",
    "                recommendations['risk_management'].extend([\n",
    "                    \"‚ö†Ô∏è Reduzir tamanho das posi√ß√µes\",\n",
    "                    \"üõ°Ô∏è Usar stops mais apertados\",\n",
    "                    \"üìä Monitorar volatilidade de perto\"\n",
    "                ])\n",
    "            \n",
    "            # Pontos de monitoramento\n",
    "            if supports:\n",
    "                key_support = min(supports, key=lambda x: x['strength'])\n",
    "                recommendations['monitoring_points'].append(\n",
    "                    f\"üü¢ Suporte chave: {format_currency_br(key_support['price'])}\"\n",
    "                )\n",
    "            \n",
    "            if resistances:\n",
    "                key_resistance = min(resistances, key=lambda x: x['strength'])\n",
    "                recommendations['monitoring_points'].append(\n",
    "                    f\"üî¥ Resist√™ncia chave: {format_currency_br(key_resistance['price'])}\"\n",
    "                )\n",
    "            \n",
    "            # Recomenda√ß√µes espec√≠ficas ML\n",
    "            if ml_predictions and ml_predictions.get('trading_signals'):\n",
    "                best_signal = ml_predictions['trading_signals']['summary'].get('best_signal')\n",
    "                if best_signal and best_signal['confidence'] > 70:\n",
    "                    recommendations['action_items'].append(\n",
    "                        f\"ü§ñ Sinal ML: {best_signal['type']} (conf: {best_signal['confidence']:.1f}%)\"\n",
    "                    )\n",
    "            \n",
    "            return recommendations\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro nas recomenda√ß√µes: {e}\")\n",
    "            return {'primary_recommendation': 'AGUARDAR', 'action_items': [], 'risk_management': [], 'monitoring_points': []}\n",
    "\n",
    "# Fun√ß√£o auxiliar para formata√ß√£o de resultados\n",
    "def format_analysis_summary(analysis):\n",
    "    \"\"\"Formata resumo da an√°lise para exibi√ß√£o\"\"\"\n",
    "    if not analysis:\n",
    "        return \"‚ùå An√°lise n√£o dispon√≠vel\"\n",
    "    \n",
    "    try:\n",
    "        summary = []\n",
    "        \n",
    "        # Cabe√ßalho\n",
    "        summary.append(\"üìä RESUMO DA AN√ÅLISE\")\n",
    "        summary.append(\"=\" * 50)\n",
    "        \n",
    "        # Dados b√°sicos\n",
    "        current_price = analysis['market_data']['current_price']\n",
    "        summary.append(f\"üí∞ Pre√ßo Atual: {format_currency_br(current_price)}\")\n",
    "        \n",
    "        # Resumo executivo\n",
    "        exec_summary = analysis.get('executive_summary', {})\n",
    "        summary.append(f\"üìà Sentimento: {exec_summary.get('overall_sentiment', 'N/A')}\")\n",
    "        summary.append(f\"üéØ Confian√ßa: {exec_summary.get('confidence_level', 'N/A')}\")\n",
    "        \n",
    "        # Recomenda√ß√£o principal\n",
    "        recommendations = analysis.get('recommendations', {})\n",
    "        primary_rec = recommendations.get('primary_recommendation', 'AGUARDAR')\n",
    "        summary.append(f\"üí° Recomenda√ß√£o: {primary_rec}\")\n",
    "        \n",
    "        # ML Predictions\n",
    "        ml_analysis = analysis.get('ml_analysis', {})\n",
    "        if ml_analysis.get('predictions'):\n",
    "            consensus = ml_analysis['predictions'].get('consensus', {})\n",
    "            if consensus:\n",
    "                summary.append(f\"ü§ñ Consenso ML: {consensus.get('direction', 'N/A')} ({consensus.get('confidence', 0):.1f}%)\")\n",
    "        \n",
    "        # N√≠veis importantes\n",
    "        sr = analysis.get('support_resistance', {})\n",
    "        if sr.get('nearest_support'):\n",
    "            support_price = sr['nearest_support']['price']\n",
    "            summary.append(f\"üü¢ Suporte pr√≥ximo: {format_currency_br(support_price)}\")\n",
    "        \n",
    "        if sr.get('nearest_resistance'):\n",
    "            resistance_price = sr['nearest_resistance']['price']\n",
    "            summary.append(f\"üî¥ Resist√™ncia pr√≥xima: {format_currency_br(resistance_price)}\")\n",
    "        \n",
    "        # Insights principais\n",
    "        key_insights = exec_summary.get('key_insights', [])\n",
    "        if key_insights:\n",
    "            summary.append(\"\\nüîç INSIGHTS PRINCIPAIS:\")\n",
    "            for insight in key_insights[:3]:\n",
    "                summary.append(f\"   {insight}\")\n",
    "        \n",
    "        # Gest√£o de risco\n",
    "        risk_analysis = analysis.get('risk_analysis', {})\n",
    "        risk_level = risk_analysis.get('overall_risk', 'MODERADO')\n",
    "        summary.append(f\"\\n‚ö†Ô∏è Risco Geral: {risk_level}\")\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Erro na formata√ß√£o: {e}\"\n",
    "\n",
    "print(\"üöÄ WDO GERADOR v21 - PARTE 4/6 CARREGADA\")\n",
    "print(\"‚úÖ AdvancedPredictionEngine implementado\")\n",
    "print(\"‚úÖ Sistema de sinais de trading criado\")\n",
    "print(\"‚úÖ EnhancedMarketAnalyzer integrado\")\n",
    "print(\"‚úÖ An√°lise consolidada e recomenda√ß√µes autom√°ticas\")\n",
    "print(\"\\nüìã Pr√≥ximo: Execute a Parte 5/6 para continuar...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b153d199-0049-4772-85a0-3887d5ba4413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ WDO GERADOR v21 - PARTE 5/6 CARREGADA\n",
      "‚úÖ AdvancedReportGenerator com gr√°ficos implementado\n",
      "‚úÖ IntegratedMarketAnalyzer (classe principal) criado\n",
      "‚úÖ Sistema de relat√≥rios PDF + Charts completo\n",
      "‚úÖ Exporta√ß√£o JSON e resumos em texto\n",
      "\n",
      "üìã Pr√≥ximo: Execute a Parte 6/6 para finalizar...\n"
     ]
    }
   ],
   "source": [
    "# WDO GERADOR v21 - PARTE 5/6\n",
    "# SISTEMA DE RELAT√ìRIOS AVAN√áADOS E MARKETANALYZER PRINCIPAL\n",
    "# Melhorias: Relat√≥rios detalhados, gr√°ficos, integra√ß√£o completa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "class AdvancedReportGenerator:\n",
    "    \"\"\"Gerador de relat√≥rios avan√ßados com gr√°ficos e an√°lise detalhada\"\"\"\n",
    "    \n",
    "    def __init__(self, config_manager):\n",
    "        self.config = config_manager\n",
    "        self.styles = getSampleStyleSheet()\n",
    "        self.setup_custom_styles()\n",
    "        \n",
    "        # Configura√ß√µes de gr√°ficos\n",
    "        plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "    def setup_custom_styles(self):\n",
    "        \"\"\"Configura estilos customizados para o relat√≥rio\"\"\"\n",
    "        # Estilo para cabe√ßalhos\n",
    "        self.styles.add(ParagraphStyle(\n",
    "            name='CustomTitle',\n",
    "            fontSize=16,\n",
    "            textColor=colors.darkblue,\n",
    "            spaceAfter=12,\n",
    "            alignment=1  # Center\n",
    "        ))\n",
    "        \n",
    "        # Estilo para se√ß√µes\n",
    "        self.styles.add(ParagraphStyle(\n",
    "            name='SectionHeader',\n",
    "            fontSize=12,\n",
    "            textColor=colors.darkgreen,\n",
    "            spaceBefore=8,\n",
    "            spaceAfter=6,\n",
    "            leftIndent=0\n",
    "        ))\n",
    "        \n",
    "        # Estilo para dados importantes\n",
    "        self.styles.add(ParagraphStyle(\n",
    "            name='Highlight',\n",
    "            fontSize=10,\n",
    "            textColor=colors.darkred,\n",
    "            spaceBefore=4,\n",
    "            spaceAfter=4\n",
    "        ))\n",
    "        \n",
    "        # Estilo para texto pequeno\n",
    "        self.styles.add(ParagraphStyle(\n",
    "            name='SmallText',\n",
    "            fontSize=8,\n",
    "            spaceBefore=2,\n",
    "            spaceAfter=2\n",
    "        ))\n",
    "    \n",
    "    def generate_comprehensive_report(self, complete_analysis, df, output_path=None):\n",
    "        \"\"\"Gera relat√≥rio PDF abrangente com todas as an√°lises\"\"\"\n",
    "        print(\"\\nüìã GERANDO RELAT√ìRIO AVAN√áADO...\")\n",
    "        \n",
    "        try:\n",
    "            if output_path is None:\n",
    "                output_path = os.path.join(\n",
    "                    self.config.get('model_persistence.cache_directory', './cache'),\n",
    "                    f\"WDO_Report_Advanced_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
    "                )\n",
    "            \n",
    "            doc = SimpleDocTemplate(\n",
    "                output_path,\n",
    "                pagesize=letter,\n",
    "                rightMargin=50, leftMargin=50, \n",
    "                topMargin=50, bottomMargin=50\n",
    "            )\n",
    "            \n",
    "            elements = []\n",
    "            \n",
    "            # 1. Cabe√ßalho do relat√≥rio\n",
    "            elements.extend(self._create_report_header(complete_analysis))\n",
    "            elements.append(Spacer(1, 0.3*inch))\n",
    "            \n",
    "            # 2. Resumo executivo\n",
    "            elements.extend(self._create_executive_summary_section(complete_analysis))\n",
    "            elements.append(Spacer(1, 0.2*inch))\n",
    "            \n",
    "            # 3. An√°lise de mercado atual\n",
    "            elements.extend(self._create_market_analysis_section(complete_analysis))\n",
    "            elements.append(Spacer(1, 0.2*inch))\n",
    "            \n",
    "            # 4. An√°lise ML e predi√ß√µes\n",
    "            elements.extend(self._create_ml_analysis_section(complete_analysis))\n",
    "            elements.append(Spacer(1, 0.2*inch))\n",
    "            \n",
    "            # 5. N√≠veis de suporte e resist√™ncia\n",
    "            elements.extend(self._create_sr_levels_section(complete_analysis))\n",
    "            elements.append(Spacer(1, 0.2*inch))\n",
    "            \n",
    "            # 6. Sinais de trading\n",
    "            elements.extend(self._create_trading_signals_section(complete_analysis))\n",
    "            elements.append(Spacer(1, 0.2*inch))\n",
    "            \n",
    "            # 7. An√°lise de risco\n",
    "            elements.extend(self._create_risk_analysis_section(complete_analysis))\n",
    "            elements.append(Spacer(1, 0.2*inch))\n",
    "            \n",
    "            # 8. Oportunidades identificadas\n",
    "            elements.extend(self._create_opportunities_section(complete_analysis))\n",
    "            elements.append(Spacer(1, 0.2*inch))\n",
    "            \n",
    "            # 9. Recomenda√ß√µes finais\n",
    "            elements.extend(self._create_recommendations_section(complete_analysis))\n",
    "            elements.append(Spacer(1, 0.2*inch))\n",
    "            \n",
    "            # 10. Ap√™ndice t√©cnico\n",
    "            elements.extend(self._create_technical_appendix(complete_analysis))\n",
    "            \n",
    "            # 11. Rodap√©\n",
    "            elements.extend(self._create_report_footer(complete_analysis))\n",
    "            \n",
    "            # Gerar PDF\n",
    "            doc.build(elements)\n",
    "            \n",
    "            print(f\"‚úÖ Relat√≥rio avan√ßado gerado: {output_path}\")\n",
    "            \n",
    "            # Gerar tamb√©m vers√£o com gr√°ficos\n",
    "            self._generate_charts_report(complete_analysis, df, output_path.replace('.pdf', '_charts.pdf'))\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na gera√ß√£o do relat√≥rio: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _create_report_header(self, analysis):\n",
    "        \"\"\"Cria cabe√ßalho do relat√≥rio\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        # T√≠tulo principal\n",
    "        elements.append(Paragraph(\n",
    "            \"RELAT√ìRIO AVAN√áADO DE AN√ÅLISE WDO v21\",\n",
    "            self.styles['CustomTitle']\n",
    "        ))\n",
    "        \n",
    "        # Informa√ß√µes b√°sicas\n",
    "        timestamp = analysis.get('timestamp', datetime.now())\n",
    "        current_price = analysis['market_data']['current_price']\n",
    "        \n",
    "        elements.append(Paragraph(\n",
    "            f\"Data/Hora: {timestamp.strftime('%d/%m/%Y %H:%M:%S')}\",\n",
    "            self.styles['Normal']\n",
    "        ))\n",
    "        \n",
    "        elements.append(Paragraph(\n",
    "            f\"Pre√ßo Atual: {format_currency_br(current_price)}\",\n",
    "            self.styles['Highlight']\n",
    "        ))\n",
    "        \n",
    "        # Per√≠odo de an√°lise\n",
    "        period_info = analysis['market_data'].get('analysis_period', {})\n",
    "        if period_info.get('start') and period_info.get('end'):\n",
    "            start_date = period_info['start'].strftime('%d/%m/%Y')\n",
    "            end_date = period_info['end'].strftime('%d/%m/%Y')\n",
    "            elements.append(Paragraph(\n",
    "                f\"Per√≠odo Analisado: {start_date} at√© {end_date}\",\n",
    "                self.styles['Normal']\n",
    "            ))\n",
    "        \n",
    "        total_records = analysis['market_data'].get('total_records', 0)\n",
    "        elements.append(Paragraph(\n",
    "            f\"Total de Registros: {total_records:,}\",\n",
    "            self.styles['Normal']\n",
    "        ))\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _create_executive_summary_section(self, analysis):\n",
    "        \"\"\"Cria se√ß√£o de resumo executivo\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        elements.append(Paragraph(\"üìä RESUMO EXECUTIVO\", self.styles['SectionHeader']))\n",
    "        \n",
    "        exec_summary = analysis.get('executive_summary', {})\n",
    "        \n",
    "        # Sentimento geral\n",
    "        sentiment = exec_summary.get('overall_sentiment', 'NEUTRO')\n",
    "        confidence = exec_summary.get('confidence_level', 'MODERADA')\n",
    "        \n",
    "        elements.append(Paragraph(\n",
    "            f\"‚Ä¢ Sentimento Geral: <b>{sentiment}</b>\",\n",
    "            self.styles['Normal']\n",
    "        ))\n",
    "        \n",
    "        elements.append(Paragraph(\n",
    "            f\"‚Ä¢ N√≠vel de Confian√ßa: <b>{confidence}</b>\",\n",
    "            self.styles['Normal']\n",
    "        ))\n",
    "        \n",
    "        # Recomenda√ß√£o principal\n",
    "        recommendations = analysis.get('recommendations', {})\n",
    "        primary_rec = recommendations.get('primary_recommendation', 'AGUARDAR')\n",
    "        \n",
    "        elements.append(Paragraph(\n",
    "            f\"‚Ä¢ Recomenda√ß√£o: <b>{primary_rec}</b>\",\n",
    "            self.styles['Highlight']\n",
    "        ))\n",
    "        \n",
    "        # Insights principais\n",
    "        key_insights = exec_summary.get('key_insights', [])\n",
    "        if key_insights:\n",
    "            elements.append(Paragraph(\"Principais Insights:\", self.styles['Normal']))\n",
    "            for insight in key_insights[:5]:\n",
    "                # Remove emojis para PDF\n",
    "                clean_insight = insight.encode('ascii', 'ignore').decode('ascii')\n",
    "                elements.append(Paragraph(f\"  - {clean_insight}\", self.styles['SmallText']))\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _create_market_analysis_section(self, analysis):\n",
    "        \"\"\"Cria se√ß√£o de an√°lise de mercado\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        elements.append(Paragraph(\"üìà AN√ÅLISE T√âCNICA DE MERCADO\", self.styles['SectionHeader']))\n",
    "        \n",
    "        technical = analysis.get('technical_analysis', {})\n",
    "        \n",
    "        # Tend√™ncia\n",
    "        trend_info = technical.get('trend', {})\n",
    "        if trend_info:\n",
    "            elements.append(Paragraph(\n",
    "                f\"Tend√™ncia: <b>{trend_info.get('direction', 'N/A')}</b> (For√ßa: {trend_info.get('strength', 0):.1f}%)\",\n",
    "                self.styles['Normal']\n",
    "            ))\n",
    "        \n",
    "        # Momentum\n",
    "        momentum = technical.get('momentum', {})\n",
    "        if momentum:\n",
    "            elements.append(Paragraph(\"Indicadores de Momentum:\", self.styles['Normal']))\n",
    "            \n",
    "            for indicator, data in momentum.items():\n",
    "                if isinstance(data, dict):\n",
    "                    value = data.get('value', 'N/A')\n",
    "                    signal = data.get('signal', 'N/A')\n",
    "                    elements.append(Paragraph(\n",
    "                        f\"  ‚Ä¢ {indicator.upper()}: {value:.2f if isinstance(value, (int, float)) else value} ({signal})\",\n",
    "                        self.styles['SmallText']\n",
    "                    ))\n",
    "        \n",
    "        # Volatilidade\n",
    "        volatility = technical.get('volatility', {})\n",
    "        if volatility:\n",
    "            vol_level = volatility.get('level', 'NORMAL')\n",
    "            current_vol = volatility.get('current', 0)\n",
    "            elements.append(Paragraph(\n",
    "                f\"Volatilidade: <b>{vol_level}</b> (Atual: {current_vol:.4f})\",\n",
    "                self.styles['Normal']\n",
    "            ))\n",
    "        \n",
    "        # Volume\n",
    "        volume_analysis = technical.get('volume', {})\n",
    "        if volume_analysis:\n",
    "            activity = volume_analysis.get('activity', 'NORMAL')\n",
    "            change = volume_analysis.get('change', 0)\n",
    "            elements.append(Paragraph(\n",
    "                f\"Atividade de Volume: <b>{activity}</b> (Varia√ß√£o: {change:+.2f}%)\",\n",
    "                self.styles['Normal']\n",
    "            ))\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _create_ml_analysis_section(self, analysis):\n",
    "        \"\"\"Cria se√ß√£o de an√°lise ML\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        elements.append(Paragraph(\"ü§ñ AN√ÅLISE MACHINE LEARNING\", self.styles['SectionHeader']))\n",
    "        \n",
    "        ml_analysis = analysis.get('ml_analysis', {})\n",
    "        \n",
    "        # Modelos treinados\n",
    "        models_info = ml_analysis.get('models_trained', {})\n",
    "        if models_info:\n",
    "            total_models = models_info.get('total_models', 0)\n",
    "            cached_models = models_info.get('cached_models', 0)\n",
    "            new_models = models_info.get('new_models', 0)\n",
    "            \n",
    "            elements.append(Paragraph(\n",
    "                f\"Modelos Processados: {total_models} (Cache: {cached_models}, Novos: {new_models})\",\n",
    "                self.styles['Normal']\n",
    "            ))\n",
    "        \n",
    "        # Performance dos modelos\n",
    "        model_performance = ml_analysis.get('model_performance', {})\n",
    "        if model_performance:\n",
    "            elements.append(Paragraph(\"Performance dos Modelos:\", self.styles['Normal']))\n",
    "            \n",
    "            # Ordenar por MAE\n",
    "            sorted_models = sorted(model_performance.items(), key=lambda x: x[1].get('mae', float('inf')))\n",
    "            \n",
    "            for i, (model_name, perf) in enumerate(sorted_models[:5], 1):\n",
    "                mae = perf.get('mae', 0)\n",
    "                r2 = perf.get('r2', 0)\n",
    "                confidence = perf.get('confidence', 0)\n",
    "                \n",
    "                rank_emoji = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else f\"{i}¬∫\"\n",
    "                \n",
    "                elements.append(Paragraph(\n",
    "                    f\"  {rank_emoji} {model_name}: MAE={mae:.6f}, R¬≤={r2:.4f}, Conf={confidence:.1f}%\",\n",
    "                    self.styles['SmallText']\n",
    "                ))\n",
    "        \n",
    "        # Predi√ß√µes e consenso\n",
    "        predictions = ml_analysis.get('predictions', {})\n",
    "        if predictions:\n",
    "            consensus = predictions.get('consensus', {})\n",
    "            if consensus:\n",
    "                direction = consensus.get('direction', 'N/A')\n",
    "                confidence = consensus.get('confidence', 0)\n",
    "                predicted_price = consensus.get('predicted_price', 0)\n",
    "                price_change = consensus.get('price_change', 0)\n",
    "                \n",
    "                elements.append(Paragraph(\"Consenso das Predi√ß√µes:\", self.styles['Normal']))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  ‚Ä¢ Dire√ß√£o: <b>{direction}</b> (Confian√ßa: {confidence:.1f}%)\",\n",
    "                    self.styles['Normal']\n",
    "                ))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  ‚Ä¢ Pre√ßo Previsto: {format_currency_br(predicted_price)}\",\n",
    "                    self.styles['Normal']\n",
    "                ))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  ‚Ä¢ Varia√ß√£o Esperada: <b>{price_change:+.2f}%</b>\",\n",
    "                    self.styles['Highlight']\n",
    "                ))\n",
    "            \n",
    "            # An√°lise de confian√ßa\n",
    "            confidence_analysis = predictions.get('confidence_analysis')\n",
    "            if confidence_analysis:\n",
    "                overall_assessment = confidence_analysis.get('overall_assessment', {})\n",
    "                confidence_level = overall_assessment.get('confidence_level', 'MODERADA')\n",
    "                reliability_score = overall_assessment.get('reliability_score', 0)\n",
    "                \n",
    "                elements.append(Paragraph(\n",
    "                    f\"Avalia√ß√£o Geral: <b>{confidence_level}</b> (Score: {reliability_score:.1f}%)\",\n",
    "                    self.styles['Normal']\n",
    "                ))\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _create_sr_levels_section(self, analysis):\n",
    "        \"\"\"Cria se√ß√£o de n√≠veis S/R\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        elements.append(Paragraph(\"üéØ N√çVEIS DE SUPORTE E RESIST√äNCIA\", self.styles['SectionHeader']))\n",
    "        \n",
    "        sr_info = analysis.get('support_resistance', {})\n",
    "        \n",
    "        # Resist√™ncias\n",
    "        resistances = sr_info.get('resistances', [])\n",
    "        if resistances:\n",
    "            elements.append(Paragraph(\"üî¥ Resist√™ncias:\", self.styles['Normal']))\n",
    "            \n",
    "            for i, res in enumerate(resistances[:8], 1):\n",
    "                price = res['price']\n",
    "                strength = res['strength']\n",
    "                touches = res['touches']\n",
    "                volume = res['volume']\n",
    "                \n",
    "                cluster_info = \"\"\n",
    "                if 'cluster_info' in res:\n",
    "                    cluster_info = f\" (Cluster {res['cluster_info']['label']})\"\n",
    "                \n",
    "                elements.append(Paragraph(\n",
    "                    f\"  R{i}. {format_currency_br(price)} - For√ßa: {strength:.1f}% - \"\n",
    "                    f\"Toques: {touches} - Vol: {format_volume_br(volume)}{cluster_info}\",\n",
    "                    self.styles['SmallText']\n",
    "                ))\n",
    "        \n",
    "        # Suportes\n",
    "        supports = sr_info.get('supports', [])\n",
    "        if supports:\n",
    "            elements.append(Paragraph(\"üü¢ Suportes:\", self.styles['Normal']))\n",
    "            \n",
    "            for i, sup in enumerate(supports[:8], 1):\n",
    "                price = sup['price']\n",
    "                strength = sup['strength']\n",
    "                touches = sup['touches']\n",
    "                volume = sup['volume']\n",
    "                \n",
    "                cluster_info = \"\"\n",
    "                if 'cluster_info' in sup:\n",
    "                    cluster_info = f\" (Cluster {sup['cluster_info']['label']})\"\n",
    "                \n",
    "                elements.append(Paragraph(\n",
    "                    f\"  S{i}. {format_currency_br(price)} - For√ßa: {strength:.1f}% - \"\n",
    "                    f\"Toques: {touches} - Vol: {format_volume_br(volume)}{cluster_info}\",\n",
    "                    self.styles['SmallText']\n",
    "                ))\n",
    "        \n",
    "        # N√≠veis mais pr√≥ximos\n",
    "        nearest_support = sr_info.get('nearest_support')\n",
    "        nearest_resistance = sr_info.get('nearest_resistance')\n",
    "        \n",
    "        if nearest_support or nearest_resistance:\n",
    "            elements.append(Paragraph(\"N√≠veis Cr√≠ticos:\", self.styles['Normal']))\n",
    "            \n",
    "            if nearest_support:\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  ‚Ä¢ Suporte mais pr√≥ximo: <b>{format_currency_br(nearest_support['price'])}</b>\",\n",
    "                    self.styles['Highlight']\n",
    "                ))\n",
    "            \n",
    "            if nearest_resistance:\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  ‚Ä¢ Resist√™ncia mais pr√≥xima: <b>{format_currency_br(nearest_resistance['price'])}</b>\",\n",
    "                    self.styles['Highlight']\n",
    "                ))\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _create_trading_signals_section(self, analysis):\n",
    "        \"\"\"Cria se√ß√£o de sinais de trading\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        elements.append(Paragraph(\"üìä SINAIS DE TRADING\", self.styles['SectionHeader']))\n",
    "        \n",
    "        ml_analysis = analysis.get('ml_analysis', {})\n",
    "        predictions = ml_analysis.get('predictions', {})\n",
    "        \n",
    "        if predictions and predictions.get('trading_signals'):\n",
    "            trading_signals = predictions['trading_signals']\n",
    "            signals = trading_signals.get('signals', [])\n",
    "            summary = trading_signals.get('summary', {})\n",
    "            \n",
    "            # Resumo dos sinais\n",
    "            total_signals = summary.get('total_signals', 0)\n",
    "            buy_signals = summary.get('buy_signals', 0)\n",
    "            sell_signals = summary.get('sell_signals', 0)\n",
    "            avg_confidence = summary.get('avg_confidence', 0)\n",
    "            \n",
    "            elements.append(Paragraph(\n",
    "                f\"Total de Sinais: {total_signals} (BUY: {buy_signals}, SELL: {sell_signals})\",\n",
    "                self.styles['Normal']\n",
    "            ))\n",
    "            \n",
    "            if avg_confidence > 0:\n",
    "                elements.append(Paragraph(\n",
    "                    f\"Confian√ßa M√©dia: {avg_confidence:.1f}%\",\n",
    "                    self.styles['Normal']\n",
    "                ))\n",
    "            \n",
    "            # Melhor sinal\n",
    "            best_signal = summary.get('best_signal')\n",
    "            if best_signal:\n",
    "                elements.append(Paragraph(\"üèÜ Melhor Sinal:\", self.styles['Normal']))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  ‚Ä¢ Tipo: <b>{best_signal['type']}</b>\",\n",
    "                    self.styles['Normal']\n",
    "                ))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  ‚Ä¢ Confian√ßa: {best_signal['confidence']:.1f}%\",\n",
    "                    self.styles['Normal']\n",
    "                ))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  ‚Ä¢ Entrada: {format_currency_br(best_signal['entry_price'])}\",\n",
    "                    self.styles['Normal']\n",
    "                ))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  ‚Ä¢ Alvo: {format_currency_br(best_signal['target_price'])}\",\n",
    "                    self.styles['Normal']\n",
    "                ))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  ‚Ä¢ Stop Loss: {format_currency_br(best_signal['stop_loss'])}\",\n",
    "                    self.styles['Normal']\n",
    "                ))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  ‚Ä¢ Risk/Reward: {best_signal['risk_reward_ratio']:.2f}\",\n",
    "                    self.styles['Normal']\n",
    "                ))\n",
    "            \n",
    "            # Outros sinais importantes\n",
    "            if len(signals) > 1:\n",
    "                elements.append(Paragraph(\"Outros Sinais Relevantes:\", self.styles['Normal']))\n",
    "                \n",
    "                for signal in signals[1:4]:  # Pr√≥ximos 3 sinais\n",
    "                    elements.append(Paragraph(\n",
    "                        f\"  ‚Ä¢ {signal['type']} ({signal['source']}) - \"\n",
    "                        f\"Conf: {signal['confidence']:.1f}% - \"\n",
    "                        f\"R/R: {signal['risk_reward_ratio']:.2f}\",\n",
    "                        self.styles['SmallText']\n",
    "                    ))\n",
    "        else:\n",
    "            elements.append(Paragraph(\n",
    "                \"Nenhum sinal de trading gerado com confian√ßa suficiente.\",\n",
    "                self.styles['Normal']\n",
    "            ))\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _create_risk_analysis_section(self, analysis):\n",
    "        \"\"\"Cria se√ß√£o de an√°lise de risco\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        elements.append(Paragraph(\"‚ö†Ô∏è AN√ÅLISE DE RISCO\", self.styles['SectionHeader']))\n",
    "        \n",
    "        risk_analysis = analysis.get('risk_analysis', {})\n",
    "        \n",
    "        if risk_analysis:\n",
    "            overall_risk = risk_analysis.get('overall_risk', 'MODERADO')\n",
    "            elements.append(Paragraph(\n",
    "                f\"Risco Geral: <b>{overall_risk}</b>\",\n",
    "                self.styles['Highlight']\n",
    "            ))\n",
    "            \n",
    "            # Tipos espec√≠ficos de risco\n",
    "            volatility_risk = risk_analysis.get('volatility_risk', 'NORMAL')\n",
    "            liquidity_risk = risk_analysis.get('liquidity_risk', 'BAIXO')\n",
    "            prediction_risk = risk_analysis.get('prediction_risk', 'MODERADO')\n",
    "            \n",
    "            elements.append(Paragraph(\n",
    "                f\"‚Ä¢ Risco de Volatilidade: {volatility_risk}\",\n",
    "                self.styles['Normal']\n",
    "            ))\n",
    "            elements.append(Paragraph(\n",
    "                f\"‚Ä¢ Risco de Liquidez: {liquidity_risk}\",\n",
    "                self.styles['Normal']\n",
    "            ))\n",
    "            elements.append(Paragraph(\n",
    "                f\"‚Ä¢ Risco de Predi√ß√£o: {prediction_risk}\",\n",
    "                self.styles['Normal']\n",
    "            ))\n",
    "            \n",
    "            # Fatores de risco\n",
    "            risk_factors = risk_analysis.get('risk_factors', [])\n",
    "            if risk_factors:\n",
    "                elements.append(Paragraph(\"Fatores de Risco Identificados:\", self.styles['Normal']))\n",
    "                for factor in risk_factors:\n",
    "                    # Remove emojis para PDF\n",
    "                    clean_factor = factor.encode('ascii', 'ignore').decode('ascii')\n",
    "                    elements.append(Paragraph(f\"  - {clean_factor}\", self.styles['SmallText']))\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _create_opportunities_section(self, analysis):\n",
    "        \"\"\"Cria se√ß√£o de oportunidades\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        elements.append(Paragraph(\"üí° OPORTUNIDADES IDENTIFICADAS\", self.styles['SectionHeader']))\n",
    "        \n",
    "        opportunities = analysis.get('opportunities', [])\n",
    "        \n",
    "        if opportunities:\n",
    "            for i, opp in enumerate(opportunities[:6], 1):\n",
    "                opp_type = opp.get('type', 'N/A')\n",
    "                description = opp.get('description', 'N/A')\n",
    "                probability = opp.get('probability', 0)\n",
    "                timeframe = opp.get('timeframe', 'N/A')\n",
    "                \n",
    "                # Remove emojis para PDF\n",
    "                clean_description = description.encode('ascii', 'ignore').decode('ascii')\n",
    "                \n",
    "                elements.append(Paragraph(\n",
    "                    f\"{i}. {opp_type}\",\n",
    "                    self.styles['Normal']\n",
    "                ))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"   {clean_description}\",\n",
    "                    self.styles['SmallText']\n",
    "                ))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"   Probabilidade: {probability:.1f}% | Prazo: {timeframe}\",\n",
    "                    self.styles['SmallText']\n",
    "                ))\n",
    "                \n",
    "                if 'price_level' in opp:\n",
    "                    elements.append(Paragraph(\n",
    "                        f\"   N√≠vel: {format_currency_br(opp['price_level'])}\",\n",
    "                        self.styles['SmallText']\n",
    "                    ))\n",
    "                \n",
    "                elements.append(Spacer(1, 0.1*inch))\n",
    "        else:\n",
    "            elements.append(Paragraph(\n",
    "                \"Nenhuma oportunidade de alto potencial identificada no momento.\",\n",
    "                self.styles['Normal']\n",
    "            ))\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _create_recommendations_section(self, analysis):\n",
    "        \"\"\"Cria se√ß√£o de recomenda√ß√µes\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        elements.append(Paragraph(\"üíº RECOMENDA√á√ïES FINAIS\", self.styles['SectionHeader']))\n",
    "        \n",
    "        recommendations = analysis.get('recommendations', {})\n",
    "        \n",
    "        if recommendations:\n",
    "            # Recomenda√ß√£o prim√°ria\n",
    "            primary_rec = recommendations.get('primary_recommendation', 'AGUARDAR')\n",
    "            elements.append(Paragraph(\n",
    "                f\"Recomenda√ß√£o Principal: <b>{primary_rec}</b>\",\n",
    "                self.styles['Highlight']\n",
    "            ))\n",
    "            \n",
    "            # Itens de a√ß√£o\n",
    "            action_items = recommendations.get('action_items', [])\n",
    "            if action_items:\n",
    "                elements.append(Paragraph(\"A√ß√µes Recomendadas:\", self.styles['Normal']))\n",
    "                for item in action_items:\n",
    "                    clean_item = item.encode('ascii', 'ignore').decode('ascii')\n",
    "                    elements.append(Paragraph(f\"  ‚Ä¢ {clean_item}\", self.styles['SmallText']))\n",
    "            \n",
    "            # Gest√£o de risco\n",
    "            risk_management = recommendations.get('risk_management', [])\n",
    "            if risk_management:\n",
    "                elements.append(Paragraph(\"Gest√£o de Risco:\", self.styles['Normal']))\n",
    "                for item in risk_management:\n",
    "                    clean_item = item.encode('ascii', 'ignore').decode('ascii')\n",
    "                    elements.append(Paragraph(f\"  ‚Ä¢ {clean_item}\", self.styles['SmallText']))\n",
    "            \n",
    "            # Pontos de monitoramento\n",
    "            monitoring_points = recommendations.get('monitoring_points', [])\n",
    "            if monitoring_points:\n",
    "                elements.append(Paragraph(\"Pontos de Monitoramento:\", self.styles['Normal']))\n",
    "                for point in monitoring_points:\n",
    "                    clean_point = point.encode('ascii', 'ignore').decode('ascii')\n",
    "                    elements.append(Paragraph(f\"  ‚Ä¢ {clean_point}\", self.styles['SmallText']))\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _create_technical_appendix(self, analysis):\n",
    "        \"\"\"Cria ap√™ndice t√©cnico\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        elements.append(Paragraph(\"üìã AP√äNDICE T√âCNICO\", self.styles['SectionHeader']))\n",
    "        \n",
    "        # Estat√≠sticas de performance dos modelos\n",
    "        ml_analysis = analysis.get('ml_analysis', {})\n",
    "        model_performance = ml_analysis.get('model_performance', {})\n",
    "        \n",
    "        if model_performance:\n",
    "            elements.append(Paragraph(\"Performance Detalhada dos Modelos:\", self.styles['Normal']))\n",
    "            \n",
    "            for model_name, perf in model_performance.items():\n",
    "                elements.append(Paragraph(f\"<b>{model_name}:</b>\", self.styles['SmallText']))\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  MAE: {perf.get('mae', 0):.8f} | \"\n",
    "                    f\"R¬≤: {perf.get('r2', 0):.6f} | \"\n",
    "                    f\"Confian√ßa: {perf.get('confidence', 0):.2f}%\",\n",
    "                    self.styles['SmallText']\n",
    "                ))\n",
    "        \n",
    "        # Informa√ß√µes sobre clustering\n",
    "        zones = analysis.get('congestion_zones', [])\n",
    "        if zones:\n",
    "            elements.append(Paragraph(\"Zonas de Congest√£o (Clustering):\", self.styles['Normal']))\n",
    "            for i, zone in enumerate(zones[:3], 1):\n",
    "                elements.append(Paragraph(\n",
    "                    f\"  Zona {i}: Centro={format_currency_br(zone['center_price'])}, \"\n",
    "                    f\"Signific√¢ncia={zone['significance']:.1f}%, \"\n",
    "                    f\"Tipo={zone['type']}\",\n",
    "                    self.styles['SmallText']\n",
    "                ))\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _create_report_footer(self, analysis):\n",
    "        \"\"\"Cria rodap√© do relat√≥rio\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        elements.append(Spacer(1, 0.3*inch))\n",
    "        elements.append(Paragraph(\"‚îÄ\" * 80, self.styles['SmallText']))\n",
    "        \n",
    "        timestamp = analysis.get('timestamp', datetime.now())\n",
    "        total_records = analysis['market_data'].get('total_records', 0)\n",
    "        \n",
    "        elements.append(Paragraph(\n",
    "            f\"Relat√≥rio gerado automaticamente em {timestamp.strftime('%d/%m/%Y %H:%M:%S')}\",\n",
    "            self.styles['SmallText']\n",
    "        ))\n",
    "        \n",
    "        elements.append(Paragraph(\n",
    "            f\"Baseado em {total_records:,} registros de dados hist√≥ricos\",\n",
    "            self.styles['SmallText']\n",
    "        ))\n",
    "        \n",
    "        elements.append(Paragraph(\n",
    "            \"Sistema: WDO Analyzer v21 - Machine Learning + An√°lise T√©cnica + Clustering\",\n",
    "            self.styles['SmallText']\n",
    "        ))\n",
    "        \n",
    "        elements.append(Paragraph(\n",
    "            \"‚ö†Ô∏è Este relat√≥rio √© apenas informativo e n√£o constitui recomenda√ß√£o de investimento\",\n",
    "            self.styles['SmallText']\n",
    "        ))\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _generate_charts_report(self, analysis, df, output_path):\n",
    "        \"\"\"Gera relat√≥rio com gr√°ficos\"\"\"\n",
    "        try:\n",
    "            print(\"üìä Gerando gr√°ficos para o relat√≥rio...\")\n",
    "            \n",
    "            with PdfPages(output_path) as pdf:\n",
    "                # Gr√°fico 1: Pre√ßo e m√©dias m√≥veis\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                \n",
    "                # Usar √∫ltimos 200 pontos para visualiza√ß√£o\n",
    "                df_plot = df.tail(200).copy()\n",
    "                \n",
    "                ax.plot(df_plot.index, df_plot['√öLT. PRE√áO'], label='Pre√ßo', linewidth=1.5)\n",
    "                \n",
    "                if 'sma_20' in df_plot.columns:\n",
    "                    ax.plot(df_plot.index, df_plot['sma_20'], label='SMA 20', alpha=0.7)\n",
    "                \n",
    "                if 'sma_50' in df_plot.columns:\n",
    "                    ax.plot(df_plot.index, df_plot['sma_50'], label='SMA 50', alpha=0.7)\n",
    "                \n",
    "                # Adicionar n√≠veis S/R\n",
    "                sr_info = analysis.get('support_resistance', {})\n",
    "                current_price = analysis['market_data']['current_price']\n",
    "                \n",
    "                supports = sr_info.get('supports', [])[:3]  # Top 3\n",
    "                resistances = sr_info.get('resistances', [])[:3]  # Top 3\n",
    "                \n",
    "                for support in supports:\n",
    "                    ax.axhline(y=support['price'], color='green', linestyle='--', alpha=0.6, \n",
    "                              label=f\"Suporte {format_currency_br(support['price'])}\")\n",
    "                \n",
    "                for resistance in resistances:\n",
    "                    ax.axhline(y=resistance['price'], color='red', linestyle='--', alpha=0.6,\n",
    "                              label=f\"Resist√™ncia {format_currency_br(resistance['price'])}\")\n",
    "                \n",
    "                ax.set_title('An√°lise T√©cnica - Pre√ßo e N√≠veis S/R', fontsize=14)\n",
    "                ax.set_ylabel('Pre√ßo (R$)', fontsize=12)\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                pdf.savefig(fig, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # Gr√°fico 2: Indicadores de Momentum\n",
    "                if 'rsi' in df_plot.columns:\n",
    "                    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "                    \n",
    "                    # RSI\n",
    "                    ax1.plot(df_plot.index, df_plot['rsi'], label='RSI', color='purple')\n",
    "                    ax1.axhline(y=70, color='red', linestyle='--', alpha=0.7, label='Sobrecomprado')\n",
    "                    ax1.axhline(y=30, color='green', linestyle='--', alpha=0.7, label='Sobrevendido')\n",
    "                    ax1.axhline(y=50, color='gray', linestyle='-', alpha=0.5)\n",
    "                    ax1.set_title('RSI (Relative Strength Index)', fontsize=12)\n",
    "                    ax1.set_ylabel('RSI', fontsize=10)\n",
    "                    ax1.legend()\n",
    "                    ax1.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # MACD (se dispon√≠vel)\n",
    "                    if 'macd' in df_plot.columns and 'macd_signal' in df_plot.columns:\n",
    "                        ax2.plot(df_plot.index, df_plot['macd'], label='MACD', color='blue')\n",
    "                        ax2.plot(df_plot.index, df_plot['macd_signal'], label='Signal', color='red')\n",
    "                        \n",
    "                        if 'macd_histogram' in df_plot.columns:\n",
    "                            ax2.bar(df_plot.index, df_plot['macd_histogram'], \n",
    "                                   label='Histogram', alpha=0.6, color='gray')\n",
    "                        \n",
    "                        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "                        ax2.set_title('MACD', fontsize=12)\n",
    "                        ax2.set_ylabel('MACD', fontsize=10)\n",
    "                        ax2.legend()\n",
    "                        ax2.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    pdf.savefig(fig, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                \n",
    "                # Gr√°fico 3: Volume e An√°lise\n",
    "                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "                \n",
    "                # Volume\n",
    "                ax1.bar(df_plot.index, df_plot['VOL.'], alpha=0.7, color='skyblue')\n",
    "                if 'volume_sma' in df_plot.columns:\n",
    "                    ax1.plot(df_plot.index, df_plot['volume_sma'], color='red', label='M√©dia Volume')\n",
    "                ax1.set_title('An√°lise de Volume', fontsize=12)\n",
    "                ax1.set_ylabel('Volume', fontsize=10)\n",
    "                ax1.legend()\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Volatilidade\n",
    "                if 'volatility_20' in df_plot.columns:\n",
    "                    ax2.plot(df_plot.index, df_plot['volatility_20'], color='orange', label='Volatilidade 20')\n",
    "                    ax2.set_title('Volatilidade', fontsize=12)\n",
    "                    ax2.set_ylabel('Volatilidade', fontsize=10)\n",
    "                    ax2.legend()\n",
    "                    ax2.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                pdf.savefig(fig, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # Gr√°fico 4: Performance dos Modelos ML\n",
    "                ml_analysis = analysis.get('ml_analysis', {})\n",
    "                model_performance = ml_analysis.get('model_performance', {})\n",
    "                \n",
    "                if model_performance:\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "                    \n",
    "                    # MAE por modelo\n",
    "                    models = list(model_performance.keys())\n",
    "                    mae_values = [model_performance[m].get('mae', 0) for m in models]\n",
    "                    \n",
    "                    ax1.bar(models, mae_values, color='lightcoral', alpha=0.7)\n",
    "                    ax1.set_title('MAE por Modelo', fontsize=12)\n",
    "                    ax1.set_ylabel('MAE', fontsize=10)\n",
    "                    ax1.tick_params(axis='x', rotation=45)\n",
    "                    \n",
    "                    # R¬≤ por modelo\n",
    "                    r2_values = [model_performance[m].get('r2', 0) for m in models]\n",
    "                    ax2.bar(models, r2_values, color='lightgreen', alpha=0.7)\n",
    "                    ax2.set_title('R¬≤ por Modelo', fontsize=12)\n",
    "                    ax2.set_ylabel('R¬≤', fontsize=10)\n",
    "                    ax2.tick_params(axis='x', rotation=45)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    pdf.savefig(fig, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                \n",
    "                # Gr√°fico 5: Distribui√ß√£o de Predi√ß√µes\n",
    "                predictions = ml_analysis.get('predictions', {})\n",
    "                if predictions and predictions.get('individual_predictions'):\n",
    "                    individual_preds = predictions['individual_predictions']\n",
    "                    \n",
    "                    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                    \n",
    "                    model_names = list(individual_preds.keys())\n",
    "                    predicted_prices = [individual_preds[m]['predicted_price'] for m in model_names]\n",
    "                    confidences = [individual_preds[m]['confidence'] for m in model_names]\n",
    "                    \n",
    "                    # Scatter plot: Pre√ßo previsto vs Confian√ßa\n",
    "                    scatter = ax.scatter(predicted_prices, confidences, \n",
    "                                       s=100, alpha=0.7, c=range(len(model_names)), \n",
    "                                       cmap='viridis')\n",
    "                    \n",
    "                    # Adicionar labels\n",
    "                    for i, model in enumerate(model_names):\n",
    "                        ax.annotate(model, (predicted_prices[i], confidences[i]), \n",
    "                                   xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "                    \n",
    "                    # Linha do pre√ßo atual\n",
    "                    current_price = analysis['market_data']['current_price']\n",
    "                    ax.axvline(x=current_price, color='red', linestyle='--', \n",
    "                              label=f'Pre√ßo Atual: {format_currency_br(current_price)}')\n",
    "                    \n",
    "                    ax.set_title('Distribui√ß√£o das Predi√ß√µes ML', fontsize=14)\n",
    "                    ax.set_xlabel('Pre√ßo Previsto (R$)', fontsize=12)\n",
    "                    ax.set_ylabel('Confian√ßa (%)', fontsize=12)\n",
    "                    ax.legend()\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    pdf.savefig(fig, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                \n",
    "                print(f\"‚úÖ Gr√°ficos salvos em: {output_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro na gera√ß√£o de gr√°ficos: {e}\")\n",
    "\n",
    "class IntegratedMarketAnalyzer:\n",
    "    \"\"\"Analisador de mercado integrado - Classe principal do sistema\"\"\"\n",
    "    \n",
    "    def __init__(self, config_file=None, args=None):\n",
    "        # Configura√ß√£o\n",
    "        self.config = ConfigManager(config_file, args)\n",
    "        self.config.print_summary()\n",
    "        \n",
    "        # Componentes principais\n",
    "        self.enhanced_analyzer = EnhancedMarketAnalyzer(self.config)\n",
    "        self.report_generator = AdvancedReportGenerator(self.config)\n",
    "        \n",
    "        # Estado da an√°lise\n",
    "        self.last_analysis = None\n",
    "        self.data_cache = {}\n",
    "        \n",
    "        print(f\"\\n‚úÖ Sistema inicializado com sucesso!\")\n",
    "    \n",
    "    def load_all_wdo_data(self, data_dir=None):\n",
    "        \"\"\"Carrega todos os dados WDO dispon√≠veis\"\"\"\n",
    "        print(f\"\\nüìÇ CARREGAMENTO DE DADOS WDO\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Encontrar arquivos\n",
    "        if data_dir:\n",
    "            search_dir = data_dir\n",
    "            wdo_files = glob.glob(os.path.join(data_dir, 'WDO*.csv'))\n",
    "        else:\n",
    "            search_dir, wdo_files = find_wdo_files()\n",
    "        \n",
    "        if not wdo_files:\n",
    "            print(\"‚ùå Nenhum arquivo WDO encontrado!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"üìÅ Diret√≥rio: {search_dir}\")\n",
    "        print(f\"üìä Arquivos encontrados: {len(wdo_files)}\")\n",
    "        \n",
    "        # Carregar dados\n",
    "        all_data = []\n",
    "        loaded_count = 0\n",
    "        total_records = 0\n",
    "        \n",
    "        max_files = self.config.get('data.max_files', None)\n",
    "        files_to_process = wdo_files[:max_files] if max_files else wdo_files\n",
    "        \n",
    "        for i, file_path in enumerate(files_to_process):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                if len(df) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Padronizar colunas\n",
    "                df = self._standardize_columns(df, file_path)\n",
    "                \n",
    "                if df is not None and len(df) > 0:\n",
    "                    all_data.append(df)\n",
    "                    loaded_count += 1\n",
    "                    total_records += len(df)\n",
    "                    \n",
    "                    # Log progressivo\n",
    "                    if loaded_count <= 10 or loaded_count % 100 == 0:\n",
    "                        filename = os.path.basename(file_path)\n",
    "                        print(f\"‚úÖ {filename}: {len(df)} registros\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"‚ùå Nenhum arquivo v√°lido carregado!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nüìä RESUMO DO CARREGAMENTO:\")\n",
    "        print(f\"   ‚úÖ Arquivos processados: {loaded_count}/{len(files_to_process)}\")\n",
    "        print(f\"   üìà Total de registros: {total_records:,}\")\n",
    "        \n",
    "        # Combinar dados\n",
    "        print(\"üîÑ Combinando datasets...\")\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Limpar e ordenar\n",
    "        combined_df = self._clean_and_sort_data(combined_df)\n",
    "        \n",
    "        # Aplicar limites de performance\n",
    "        max_records = self.config.get('data.max_data_points', 500000)\n",
    "        if len(combined_df) > max_records:\n",
    "            print(f\"üìâ Limitando para performance: {max_records:,} registros mais recentes\")\n",
    "            combined_df = combined_df.tail(max_records)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset final: {len(combined_df):,} registros\")\n",
    "        \n",
    "        # Cache dos dados\n",
    "        self.data_cache['raw_data'] = combined_df\n",
    "        self.data_cache['load_timestamp'] = datetime.now()\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def _standardize_columns(self, df, file_path):\n",
    "        \"\"\"Padroniza colunas do DataFrame\"\"\"\n",
    "        try:\n",
    "            # Mapeamento de colunas poss√≠veis\n",
    "            column_mapping = {\n",
    "                'close': '√öLT. PRE√áO',\n",
    "                'Close': '√öLT. PRE√áO',\n",
    "                'CLOSE': '√öLT. PRE√áO',\n",
    "                'last': '√öLT. PRE√áO',\n",
    "                'price': '√öLT. PRE√áO',\n",
    "                \n",
    "                'volume': 'VOL.',\n",
    "                'Volume': 'VOL.',\n",
    "                'VOLUME': 'VOL.',\n",
    "                'tick_volume': 'VOL.',\n",
    "                'vol': 'VOL.',\n",
    "                \n",
    "                'high': 'PRE√áO M√ÅX.',\n",
    "                'High': 'PRE√áO M√ÅX.',\n",
    "                'HIGH': 'PRE√áO M√ÅX.',\n",
    "                'max': 'PRE√áO M√ÅX.',\n",
    "                \n",
    "                'low': 'PRE√áO M√çN.',\n",
    "                'Low': 'PRE√áO M√çN.',\n",
    "                'LOW': 'PRE√áO M√çN.',\n",
    "                'min': 'PRE√áO M√çN.',\n",
    "                \n",
    "                'open': 'PRE√áO ABERT.',\n",
    "                'Open': 'PRE√áO ABERT.',\n",
    "                'OPEN': 'PRE√áO ABERT.',\n",
    "                \n",
    "                'time': 'DATA',\n",
    "                'datetime': 'DATA',\n",
    "                'date': 'DATA',\n",
    "                'timestamp': 'DATA'\n",
    "            }\n",
    "            \n",
    "            # Aplicar mapeamento\n",
    "            df = df.rename(columns=column_mapping)\n",
    "            \n",
    "            # Verificar coluna essencial\n",
    "            if '√öLT. PRE√áO' not in df.columns:\n",
    "                return None\n",
    "            \n",
    "            # Adicionar coluna de volume se n√£o existir\n",
    "            if 'VOL.' not in df.columns:\n",
    "                df['VOL.'] = 1000  # Volume padr√£o\n",
    "            \n",
    "            # Processar coluna de data\n",
    "            if 'DATA' not in df.columns:\n",
    "                # Tentar extrair data do nome do arquivo\n",
    "                filename = os.path.basename(file_path)\n",
    "                date_match = re.search(r'(\\d{8})', filename)\n",
    "                \n",
    "                if date_match:\n",
    "                    base_date = pd.to_datetime(date_match.group(1), format='%Y%m%d')\n",
    "                    df['DATA'] = pd.date_range(start=base_date, periods=len(df), freq='1min')\n",
    "                else:\n",
    "                    df['DATA'] = pd.date_range(start='2024-01-01', periods=len(df), freq='1min')\n",
    "            else:\n",
    "                df['DATA'] = pd.to_datetime(df['DATA'], errors='coerce')\n",
    "            \n",
    "            # Garantir OHLC se poss√≠vel\n",
    "            if 'PRE√áO M√ÅX.' not in df.columns:\n",
    "                spread = df['√öLT. PRE√áO'] * 0.001\n",
    "                df['PRE√áO M√ÅX.'] = df['√öLT. PRE√áO'] + spread\n",
    "                df['PRE√áO M√çN.'] = df['√öLT. PRE√áO'] - spread\n",
    "                df['PRE√áO ABERT.'] = df['√öLT. PRE√áO'].shift(1).fillna(df['√öLT. PRE√áO'])\n",
    "            \n",
    "            # Validar dados\n",
    "            df = df.dropna(subset=['√öLT. PRE√áO'])\n",
    "            df = df[df['√öLT. PRE√áO'] > 0]\n",
    "            df = df[df['VOL.'] > 0]\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro na padroniza√ß√£o de {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _clean_and_sort_data(self, df):\n",
    "        \"\"\"Limpa e ordena dados\"\"\"\n",
    "        try:\n",
    "            # Remover duplicatas por data\n",
    "            if 'DATA' in df.columns:\n",
    "                df = df.sort_values('DATA')\n",
    "                df = df.drop_duplicates(subset=['DATA'], keep='last')\n",
    "            \n",
    "            # Remover outliers extremos (pre√ßos muito fora da m√©dia)\n",
    "            price_mean = df['√öLT. PRE√áO'].mean()\n",
    "            price_std = df['√öLT. PRE√áO'].std()\n",
    "            \n",
    "            # Manter apenas pre√ßos dentro de 5 desvios padr√£o\n",
    "            price_filter = (\n",
    "                (df['√öLT. PRE√áO'] >= price_mean - 5 * price_std) &\n",
    "                (df['√öLT. PRE√áO'] <= price_mean + 5 * price_std)\n",
    "            )\n",
    "            df = df[price_filter]\n",
    "            \n",
    "            # Reset index\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro na limpeza de dados: {e}\")\n",
    "            return df\n",
    "    \n",
    "    def run_complete_analysis(self, df=None, force_retrain=False, generate_reports=True):\n",
    "        \"\"\"Executa an√°lise completa do mercado\"\"\"\n",
    "        print(f\"\\nüöÄ INICIANDO AN√ÅLISE COMPLETA\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        analysis_start = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Usar dados do cache se n√£o fornecidos\n",
    "            if df is None:\n",
    "                df = self.data_cache.get('raw_data')\n",
    "                if df is None:\n",
    "                    print(\"‚ùå Nenhum dado dispon√≠vel. Execute load_all_wdo_data() primeiro.\")\n",
    "                    return None\n",
    "            \n",
    "            # Executar an√°lise\n",
    "            complete_analysis = self.enhanced_analyzer.perform_complete_analysis(df, force_retrain)\n",
    "            \n",
    "            if complete_analysis is None:\n",
    "                print(\"‚ùå Falha na an√°lise completa\")\n",
    "                return None\n",
    "            \n",
    "            # Salvar an√°lise\n",
    "            self.last_analysis = complete_analysis\n",
    "            \n",
    "            # Gerar relat√≥rios se solicitado\n",
    "            if generate_reports:\n",
    "                print(f\"\\nüìã GERANDO RELAT√ìRIOS...\")\n",
    "                \n",
    "                # Relat√≥rio principal\n",
    "                main_report = self.report_generator.generate_comprehensive_report(\n",
    "                    complete_analysis, df\n",
    "                )\n",
    "                \n",
    "                if main_report:\n",
    "                    print(f\"‚úÖ Relat√≥rio principal: {main_report}\")\n",
    "                \n",
    "                # Relat√≥rio de texto simples\n",
    "                text_report = self._generate_text_summary(complete_analysis)\n",
    "                if text_report:\n",
    "                    print(f\"‚úÖ Resumo texto: {text_report}\")\n",
    "            \n",
    "            # Estat√≠sticas finais\n",
    "            analysis_time = (datetime.now() - analysis_start).total_seconds()\n",
    "            \n",
    "            print(f\"\\nüéâ AN√ÅLISE CONCLU√çDA COM SUCESSO!\")\n",
    "            print(f\"‚è±Ô∏è Tempo total: {analysis_time:.1f} segundos\")\n",
    "            \n",
    "            # Mostrar resumo r√°pido\n",
    "            print(f\"\\n{format_analysis_summary(complete_analysis)}\")\n",
    "            \n",
    "            return complete_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na an√°lise completa: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def _generate_text_summary(self, analysis):\n",
    "        \"\"\"Gera resumo em texto simples\"\"\"\n",
    "        try:\n",
    "            output_path = os.path.join(\n",
    "                self.config.get('model_persistence.cache_directory', './cache'),\n",
    "                f\"WDO_Summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "            )\n",
    "            \n",
    "            summary_text = format_analysis_summary(analysis)\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(summary_text)\n",
    "                f.write(f\"\\n\\n\" + \"=\" * 60)\n",
    "                f.write(f\"\\nRelat√≥rio detalhado dispon√≠vel em PDF\")\n",
    "                f.write(f\"\\nGerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro no resumo texto: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_latest_predictions(self):\n",
    "        \"\"\"Retorna √∫ltimas predi√ß√µes geradas\"\"\"\n",
    "        if not self.last_analysis:\n",
    "            return None\n",
    "        \n",
    "        ml_analysis = self.last_analysis.get('ml_analysis', {})\n",
    "        return ml_analysis.get('predictions')\n",
    "    \n",
    "    def get_trading_signals(self):\n",
    "        \"\"\"Retorna sinais de trading atuais\"\"\"\n",
    "        predictions = self.get_latest_predictions()\n",
    "        if not predictions:\n",
    "            return None\n",
    "        \n",
    "        return predictions.get('trading_signals')\n",
    "    \n",
    "    def get_support_resistance_levels(self):\n",
    "        \"\"\"Retorna n√≠veis de S/R atuais\"\"\"\n",
    "        if not self.last_analysis:\n",
    "            return None, None\n",
    "        \n",
    "        sr_info = self.last_analysis.get('support_resistance', {})\n",
    "        return sr_info.get('supports', []), sr_info.get('resistances', [])\n",
    "    \n",
    "    def export_analysis_json(self, output_path=None):\n",
    "        \"\"\"Exporta an√°lise completa em JSON\"\"\"\n",
    "        if not self.last_analysis:\n",
    "            print(\"‚ùå Nenhuma an√°lise dispon√≠vel para exportar\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            if output_path is None:\n",
    "                output_path = os.path.join(\n",
    "                    self.config.get('model_persistence.cache_directory', './cache'),\n",
    "                    f\"WDO_Analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "                )\n",
    "            \n",
    "            # Preparar dados para JSON (remover objetos n√£o serializ√°veis)\n",
    "            exportable_analysis = self._prepare_for_json_export(self.last_analysis)\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(exportable_analysis, f, indent=2, ensure_ascii=False, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ An√°lise exportada: {output_path}\")\n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na exporta√ß√£o JSON: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _prepare_for_json_export(self, analysis):\n",
    "        \"\"\"Prepara an√°lise para exporta√ß√£o JSON\"\"\"\n",
    "        try:\n",
    "            import copy\n",
    "            \n",
    "            def make_serializable(obj):\n",
    "                \"\"\"Converte objetos para formato serializ√°vel\"\"\"\n",
    "                if obj is None:\n",
    "                    return None\n",
    "                elif isinstance(obj, (int, float, str, bool)):\n",
    "                    return obj\n",
    "                elif isinstance(obj, datetime):\n",
    "                    return obj.isoformat()\n",
    "                elif isinstance(obj, (list, tuple)):\n",
    "                    return [make_serializable(item) for item in obj]\n",
    "                elif isinstance(obj, dict):\n",
    "                    result = {}\n",
    "                    for key, value in obj.items():\n",
    "                        try:\n",
    "                            result[str(key)] = make_serializable(value)\n",
    "                        except Exception:\n",
    "                            result[str(key)] = str(value)\n",
    "                    return result\n",
    "                elif hasattr(obj, '__dict__'):\n",
    "                    # Para objetos com atributos, tentar converter para dict\n",
    "                    try:\n",
    "                        return make_serializable(obj.__dict__)\n",
    "                    except:\n",
    "                        return str(obj)\n",
    "                else:\n",
    "                    return str(obj)\n",
    "            \n",
    "            # Fazer c√≥pia e converter\n",
    "            exportable = make_serializable(analysis)\n",
    "            \n",
    "            # Limpeza espec√≠fica para remover objetos problem√°ticos\n",
    "            if 'ml_analysis' in exportable and exportable['ml_analysis']:\n",
    "                ml_analysis = exportable['ml_analysis']\n",
    "                \n",
    "                # Remover objetos de modelo que n√£o s√£o serializ√°veis\n",
    "                if 'predictions' in ml_analysis and ml_analysis['predictions']:\n",
    "                    predictions = ml_analysis['predictions']\n",
    "                    \n",
    "                    # Limpar predi√ß√µes individuais\n",
    "                    if 'individual_predictions' in predictions and predictions['individual_predictions']:\n",
    "                        for pred_name, pred_data in predictions['individual_predictions'].items():\n",
    "                            if isinstance(pred_data, dict):\n",
    "                                # Remover campos problem√°ticos\n",
    "                                for key in ['model', 'trained_model', 'model_object']:\n",
    "                                    if key in pred_data:\n",
    "                                        del pred_data[key]\n",
    "                    \n",
    "                    # Limpar consensus se necess√°rio\n",
    "                    if 'consensus' in predictions and predictions['consensus']:\n",
    "                        consensus = predictions['consensus']\n",
    "                        if isinstance(consensus, dict):\n",
    "                            for key in ['model', 'trained_model', 'model_object']:\n",
    "                                if key in consensus:\n",
    "                                    del consensus[key]\n",
    "                    \n",
    "                    # Limpar trading signals\n",
    "                    if 'trading_signals' in predictions and predictions['trading_signals']:\n",
    "                        signals = predictions['trading_signals']\n",
    "                        if isinstance(signals, dict) and 'signals' in signals:\n",
    "                            for signal in signals['signals']:\n",
    "                                if isinstance(signal, dict):\n",
    "                                    for key in ['model', 'trained_model', 'model_object']:\n",
    "                                        if key in signal:\n",
    "                                            del signal[key]\n",
    "            \n",
    "            # Limpar outras se√ß√µes que podem ter objetos n√£o serializ√°veis\n",
    "            sections_to_clean = ['support_resistance', 'congestion_zones', 'opportunities']\n",
    "            for section in sections_to_clean:\n",
    "                if section in exportable and exportable[section]:\n",
    "                    if isinstance(exportable[section], dict):\n",
    "                        for key, value in exportable[section].items():\n",
    "                            if isinstance(value, list):\n",
    "                                for i, item in enumerate(value):\n",
    "                                    if isinstance(item, dict):\n",
    "                                        for model_key in ['model', 'trained_model', 'model_object']:\n",
    "                                            if model_key in item:\n",
    "                                                del item[model_key]\n",
    "            \n",
    "            return exportable\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro na prepara√ß√£o para JSON: {e}\")\n",
    "            # Fallback: criar estrutura b√°sica serializ√°vel\n",
    "            try:\n",
    "                basic_export = {\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'market_data': {\n",
    "                        'current_price': analysis.get('market_data', {}).get('current_price', 0),\n",
    "                        'total_records': analysis.get('market_data', {}).get('total_records', 0)\n",
    "                    },\n",
    "                    'executive_summary': analysis.get('executive_summary', {}),\n",
    "                    'recommendations': analysis.get('recommendations', {}),\n",
    "                    'export_note': 'Exporta√ß√£o simplificada devido a erro na convers√£o completa'\n",
    "                }\n",
    "                return basic_export\n",
    "            except:\n",
    "                return {'error': 'Falha na exporta√ß√£o JSON', 'timestamp': datetime.now().isoformat()}\n",
    "\n",
    "print(\"üöÄ WDO GERADOR v21 - PARTE 5/6 CARREGADA\")\n",
    "print(\"‚úÖ AdvancedReportGenerator com gr√°ficos implementado\")\n",
    "print(\"‚úÖ IntegratedMarketAnalyzer (classe principal) criado\")\n",
    "print(\"‚úÖ Sistema de relat√≥rios PDF + Charts completo\")\n",
    "print(\"‚úÖ Exporta√ß√£o JSON e resumos em texto\")\n",
    "print(\"\\nüìã Pr√≥ximo: Execute a Parte 6/6 para finalizar...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac491b6-cdf4-4e07-bec4-1ca71f27bc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ WDO ANALYZER v21 - SISTEMA COMPLETO\n",
      "============================================================\n",
      "üìä An√°lise Avan√ßada de Mercado com Machine Learning\n",
      "ü§ñ Clustering, Valida√ß√£o Cruzada, Cache Inteligente\n",
      "üìã Relat√≥rios Profissionais e Sinais de Trading\n",
      "============================================================\n",
      "üöÄ INICIALIZANDO WDO ANALYZER v21\n",
      "============================================================\n",
      "\n",
      "üìã CONFIGURA√á√ÉO ATUAL:\n",
      "==================================================\n",
      "ü§ñ Modelos habilitados: GradientBoosting, XGBoost, LightGBM, CatBoost\n",
      "üîÑ Valida√ß√£o cruzada: 5 folds\n",
      "üíæ Salvamento de modelos: ./saved_models\n",
      "üéØ Clustering S/R: kmeans\n",
      "üìä Dados: 5 - 500,000 registros\n",
      "\n",
      "‚úÖ Sistema inicializado com sucesso!\n",
      "‚úÖ Sistema inicializado com sucesso!\n",
      "\n",
      "üîÑ EXECUTANDO PIPELINE COMPLETO\n",
      "============================================================\n",
      "1Ô∏è‚É£ Carregando dados WDO...\n",
      "\n",
      "üìÇ CARREGAMENTO DE DADOS WDO\n",
      "==================================================\n",
      "üîç Procurando arquivos WDO reais...\n",
      "üìÅ Encontrado: C://Users//thiag//OneDrive//ARQUIVOS//Bolsa//COLETA//MT//FUTUROS\n",
      "üìä Arquivos WDO: 1320\n",
      "üìÅ Diret√≥rio: C://Users//thiag//OneDrive//ARQUIVOS//Bolsa//COLETA//MT//FUTUROS\n",
      "üìä Arquivos encontrados: 1320\n",
      "‚úÖ WDON_D1_20240526.csv: 1 registros\n",
      "‚úÖ WDON_D1_20240527.csv: 1 registros\n",
      "‚úÖ WDON_D1_20240528.csv: 1 registros\n",
      "‚úÖ WDON_D1_20240530.csv: 1 registros\n",
      "‚úÖ WDON_D1_20240602.csv: 1 registros\n",
      "‚úÖ WDON_D1_20240603.csv: 1 registros\n",
      "‚úÖ WDON_D1_20240604.csv: 1 registros\n",
      "‚úÖ WDON_D1_20240605.csv: 1 registros\n",
      "‚úÖ WDON_D1_20240606.csv: 1 registros\n",
      "‚úÖ WDON_D1_20240609.csv: 1 registros\n",
      "‚úÖ WDON_D1_20241013.csv: 1 registros\n",
      "‚úÖ WDON_D1_20250312.csv: 1 registros\n",
      "‚úÖ WDON_H1_20240802.csv: 10 registros\n",
      "‚úÖ WDON_H1_20241226.csv: 10 registros\n",
      "‚úÖ WDON_H1_20250526.csv: 10 registros\n",
      "‚úÖ WDON_M15_20241010.csv: 38 registros\n",
      "‚úÖ WDON_M15_20250311.csv: 38 registros\n",
      "‚úÖ WDON_M2_20240731.csv: 285 registros\n",
      "‚úÖ WDON_M2_20241220.csv: 285 registros\n",
      "‚úÖ WDON_M2_20250522.csv: 285 registros\n",
      "‚úÖ WDON_M5_20241008.csv: 114 registros\n",
      "‚úÖ WDON_M5_20250307.csv: 114 registros\n",
      "‚úÖ WDON_W1_20250104.csv: 1 registros\n",
      "\n",
      "üìä RESUMO DO CARREGAMENTO:\n",
      "   ‚úÖ Arquivos processados: 1320/1320\n",
      "   üìà Total de registros: 112,273\n",
      "üîÑ Combinando datasets...\n",
      "‚úÖ Dataset final: 85,971 registros\n",
      "‚úÖ 85,971 registros carregados\n",
      "\n",
      "2Ô∏è‚É£ Executando an√°lise completa...\n",
      "\n",
      "üöÄ INICIANDO AN√ÅLISE COMPLETA\n",
      "============================================================\n",
      "\n",
      "üìä AN√ÅLISE COMPLETA DE MERCADO\n",
      "============================================================\n",
      "1Ô∏è‚É£ Preparando dados...\n",
      "üîß Criando features melhoradas...\n",
      "‚úÖ Features melhoradas criadas: (85971, 61)\n",
      "   üìà Pre√ßo atual: 5.694,00\n",
      "   üìä Registros: 85,971\n",
      "\n",
      "2Ô∏è‚É£ An√°lise t√©cnica...\n",
      "\n",
      "3Ô∏è‚É£ An√°lise S/R com clustering...\n",
      "üéØ Identificando S/R com clustering...\n",
      "‚úÖ Clustering S/R: 6 suportes, 1 resist√™ncias\n",
      "\n",
      "4Ô∏è‚É£ An√°lise de zonas...\n",
      "üéØ Identificando zonas de congest√£o...\n",
      "‚úÖ 8 zonas de congest√£o identificadas\n",
      "\n",
      "5Ô∏è‚É£ Machine Learning...\n",
      "\n",
      "üß† TREINAMENTO INTELIGENTE DE MODELOS\n",
      "==================================================\n",
      "üìä Preparando features para 85,971 registros...\n",
      "üîß Prepara√ß√£o avan√ßada de features para 85971 registros...\n",
      "‚úÖ Features avan√ßadas preparadas: (85971, 95)\n",
      "üîë Hash dos dados: 687859796778926d\n",
      "‚úÖ Features selecionadas (20): ['√öLT. PRE√áO', 'price_change', 'rsi', 'volatility_10', 'volatility_20', 'momentum_5', 'momentum_10', 'price_vs_sma_10', 'price_vs_sma_20', 'price_vs_ema_10']...\n",
      "üéØ Features selecionadas: 20\n",
      "   √öLT. PRE√áO, price_change, rsi, volatility_10, volatility_20, momentum_5, momentum_10, price_vs_sma_10, price_vs_sma_20, price_vs_ema_10...\n",
      "‚úÖ Dados preparados: (85971, 20) -> 85971\n",
      "\n",
      "ü§ñ Processando modelo: GradientBoosting\n",
      "   üîÑ Treinando GradientBoosting...\n",
      "üîÑ Valida√ß√£o cruzada temporal: 5 folds para GradientBoosting\n",
      "   Fold 1: MAE=2.561280, RMSE=11.593344, R¬≤=0.9832\n",
      "   Fold 2: MAE=0.667908, RMSE=4.423360, R¬≤=0.9988\n",
      "   Fold 3: MAE=170.393697, RMSE=193.990807, R¬≤=-2.0671\n",
      "   Fold 4: MAE=0.173418, RMSE=0.285373, R¬≤=1.0000\n",
      "   Fold 5: MAE=0.124908, RMSE=0.171728, R¬≤=1.0000\n",
      "‚úÖ Modelo GradientBoosting salvo com sucesso usando joblib\n",
      "‚úÖ Modelo salvo: GradientBoosting (00c37ea5a7affa74)\n",
      "\n",
      "ü§ñ Processando modelo: XGBoost\n",
      "‚úÖ Modelo carregado: GradientBoosting (00c37ea5a7affa74)\n",
      "   ‚úÖ Modelo carregado do cache\n",
      "   üîÑ Performance degradou, re-treinando...\n",
      "   üîÑ Treinando XGBoost...\n",
      "üîÑ Valida√ß√£o cruzada temporal: 5 folds para XGBoost\n",
      "   Fold 1: MAE=5.295542, RMSE=13.972193, R¬≤=0.9756\n",
      "   Fold 2: MAE=5.744201, RMSE=12.633059, R¬≤=0.9900\n",
      "   Fold 3: MAE=193.653695, RMSE=216.887960, R¬≤=-2.8338\n",
      "   Fold 4: MAE=4.745321, RMSE=7.800417, R¬≤=0.9869\n",
      "   Fold 5: MAE=2.333661, RMSE=3.409852, R¬≤=0.9988\n",
      "‚úÖ Modelo XGBoost salvo com sucesso usando joblib\n",
      "‚úÖ Modelo salvo: XGBoost (00c37ea5a7affa74)\n",
      "\n",
      "ü§ñ Processando modelo: LightGBM\n",
      "‚úÖ Modelo carregado: XGBoost (00c37ea5a7affa74)\n",
      "   ‚úÖ Modelo carregado do cache\n",
      "   üîÑ Performance degradou, re-treinando...\n",
      "   üîÑ Treinando LightGBM...\n",
      "üîÑ Valida√ß√£o cruzada temporal: 5 folds para LightGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 14331, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 5432,822553\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "   Fold 1: MAE=5.405560, RMSE=13.199640, R¬≤=0.9782\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 28659, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 5507,312398\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "   Fold 2: MAE=5.030311, RMSE=11.237260, R¬≤=0.9921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 42987, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 5556,231640\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "   Fold 3: MAE=184.131615, RMSE=207.234812, R¬≤=-2.5002\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,001871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 57315, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 5682,788694\n",
      "   Fold 4: MAE=4.881315, RMSE=7.177043, R¬≤=0.9889\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,002311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 71643, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 5703,942779\n",
      "   Fold 5: MAE=2.618133, RMSE=3.691322, R¬≤=0.9986\n",
      "‚úÖ Modelo LightGBM salvo com sucesso usando joblib\n",
      "‚úÖ Modelo salvo: LightGBM (00c37ea5a7affa74)\n",
      "\n",
      "ü§ñ Processando modelo: CatBoost\n",
      "   üîÑ Treinando CatBoost...\n",
      "üîÑ Valida√ß√£o cruzada temporal: 5 folds para CatBoost\n",
      "   Fold 1: MAE=5.668097, RMSE=15.368220, R¬≤=0.9705\n",
      "   Fold 2: MAE=7.964724, RMSE=17.345515, R¬≤=0.9812\n",
      "   Fold 3: MAE=214.984747, RMSE=237.900700, R¬≤=-3.6127\n",
      "   Fold 4: MAE=6.257314, RMSE=11.167828, R¬≤=0.9731\n",
      "   Fold 5: MAE=2.061620, RMSE=2.887029, R¬≤=0.9992\n",
      "‚úÖ Modelo CatBoost salvo com sucesso usando joblib\n",
      "‚úÖ Modelo salvo: CatBoost (38888329989c4f2c)\n",
      "\n",
      "‚ö° TREINAMENTO CONCLU√çDO EM: 413.5 segundos\n",
      "‚úÖ Modelos processados: 4\n",
      "üíæ Cache hits: 0\n",
      "üîÑ Novos treinamentos: 4\n",
      "\n",
      "üèÜ RANKING DE MODELOS:\n",
      "----------------------------------------\n",
      "ü•á GradientBoosting (Novo)\n",
      "   MAE: 34.784242 ¬± 67.810540\n",
      "   RMSE: 42.092922 | R¬≤: 0.3830\n",
      "   CV Range: 0.124908 - 170.393697\n",
      "\n",
      "ü•à LightGBM (Novo)\n",
      "   MAE: 40.413387 ¬± 71.865775\n",
      "   RMSE: 48.508016 | R¬≤: 0.2915\n",
      "   CV Range: 2.618133 - 184.131615\n",
      "\n",
      "ü•â XGBoost (Novo)\n",
      "   MAE: 42.354484 ¬± 75.658766\n",
      "   RMSE: 50.940696 | R¬≤: 0.2235\n",
      "   CV Range: 2.333661 - 193.653695\n",
      "\n",
      "4¬∞ CatBoost (Novo)\n",
      "   MAE: 47.387300 ¬± 83.820795\n",
      "   RMSE: 56.933858 | R¬≤: 0.0623\n",
      "   CV Range: 2.061620 - 214.984747\n",
      "\n",
      "\n",
      "6Ô∏è‚É£ Gerando predi√ß√µes...\n",
      "\n",
      "üîÆ GERANDO PREDI√á√ïES CORRIGIDAS\n",
      "==================================================\n",
      "üìä Dados preparados: (100, 13) features, pre√ßo: 5.694,00\n",
      "üí∞ Pre√ßo atual para refer√™ncia: 5.694,00\n",
      "ü§ñ Predi√ß√µes GradientBoosting...\n",
      "   ‚úÖ GradientBoosting: 5.693,72 (-0.00%) - LATERAL ‚Üí - Conf: 55.0%\n",
      "ü§ñ Predi√ß√µes LightGBM...\n",
      "   ‚úÖ LightGBM: 5.698,98 (+0.09%) - LATERAL ‚Üí - Conf: 55.0%\n",
      "ü§ñ Predi√ß√µes XGBoost...\n",
      "   ‚úÖ XGBoost: 5.722,59 (+0.50%) - ALTA ‚Üë - Conf: 55.0%\n",
      "ü§ñ Predi√ß√µes CatBoost...\n",
      "   ‚úÖ CatBoost: 5.698,47 (+0.08%) - LATERAL ‚Üí - Conf: 55.0%\n",
      "\n",
      "‚úÖ Predi√ß√µes validadas: 4 modelo(s)\n",
      "üéØ Consenso: LATERAL ‚Üí - 5.698,73 (+0.08%) - Conf: 53.9%\n",
      "\n",
      "7Ô∏è‚É£ Consolidando an√°lise...\n",
      "‚ö†Ô∏è Erro no resumo executivo: 'reliability_score'\n",
      "‚ö†Ô∏è Erro na an√°lise de risco: 'agreement_level'\n",
      "\n",
      "‚úÖ AN√ÅLISE COMPLETA FINALIZADA EM 415.5s\n",
      "\n",
      "üìã GERANDO RELAT√ìRIOS...\n",
      "\n",
      "üìã GERANDO RELAT√ìRIO AVAN√áADO...\n",
      "‚úÖ Relat√≥rio avan√ßado gerado: ./cache\\WDO_Report_Advanced_20250528_184225.pdf\n",
      "üìä Gerando gr√°ficos para o relat√≥rio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 18:42:25,452 - matplotlib.legend - WARNING - No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gr√°ficos salvos em: ./cache\\WDO_Report_Advanced_20250528_184225_charts.pdf\n",
      "‚úÖ Relat√≥rio principal: ./cache\\WDO_Report_Advanced_20250528_184225.pdf\n",
      "‚úÖ Resumo texto: ./cache\\WDO_Summary_20250528_184225.txt\n",
      "\n",
      "üéâ AN√ÅLISE CONCLU√çDA COM SUCESSO!\n",
      "‚è±Ô∏è Tempo total: 416.2 segundos\n",
      "\n",
      "üìä RESUMO DA AN√ÅLISE\n",
      "==================================================\n",
      "üí∞ Pre√ßo Atual: 5.694,00\n",
      "üìà Sentimento: NEUTRO\n",
      "üéØ Confian√ßa: BAIXA\n",
      "üí° Recomenda√ß√£o: AGUARDAR\n",
      "ü§ñ Consenso ML: LATERAL ‚Üí (53.9%)\n",
      "üü¢ Suporte pr√≥ximo: 5.689,57\n",
      "üî¥ Resist√™ncia pr√≥xima: 5.710,41\n",
      "\n",
      "‚ö†Ô∏è Risco Geral: MODERADO\n",
      "\n",
      "3Ô∏è‚É£ Gerando exporta√ß√µes...\n",
      "‚úÖ An√°lise exportada: ./cache\\WDO_Analysis_20250528_184225.json\n",
      "\n",
      "üéâ PIPELINE CONCLU√çDO COM SUCESSO!\n",
      "‚è±Ô∏è Tempo total: 420.5 segundos\n",
      "\n",
      "üìä ESTAT√çSTICAS DO SISTEMA:\n",
      "----------------------------------------\n",
      "üìà Registros processados: 85,971\n",
      "üìÖ Per√≠odo: 26/05/2024 at√© 28/05/2025\n",
      "üí∞ Pre√ßo atual: 5.694,00\n",
      "ü§ñ Modelos ML: 4 (Cache: 0, Novos: 4)\n",
      "üéØ Consenso ML: LATERAL ‚Üí (53.9%)\n",
      "üéØ N√≠veis S/R: 6 suportes, 1 resist√™ncias\n",
      "üìä Sinais de trading: 0\n",
      "üí° Recomenda√ß√£o: AGUARDAR\n",
      "‚ö†Ô∏è Risco geral: MODERADO\n",
      "\n",
      "‚úÖ WDO ANALYZER v21 CONCLU√çDO COM SUCESSO!\n",
      "üìÅ Arquivos gerados na pasta: ./cache\n",
      "üíæ Modelos salvos em: ./saved_models\n",
      "\n",
      "üí° PR√ìXIMOS PASSOS:\n",
      "   1. üìÑ Verificar relat√≥rios PDF gerados\n",
      "   2. üìä Analisar sinais de trading identificados\n",
      "   3. üéØ Monitorar n√≠veis S/R identificados\n",
      "   4. üîÑ Re-executar com novos dados quando dispon√≠veis\n",
      "   5. ‚öôÔ∏è Ajustar configura√ß√µes se necess√°rio\n",
      "\n",
      "üëã Obrigado por usar o WDO Analyzer v21!\n",
      "üéâ WDO GERADOR v21 - SISTEMA COMPLETO CARREGADO!\n",
      "============================================================\n",
      "‚úÖ Todas as 6 partes implementadas com sucesso:\n",
      "   1Ô∏è‚É£ Configura√ß√µes customiz√°veis e persist√™ncia\n",
      "   2Ô∏è‚É£ Valida√ß√£o cruzada temporal e modelos avan√ßados\n",
      "   3Ô∏è‚É£ Clustering S/R e treinamento inteligente\n",
      "   4Ô∏è‚É£ Sistema de predi√ß√µes e an√°lise integrada\n",
      "   5Ô∏è‚É£ Relat√≥rios avan√ßados com gr√°ficos\n",
      "   6Ô∏è‚É£ Sistema completo e fun√ß√£o main\n",
      "============================================================\n",
      "üöÄ EXECUTE: python wdo_v21.py --help para ver todas as op√ß√µes\n",
      "üí° DICA: python wdo_v21.py --create-sample-config para criar configura√ß√£o exemplo\n",
      "üìã Sistema pronto para an√°lise profissional de mercado!\n"
     ]
    }
   ],
   "source": [
    "# WDO GERADOR v21 - PARTE 6/6 - SISTEMA COMPLETO\n",
    "# FUN√á√ÉO MAIN, EXEMPLOS E INTEGRA√á√ÉO FINAL\n",
    "# Sistema completo com todas as melhorias implementadas\n",
    "\n",
    "import signal\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "class WDOAnalyzerSystem:\n",
    "    \"\"\"Sistema completo WDO Analyzer v21\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.analyzer = None\n",
    "        self.config = None\n",
    "        \n",
    "    def initialize_system(self, config_file=None, args=None):\n",
    "        \"\"\"Inicializa o sistema completo\"\"\"\n",
    "        try:\n",
    "            print(\"üöÄ INICIALIZANDO WDO ANALYZER v21\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # Inicializar analisador integrado\n",
    "            self.analyzer = IntegratedMarketAnalyzer(config_file, args)\n",
    "            self.config = self.analyzer.config\n",
    "            \n",
    "            print(\"‚úÖ Sistema inicializado com sucesso!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na inicializa√ß√£o: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def run_full_analysis_pipeline(self, data_dir=None, force_retrain=False):\n",
    "        \"\"\"Executa pipeline completo de an√°lise\"\"\"\n",
    "        if not self.analyzer:\n",
    "            print(\"‚ùå Sistema n√£o inicializado!\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nüîÑ EXECUTANDO PIPELINE COMPLETO\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            pipeline_start = datetime.now()\n",
    "            \n",
    "            # 1. Carregamento de dados\n",
    "            print(\"1Ô∏è‚É£ Carregando dados WDO...\")\n",
    "            df = self.analyzer.load_all_wdo_data(data_dir)\n",
    "            \n",
    "            if df is None or len(df) == 0:\n",
    "                print(\"‚ùå Falha no carregamento de dados\")\n",
    "                return None\n",
    "            \n",
    "            print(f\"‚úÖ {len(df):,} registros carregados\")\n",
    "            \n",
    "            # 2. An√°lise completa\n",
    "            print(\"\\n2Ô∏è‚É£ Executando an√°lise completa...\")\n",
    "            analysis = self.analyzer.run_complete_analysis(\n",
    "                df, \n",
    "                force_retrain=force_retrain, \n",
    "                generate_reports=True\n",
    "            )\n",
    "            \n",
    "            if analysis is None:\n",
    "                print(\"‚ùå Falha na an√°lise\")\n",
    "                return None\n",
    "            \n",
    "            # 3. Exporta√ß√µes adicionais\n",
    "            print(\"\\n3Ô∏è‚É£ Gerando exporta√ß√µes...\")\n",
    "            \n",
    "            # JSON export\n",
    "            json_path = self.analyzer.export_analysis_json()\n",
    "            \n",
    "            # 4. Resumo final\n",
    "            pipeline_time = (datetime.now() - pipeline_start).total_seconds()\n",
    "            \n",
    "            print(f\"\\nüéâ PIPELINE CONCLU√çDO COM SUCESSO!\")\n",
    "            print(f\"‚è±Ô∏è Tempo total: {pipeline_time:.1f} segundos\")\n",
    "            \n",
    "            # Estat√≠sticas do sistema\n",
    "            self._print_system_statistics(analysis, df)\n",
    "            \n",
    "            return analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro no pipeline: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def _print_system_statistics(self, analysis, df):\n",
    "        \"\"\"Imprime estat√≠sticas do sistema\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nüìä ESTAT√çSTICAS DO SISTEMA:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Dados processados\n",
    "            print(f\"üìà Registros processados: {len(df):,}\")\n",
    "            \n",
    "            # Per√≠odo dos dados\n",
    "            if 'DATA' in df.columns:\n",
    "                start_date = df['DATA'].min().strftime('%d/%m/%Y')\n",
    "                end_date = df['DATA'].max().strftime('%d/%m/%Y')\n",
    "                print(f\"üìÖ Per√≠odo: {start_date} at√© {end_date}\")\n",
    "            \n",
    "            # Pre√ßo atual\n",
    "            current_price = analysis['market_data']['current_price']\n",
    "            print(f\"üí∞ Pre√ßo atual: {format_currency_br(current_price)}\")\n",
    "            \n",
    "            # Modelos ML\n",
    "            ml_analysis = analysis.get('ml_analysis', {})\n",
    "            models_info = ml_analysis.get('models_trained', {})\n",
    "            \n",
    "            if models_info:\n",
    "                total_models = models_info.get('total_models', 0)\n",
    "                cached_models = models_info.get('cached_models', 0)\n",
    "                new_models = models_info.get('new_models', 0)\n",
    "                \n",
    "                print(f\"ü§ñ Modelos ML: {total_models} (Cache: {cached_models}, Novos: {new_models})\")\n",
    "            \n",
    "            # Consenso ML\n",
    "            predictions = ml_analysis.get('predictions', {})\n",
    "            if predictions and predictions.get('consensus'):\n",
    "                consensus = predictions['consensus']\n",
    "                direction = consensus.get('direction', 'N/A')\n",
    "                confidence = consensus.get('confidence', 0)\n",
    "                print(f\"üéØ Consenso ML: {direction} ({confidence:.1f}%)\")\n",
    "            \n",
    "            # N√≠veis S/R\n",
    "            sr_info = analysis.get('support_resistance', {})\n",
    "            supports = sr_info.get('supports', [])\n",
    "            resistances = sr_info.get('resistances', [])\n",
    "            print(f\"üéØ N√≠veis S/R: {len(supports)} suportes, {len(resistances)} resist√™ncias\")\n",
    "            \n",
    "            # Sinais de trading\n",
    "            if predictions and predictions.get('trading_signals'):\n",
    "                signals_summary = predictions['trading_signals'].get('summary', {})\n",
    "                total_signals = signals_summary.get('total_signals', 0)\n",
    "                print(f\"üìä Sinais de trading: {total_signals}\")\n",
    "            \n",
    "            # Recomenda√ß√£o\n",
    "            recommendations = analysis.get('recommendations', {})\n",
    "            primary_rec = recommendations.get('primary_recommendation', 'N/A')\n",
    "            print(f\"üí° Recomenda√ß√£o: {primary_rec}\")\n",
    "            \n",
    "            # Risco\n",
    "            risk_analysis = analysis.get('risk_analysis', {})\n",
    "            overall_risk = risk_analysis.get('overall_risk', 'N/A')\n",
    "            print(f\"‚ö†Ô∏è Risco geral: {overall_risk}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro nas estat√≠sticas: {e}\")\n",
    "    \n",
    "    def interactive_mode(self):\n",
    "        \"\"\"Modo interativo para explorar an√°lises\"\"\"\n",
    "        if not self.analyzer or not self.analyzer.last_analysis:\n",
    "            print(\"‚ùå Nenhuma an√°lise dispon√≠vel. Execute a an√°lise primeiro.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüîç MODO INTERATIVO\")\n",
    "        print(\"=\" * 40)\n",
    "        print(\"Comandos dispon√≠veis:\")\n",
    "        print(\"  1 - Mostrar resumo executivo\")\n",
    "        print(\"  2 - Mostrar predi√ß√µes ML\")\n",
    "        print(\"  3 - Mostrar sinais de trading\")\n",
    "        print(\"  4 - Mostrar n√≠veis S/R\")\n",
    "        print(\"  5 - Mostrar oportunidades\")\n",
    "        print(\"  6 - Mostrar an√°lise de risco\")\n",
    "        print(\"  7 - Exportar an√°lise\")\n",
    "        print(\"  q - Sair\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                choice = input(\"\\nEscolha uma op√ß√£o: \").strip().lower()\n",
    "                \n",
    "                if choice == 'q':\n",
    "                    break\n",
    "                elif choice == '1':\n",
    "                    self._show_executive_summary()\n",
    "                elif choice == '2':\n",
    "                    self._show_ml_predictions()\n",
    "                elif choice == '3':\n",
    "                    self._show_trading_signals()\n",
    "                elif choice == '4':\n",
    "                    self._show_sr_levels()\n",
    "                elif choice == '5':\n",
    "                    self._show_opportunities()\n",
    "                elif choice == '6':\n",
    "                    self._show_risk_analysis()\n",
    "                elif choice == '7':\n",
    "                    self._export_analysis()\n",
    "                else:\n",
    "                    print(\"Op√ß√£o inv√°lida!\")\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Erro: {e}\")\n",
    "        \n",
    "        print(\"üëã Saindo do modo interativo...\")\n",
    "    \n",
    "    def _show_executive_summary(self):\n",
    "        \"\"\"Mostra resumo executivo\"\"\"\n",
    "        analysis = self.analyzer.last_analysis\n",
    "        exec_summary = analysis.get('executive_summary', {})\n",
    "        \n",
    "        print(f\"\\nüìã RESUMO EXECUTIVO:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Sentimento: {exec_summary.get('overall_sentiment', 'N/A')}\")\n",
    "        print(f\"Confian√ßa: {exec_summary.get('confidence_level', 'N/A')}\")\n",
    "        \n",
    "        recommendations = analysis.get('recommendations', {})\n",
    "        print(f\"Recomenda√ß√£o: {recommendations.get('primary_recommendation', 'N/A')}\")\n",
    "        \n",
    "        key_insights = exec_summary.get('key_insights', [])\n",
    "        if key_insights:\n",
    "            print(\"\\nInsights principais:\")\n",
    "            for insight in key_insights[:3]:\n",
    "                print(f\"  ‚Ä¢ {insight}\")\n",
    "    \n",
    "    def _show_ml_predictions(self):\n",
    "        \"\"\"Mostra predi√ß√µes ML\"\"\"\n",
    "        predictions = self.analyzer.get_latest_predictions()\n",
    "        \n",
    "        if not predictions:\n",
    "            print(\"‚ùå Nenhuma predi√ß√£o ML dispon√≠vel\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nü§ñ PREDI√á√ïES MACHINE LEARNING:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Consenso\n",
    "        consensus = predictions.get('consensus', {})\n",
    "        if consensus:\n",
    "            print(f\"Consenso: {consensus.get('direction', 'N/A')} ({consensus.get('confidence', 0):.1f}%)\")\n",
    "            print(f\"Pre√ßo previsto: {format_currency_br(consensus.get('predicted_price', 0))}\")\n",
    "            print(f\"Varia√ß√£o: {consensus.get('price_change', 0):+.2f}%\")\n",
    "        \n",
    "        # Predi√ß√µes individuais\n",
    "        individual = predictions.get('individual_predictions', {})\n",
    "        if individual:\n",
    "            print(f\"\\nPredi√ß√µes por modelo:\")\n",
    "            for model_name, pred in individual.items():\n",
    "                direction = pred.get('direction', 'N/A')\n",
    "                confidence = pred.get('confidence', 0)\n",
    "                change = pred.get('price_change', 0)\n",
    "                print(f\"  {model_name}: {direction} ({confidence:.1f}%) - {change:+.2f}%\")\n",
    "    \n",
    "    def _show_trading_signals(self):\n",
    "        \"\"\"Mostra sinais de trading\"\"\"\n",
    "        signals_data = self.analyzer.get_trading_signals()\n",
    "        \n",
    "        if not signals_data:\n",
    "            print(\"‚ùå Nenhum sinal de trading dispon√≠vel\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüìä SINAIS DE TRADING:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        summary = signals_data.get('summary', {})\n",
    "        signals = signals_data.get('signals', [])\n",
    "        \n",
    "        print(f\"Total de sinais: {summary.get('total_signals', 0)}\")\n",
    "        print(f\"BUY: {summary.get('buy_signals', 0)} | SELL: {summary.get('sell_signals', 0)}\")\n",
    "        \n",
    "        # Melhor sinal\n",
    "        best_signal = summary.get('best_signal')\n",
    "        if best_signal:\n",
    "            print(f\"\\nüèÜ Melhor sinal:\")\n",
    "            print(f\"  Tipo: {best_signal['type']}\")\n",
    "            print(f\"  Confian√ßa: {best_signal['confidence']:.1f}%\")\n",
    "            print(f\"  Entrada: {format_currency_br(best_signal['entry_price'])}\")\n",
    "            print(f\"  Alvo: {format_currency_br(best_signal['target_price'])}\")\n",
    "            print(f\"  Stop: {format_currency_br(best_signal['stop_loss'])}\")\n",
    "        \n",
    "        # Outros sinais\n",
    "        if len(signals) > 1:\n",
    "            print(f\"\\nOutros sinais:\")\n",
    "            for i, signal in enumerate(signals[1:4], 2):\n",
    "                print(f\"  {i}. {signal['type']} - Conf: {signal['confidence']:.1f}%\")\n",
    "    \n",
    "    def _show_sr_levels(self):\n",
    "        \"\"\"Mostra n√≠veis S/R\"\"\"\n",
    "        supports, resistances = self.analyzer.get_support_resistance_levels()\n",
    "        \n",
    "        print(f\"\\nüéØ N√çVEIS DE SUPORTE E RESIST√äNCIA:\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        if resistances:\n",
    "            print(f\"üî¥ Resist√™ncias:\")\n",
    "            for i, res in enumerate(resistances[:5], 1):\n",
    "                price = format_currency_br(res['price'])\n",
    "                strength = res['strength']\n",
    "                print(f\"  R{i}. {price} (For√ßa: {strength:.1f}%)\")\n",
    "        \n",
    "        if supports:\n",
    "            print(f\"\\nüü¢ Suportes:\")\n",
    "            for i, sup in enumerate(supports[:5], 1):\n",
    "                price = format_currency_br(sup['price'])\n",
    "                strength = sup['strength']\n",
    "                print(f\"  S{i}. {price} (For√ßa: {strength:.1f}%)\")\n",
    "    \n",
    "    def _show_opportunities(self):\n",
    "        \"\"\"Mostra oportunidades\"\"\"\n",
    "        analysis = self.analyzer.last_analysis\n",
    "        opportunities = analysis.get('opportunities', [])\n",
    "        \n",
    "        print(f\"\\nüí° OPORTUNIDADES IDENTIFICADAS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if opportunities:\n",
    "            for i, opp in enumerate(opportunities[:5], 1):\n",
    "                opp_type = opp.get('type', 'N/A')\n",
    "                description = opp.get('description', 'N/A')\n",
    "                probability = opp.get('probability', 0)\n",
    "                \n",
    "                print(f\"{i}. {opp_type}\")\n",
    "                print(f\"   {description}\")\n",
    "                print(f\"   Probabilidade: {probability:.1f}%\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"Nenhuma oportunidade de alto potencial identificada.\")\n",
    "    \n",
    "    def _show_risk_analysis(self):\n",
    "        \"\"\"Mostra an√°lise de risco\"\"\"\n",
    "        analysis = self.analyzer.last_analysis\n",
    "        risk_analysis = analysis.get('risk_analysis', {})\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è AN√ÅLISE DE RISCO:\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        overall_risk = risk_analysis.get('overall_risk', 'N/A')\n",
    "        print(f\"Risco geral: {overall_risk}\")\n",
    "        \n",
    "        print(f\"Volatilidade: {risk_analysis.get('volatility_risk', 'N/A')}\")\n",
    "        print(f\"Liquidez: {risk_analysis.get('liquidity_risk', 'N/A')}\")\n",
    "        print(f\"Predi√ß√£o: {risk_analysis.get('prediction_risk', 'N/A')}\")\n",
    "        \n",
    "        risk_factors = risk_analysis.get('risk_factors', [])\n",
    "        if risk_factors:\n",
    "            print(f\"\\nFatores de risco:\")\n",
    "            for factor in risk_factors:\n",
    "                clean_factor = factor.encode('ascii', 'ignore').decode('ascii')\n",
    "                print(f\"  ‚Ä¢ {clean_factor}\")\n",
    "    \n",
    "    def _export_analysis(self):\n",
    "        \"\"\"Exporta an√°lise\"\"\"\n",
    "        print(f\"\\nExportando an√°lise...\")\n",
    "        \n",
    "        json_path = self.analyzer.export_analysis_json()\n",
    "        if json_path:\n",
    "            print(f\"‚úÖ An√°lise exportada: {json_path}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Erro na exporta√ß√£o\")\n",
    "\n",
    "def create_sample_config():\n",
    "    \"\"\"Cria arquivo de configura√ß√£o de exemplo\"\"\"\n",
    "    sample_config = {\n",
    "        \"data\": {\n",
    "            \"min_data_points\": 5,\n",
    "            \"max_data_points\": 100000,\n",
    "            \"price_range\": 80,\n",
    "            \"min_volume\": 500,\n",
    "            \"top_levels\": 10,\n",
    "            \"max_files\": None\n",
    "        },\n",
    "        \"ml_models\": {\n",
    "            \"enabled_models\": [\"GradientBoosting\", \"XGBoost\", \"LightGBM\", \"CatBoost\"],\n",
    "            \"lstm\": {\n",
    "                \"units\": [128, 64, 32],\n",
    "                \"dropout\": 0.3,\n",
    "                \"epochs\": 100,\n",
    "                \"batch_size\": 64,\n",
    "                \"learning_rate\": 0.001,\n",
    "                \"sequence_length\": 100,\n",
    "                \"bidirectional\": True\n",
    "            },\n",
    "            \"xgboost\": {\n",
    "                \"n_estimators\": 200,\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"max_depth\": 6,\n",
    "                \"min_child_weight\": 1,\n",
    "                \"subsample\": 0.8,\n",
    "                \"colsample_bytree\": 0.8,\n",
    "                \"tree_method\": \"hist\",  # Mais r√°pido para grandes datasets\n",
    "                \"random_state\": 42\n",
    "            },\n",
    "            \"lightgbm\": {\n",
    "                \"n_estimators\": 200,\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"max_depth\": 6,\n",
    "                \"num_leaves\": 31,\n",
    "                \"subsample\": 0.8,\n",
    "                \"colsample_bytree\": 0.8,\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": -1\n",
    "            },\n",
    "            \"gru\": {\n",
    "                \"units\": [96, 48],\n",
    "                \"dropout\": 0.25,\n",
    "                \"epochs\": 80,\n",
    "                \"batch_size\": 32,\n",
    "                \"learning_rate\": 0.0008,\n",
    "                \"sequence_length\": 80,\n",
    "                \"bidirectional\": True\n",
    "            },\n",
    "            \"random_forest\": {\n",
    "                \"n_estimators\": 300,\n",
    "                \"max_depth\": 20,\n",
    "                \"min_samples_split\": 8,\n",
    "                \"min_samples_leaf\": 4\n",
    "            },\n",
    "            \"gradient_boosting\": {\n",
    "                \"n_estimators\": 250,\n",
    "                \"learning_rate\": 0.08,\n",
    "                \"max_depth\": 8\n",
    "            },\n",
    "            \"lstm_bidirectional\": {\n",
    "                \"units\": [128, 64, 32],\n",
    "                \"dropout\": 0.25,\n",
    "                \"recurrent_dropout\": 0.15,\n",
    "                \"epochs\": 60,\n",
    "                \"batch_size\": 32,\n",
    "                \"learning_rate\": 0.0008,\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"early_stopping_patience\": 15,\n",
    "                \"sequence_length\": 80,\n",
    "                \"bidirectional\": True\n",
    "            },\n",
    "            \n",
    "            # Adicionar ap√≥s gradient_boosting:\n",
    "            \"catboost\": {\n",
    "                \"iterations\": 500,\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"depth\": 6,\n",
    "                \"l2_leaf_reg\": 3,\n",
    "                \"border_count\": 128,\n",
    "                \"random_state\": 42,\n",
    "                \"verbose\": False,\n",
    "                \"early_stopping_rounds\": 50,\n",
    "                \"use_best_model\": True,\n",
    "                \"task_type\": \"CPU\"\n",
    "            }\n",
    "            \n",
    "        },\n",
    "        \"validation\": {\n",
    "            \"use_cross_validation\": True,\n",
    "            \"cv_folds\": 5,\n",
    "            \"test_size\": 0.15\n",
    "        },\n",
    "        \"clustering\": {\n",
    "            \"method\": \"kmeans\",\n",
    "            \"kmeans_clusters\": 10,\n",
    "            \"feature_scaling\": \"standard\"\n",
    "        },\n",
    "        \"model_persistence\": {\n",
    "            \"save_models\": True,\n",
    "            \"models_directory\": \"./saved_models\",\n",
    "            \"cache_directory\": \"./cache\",\n",
    "            \"model_cache_hours\": 48\n",
    "        },\n",
    "        \"trading\": {\n",
    "            \"min_confidence\": 75,\n",
    "            \"min_price_change\": 1.5\n",
    "        },\n",
    "        \"prediction\": {\n",
    "            \"horizon\": 5,\n",
    "            \"confidence_threshold\": 0.7\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = \"./wdo_config_sample.yaml\"\n",
    "    \n",
    "    try:\n",
    "        import yaml\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(sample_config, f, default_flow_style=False, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Configura√ß√£o de exemplo criada: {config_path}\")\n",
    "        return config_path\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback para JSON se YAML n√£o dispon√≠vel\n",
    "        config_path = \"./wdo_config_sample.json\"\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sample_config, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Configura√ß√£o de exemplo criada: {config_path}\")\n",
    "        return config_path\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"Handler para interrup√ß√£o do sistema\"\"\"\n",
    "    print(f\"\\n\\n‚ö†Ô∏è Interrup√ß√£o recebida (Signal {signum})\")\n",
    "    print(\"üõë Finalizando WDO Analyzer v21...\")\n",
    "    sys.exit(0)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fun√ß√£o principal do sistema\"\"\"\n",
    "    # Configurar handler de sinais\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    signal.signal(signal.SIGTERM, signal_handler)\n",
    "    \n",
    "    print(\"üöÄ WDO ANALYZER v21 - SISTEMA COMPLETO\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä An√°lise Avan√ßada de Mercado com Machine Learning\")\n",
    "    print(\"ü§ñ Clustering, Valida√ß√£o Cruzada, Cache Inteligente\")\n",
    "    print(\"üìã Relat√≥rios Profissionais e Sinais de Trading\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Parse de argumentos\n",
    "        args = parse_arguments()\n",
    "        \n",
    "        # Criar configura√ß√£o de exemplo se solicitado\n",
    "        if hasattr(args, 'create_sample_config') and args.create_sample_config:\n",
    "            create_sample_config()\n",
    "            return\n",
    "        \n",
    "        # Inicializar sistema\n",
    "        system = WDOAnalyzerSystem()\n",
    "        \n",
    "        if not system.initialize_system(args.config if hasattr(args, 'config') else None, args):\n",
    "            print(\"‚ùå Falha na inicializa√ß√£o do sistema\")\n",
    "            return\n",
    "        \n",
    "        # Limpeza de modelos antigos se solicitado\n",
    "        if hasattr(args, 'cleanup_models') and args.cleanup_models:\n",
    "            print(\"üßπ Limpando modelos antigos...\")\n",
    "            system.analyzer.trainer.persistence.cleanup_old_models()\n",
    "        \n",
    "        # Executar an√°lise completa\n",
    "        force_retrain = hasattr(args, 'force_retrain') and args.force_retrain\n",
    "        data_dir = args.data_dir if hasattr(args, 'data_dir') else None\n",
    "        \n",
    "        analysis = system.run_full_analysis_pipeline(data_dir, force_retrain)\n",
    "        \n",
    "        if analysis is None:\n",
    "            print(\"‚ùå Falha na an√°lise. Verifique os dados e configura√ß√µes.\")\n",
    "            return\n",
    "        \n",
    "        # Salvar configura√ß√£o atual se solicitado\n",
    "        if hasattr(args, 'save_config') and args.save_config:\n",
    "            system.config.save_config(args.save_config)\n",
    "        \n",
    "        # Modo interativo se solicitado ou se n√£o especificado o contr√°rio\n",
    "        if hasattr(args, 'interactive') and args.interactive:\n",
    "            system.interactive_mode()\n",
    "        elif not hasattr(args, 'no_interactive'):\n",
    "            # Oferecer modo interativo por padr√£o\n",
    "            try:\n",
    "                response = input(f\"\\nüîç Deseja entrar no modo interativo? (s/N): \").strip().lower()\n",
    "                if response in ['s', 'sim', 'y', 'yes']:\n",
    "                    system.interactive_mode()\n",
    "            except KeyboardInterrupt:\n",
    "                pass\n",
    "        \n",
    "        print(f\"\\n‚úÖ WDO ANALYZER v21 CONCLU√çDO COM SUCESSO!\")\n",
    "        print(f\"üìÅ Arquivos gerados na pasta: {system.config.get('model_persistence.cache_directory', './cache')}\")\n",
    "        print(f\"üíæ Modelos salvos em: {system.config.get('model_persistence.models_directory', './saved_models')}\")\n",
    "        \n",
    "        # Mostrar pr√≥ximos passos\n",
    "        print(f\"\\nüí° PR√ìXIMOS PASSOS:\")\n",
    "        print(f\"   1. üìÑ Verificar relat√≥rios PDF gerados\")\n",
    "        print(f\"   2. üìä Analisar sinais de trading identificados\")\n",
    "        print(f\"   3. üéØ Monitorar n√≠veis S/R identificados\")\n",
    "        print(f\"   4. üîÑ Re-executar com novos dados quando dispon√≠veis\")\n",
    "        print(f\"   5. ‚öôÔ∏è Ajustar configura√ß√µes se necess√°rio\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n\\n‚ö†Ô∏è Execu√ß√£o interrompida pelo usu√°rio\")\n",
    "        print(f\"üõë WDO Analyzer v21 finalizado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERRO CR√çTICO NO SISTEMA:\")\n",
    "        print(f\"   {str(e)}\")\n",
    "        print(f\"\\nüîß SUGEST√ïES DE CORRE√á√ÉO:\")\n",
    "        print(f\"   1. Verificar se os arquivos WDO*.csv existem\")\n",
    "        print(f\"   2. Confirmar instala√ß√£o das depend√™ncias:\")\n",
    "        print(f\"      pip install tensorflow scikit-learn pandas numpy matplotlib seaborn reportlab\")\n",
    "        print(f\"   3. Verificar permiss√µes de escrita nos diret√≥rios\")\n",
    "        print(f\"   4. Tentar com --force-retrain para recriar modelos\")\n",
    "        print(f\"   5. Usar --config com arquivo de configura√ß√£o personalizado\")\n",
    "        \n",
    "        # Detalhes t√©cnicos do erro\n",
    "        if hasattr(args, 'verbose') and args.verbose:\n",
    "            print(f\"\\nüêõ DETALHES T√âCNICOS:\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        print(f\"\\nüëã Obrigado por usar o WDO Analyzer v21!\")\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Parser de argumentos da linha de comando com suporte para Jupyter\"\"\"\n",
    "    import sys\n",
    "    import argparse\n",
    "    \n",
    "    # Remover argumentos espec√≠ficos do Jupyter (como -f kernel.json)\n",
    "    jupyter_args = []\n",
    "    i = 1\n",
    "    while i < len(sys.argv):\n",
    "        if sys.argv[i] == '-f' and i+1 < len(sys.argv):\n",
    "            jupyter_args.append(sys.argv[i])\n",
    "            jupyter_args.append(sys.argv[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "            \n",
    "    # Criar uma c√≥pia dos argumentos sem os argumentos do Jupyter\n",
    "    filtered_args = [arg for arg in sys.argv if arg not in jupyter_args]\n",
    "    \n",
    "    # Substituir temporariamente sys.argv para o parser\n",
    "    original_args = sys.argv\n",
    "    sys.argv = filtered_args\n",
    "    \n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='WDO Analyzer v21 - Sistema Avan√ßado de An√°lise de Mercado',\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "Exemplos de uso:\n",
    "  python wdo_v21.py                                    # Executar com configura√ß√µes padr√£o\n",
    "  python wdo_v21.py --config config.yaml               # Usar arquivo de configura√ß√£o\n",
    "  python wdo_v21.py --models LSTM,RandomForest         # Usar modelos espec√≠ficos\n",
    "  python wdo_v21.py --force-retrain                    # For√ßar re-treinamento\n",
    "  python wdo_v21.py --create-sample-config             # Criar arquivo de configura√ß√£o exemplo\n",
    "  python wdo_v21.py --cleanup-models                   # Limpar modelos antigos\n",
    "  python wdo_v21.py --data-dir ./meus_dados             # Usar diret√≥rio customizado\n",
    "  python wdo_v21.py --interactive                      # For√ßar modo interativo\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Configura√ß√£o\n",
    "    parser.add_argument('--config', type=str, help='Arquivo de configura√ß√£o (YAML/JSON)')\n",
    "    parser.add_argument('--save-config', type=str, help='Salvar configura√ß√£o atual em arquivo')\n",
    "    parser.add_argument('--create-sample-config', action='store_true', \n",
    "                       help='Criar arquivo de configura√ß√£o exemplo')\n",
    "    \n",
    "    # Modelos\n",
    "    parser.add_argument('--models', type=str, \n",
    "                       help='Modelos a usar (separados por v√≠rgula): LSTM,GRU,RandomForest,GradientBoosting,CNN_LSTM,SVR')\n",
    "    parser.add_argument('--epochs', type=int, help='N√∫mero de √©pocas para modelos de deep learning')\n",
    "    parser.add_argument('--batch-size', type=int, help='Tamanho do batch')\n",
    "    parser.add_argument('--cv-folds', type=int, help='N√∫mero de folds para valida√ß√£o cruzada')\n",
    "    \n",
    "    # Persist√™ncia\n",
    "    parser.add_argument('--no-save-models', action='store_true', help='N√£o salvar modelos treinados')\n",
    "    parser.add_argument('--force-retrain', action='store_true', \n",
    "                       help='For√ßar re-treinamento mesmo com cache v√°lido')\n",
    "    parser.add_argument('--cleanup-models', action='store_true', help='Limpar modelos antigos')\n",
    "    \n",
    "    # Dados\n",
    "    parser.add_argument('--data-dir', type=str, help='Diret√≥rio dos dados WDO')\n",
    "    parser.add_argument('--max-records', type=int, help='M√°ximo de registros a processar')\n",
    "    \n",
    "    # Interface\n",
    "    parser.add_argument('--interactive', action='store_true', help='For√ßar modo interativo')\n",
    "    parser.add_argument('--no-interactive', action='store_true', help='Pular modo interativo')\n",
    "    \n",
    "    # Sa√≠da\n",
    "    parser.add_argument('--output-dir', type=str, help='Diret√≥rio de sa√≠da para relat√≥rios')\n",
    "    parser.add_argument('--verbose', action='store_true', help='Sa√≠da detalhada')\n",
    "    \n",
    "    # Permitir argumentos desconhecidos para evitar erros com os argumentos do Jupyter\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    # Restaurar os argumentos originais\n",
    "    sys.argv = original_args\n",
    "    \n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print(\"üéâ WDO GERADOR v21 - SISTEMA COMPLETO CARREGADO!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Todas as 6 partes implementadas com sucesso:\")\n",
    "print(\"   1Ô∏è‚É£ Configura√ß√µes customiz√°veis e persist√™ncia\")\n",
    "print(\"   2Ô∏è‚É£ Valida√ß√£o cruzada temporal e modelos avan√ßados\") \n",
    "print(\"   3Ô∏è‚É£ Clustering S/R e treinamento inteligente\")\n",
    "print(\"   4Ô∏è‚É£ Sistema de predi√ß√µes e an√°lise integrada\")\n",
    "print(\"   5Ô∏è‚É£ Relat√≥rios avan√ßados com gr√°ficos\")\n",
    "print(\"   6Ô∏è‚É£ Sistema completo e fun√ß√£o main\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ EXECUTE: python wdo_v21.py --help para ver todas as op√ß√µes\")\n",
    "print(\"üí° DICA: python wdo_v21.py --create-sample-config para criar configura√ß√£o exemplo\")\n",
    "print(\"üìã Sistema pronto para an√°lise profissional de mercado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdaf9ac-8bcb-48ff-9e51-fab22b3d6232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
